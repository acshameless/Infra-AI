AI Systems Performance Engineering

AI 系统性能工程师的箴言是“机械共鸣”，协同硬件、软件和算法，深入理解机器——包括硬件和软件——以便定制高效的解决方案，充分利用整个技术栈的性能能力。

神经网络、python、机器学习


PUE
- 液冷
- 风冷




第一章

费用 -> 效率，充分利用可用硬件，例如 deepseek 利用通信带宽、FlashAttention

训练 推理 ROI

1. 硬件
    H100 H200 
    B200 B300

2. AI Systems Performance Engineer AI 系统性能工程师的箴言是“机械共鸣”
    1. 协同硬件、软件和算法，深入理解机器——包括硬件和软件——以便定制高效的解决方案，充分利用整个技术栈的性能能力。
    2. 低层操作系统、存储层次结构、网络基础，并掌握多种语言如 Python 和 C++，以及诸如 PyTorch、OpenAI 的 Triton 和 NVIDIA 的Compute Unified Device Architecture (CUDA) 等不同的 AI 框架与库。       
    3. 职责：基准测试、性能分析、调试、优化、扩展分布式训练与推理、高效管理资源
        - 基准测试和性能分析包括在不同工作负载（包括训练和推理）下测量延迟、吞吐量、内存使用和其他性能指标。
        - 为了识别瓶颈，迭代性地将NVIDIA Nsight Systems和 NVIDIA Nsight Compute 与 PyTorch分析器一起使用。
        - 调试分布式训练算法：NCCL NIXL
    4. 要点 
        - 测量有效吞吐量，关注有效、有用的 GPU 利用率。GPU 用于执行有用工作（例如前向/反向传播计算）与等待数据或其他开销的时间比率
        - 优先采用高超的工程优化，而不是盲目加钱堆硬件
        - 通过渐进式优化寻找数量级的影响
        - 以基于分析的思维方式进行性能调优，使用数据和分析工具来指导优化。
        - 保持整体视角，考虑软硬件协同设计：有时算法调整可以缓解硬件限制，有时新的硬件特性又能催生新的算法。
        - 关注最新的硬件、软件与算法动态

3. MLPerf Deepseek开源周 
    开放基准套件为在不同硬件和软件配置间可复现地比较训练与推理性能提供了标准。

4. 100万亿参数模型
    每个参数以16位（2字节）精度存储，加载该模型大约需要182TB的GPU内存（182TB= 100 万亿参数× 16 位每权重× 8 位每字节）。
    B200GPU上的显存为 192GB（可用约180GB）1000块，约等于 700块 B300 288GB

5. goodput 有效吞吐量：理论硬件能力中有多少真正用于有用计算。

    传统的吞吐量指标如FLOPS和设备利用率往往显得偏高，因为大量时间可能被阻塞的通信、空闲的计算或失败的任务重启所占用。
    
    提升有效产出需要深入理解硬件（例如 CPU、GPU、网络拓扑、内存层次结构、存储布局）、软件（例如操作系统配置、分页内存、I/O 利用）和
    算法（例如 Transformer 架构变体、注意力机制替代方案以及不同的缓存与批处理策略）之间的相互作用。




第二章

传统系统中，CPU 和 GPU 拥有各自独立的内存池；GB200/300 使用 NVLink‑C2C（芯片到芯片 双向～900GB/s ）的定制高速链路连接 CPU 和 GPU。
PCIe Gen5 x16 每个方向 ～64GB/s
PCIe Gen6 x16 每个方向 ～128GB/s

H100 GPU 的带宽为 ~3.35 TB/s
H200 
GH200    

与 H100 相比，H800 降低了 NVLink 互连带宽和 FP64 性能，同时保持 HBM 容量和带宽在大体上相似。
    1. NVIDIAH100每块GPU的NVLink互连带宽约为900GB/s，而H800每块GPU约为400GB/s。
    2. 3.35 TB/s 的显存带宽（SXM版本）或 2 TB/s 的显存带宽（PCIe版本）


Blackwell 将两个 GPU 芯片模组封装在同一模块内的多芯片模块 (MCM) 设计，专用的高速 10 TB/s 晶片间互连NV‑HBI（高带宽接口）通信。
B200 GPU上的显存为 192GB（可用约180GB，每个芯片96GB），8 个HBM3e，HBM3e 8层堆栈 DRAM芯片，每层3GB。    
B300 GPU上的显存为 288GB                     

GB200  ～900GB统一内存  CPU 内存 ~480GB LPDDR5X，带宽 ~500 GB/s    ~384 GB 的 HBM3e内存（每个 GPU 192GB），带宽 ～8TB/s
GB300  ～900GB统一内存  CPU 内存 ~480GB LPDDR5X，带宽 ~500 GB/s    ~576 GB 的 HBM3e内存（每个 GPU 288GB），带宽 ～8TB/s


GB200 NVL72 是一个自成体系的 72 GPU、1.44 exaFLOPS（FP4），机架功耗约为 120 到 132 kW。
每个 GPU 使用其 18 条 NVLink 链路连接到 18 个 NVSwitch 芯片，从而可以通过单个交换机形成路径。
每个Blackwell GPU暴露18个NVLink5端口。单个 NVLink 端口 = 100 GB/s 双向，18 个端口全部接入 NVLink 网络 → 聚合双向带宽 = 18 × 100 GB/s = 1800 GB/s ≈ 1.8 TB/s，NVL72 域内提供约 130 TB/s 的总体 GPU 到 GPU 带宽。

NVL72 中的每个计算节点约消耗 6 kW，18 个计算节点的总耗电为~110 kW。NVSwitch 托盘、网络交换机、空气冷却和水冷泵共计占用~20 kW，总计整个 NVL72 机架的耗电为 130 kW。

42U 机架：18个GPU托盘 + 9个交换机托盘 + 上下各2个33kW电源托盘 + 顶上2个ipmi托盘。
多机架通信带宽：每个计算节点配备四个 800 Gb/s 的网卡ConnectX-8，总计每节点 3.2 Tbit/s，每个机架约 57.6 Tbit/s。

NVL72         ~13.5 TB of HBM (72 × 192 GB), and ~18 TB of DDR (36 × 480 GB).
NVL72 Ultra   ~20.7 TB of HBM (72 × 288 GB), and ~18 TB of DDR (36 × 480 GB).


液冷：粗略估算，在水温上升10–12°C时，需达到约150–200升/分钟的流量以散热约130 kW。
NVL72 机架在装满硬件和冷却液时重约 3,000 磅（ 1.3–1.4 吨）。



12. SM??
17. 对于需要在网络内加速或卸载以处理存储、安全和控制平面任务的场景，会使用BlueField‑3DPU。
18. 可帮助卸载诸如RDMA、TCP/IP和NVMeSSD存储访问等网络任务，
21. 要高效运行NVL72，需要对性能、利用率和功耗进行仔细监控。
22. DataCenterGPUManager(DCGM)，每个GPU的指标，例如GPU利用率百分比、显存使用、温度和NVLink吞吐量



### 1. 核心芯片架构 (Blackwell GPU)
*   **设计形态**：采用 **MCM（多芯片模块）** 设计，将两个 GPU 芯片模组封装在同一模块内。
*   **片间互连**：通过专用的 **NV-HBI（高带宽接口）** 通信，带宽高达 **10 TB/s**。
*   **显存规格 (HBM3e)**：
    *   **B200 GPU**：总显存 **192GB**（由 8 个 8层堆栈的 HBM3e 组成，每层 3GB，单芯片 96GB），实际可用约 180GB。
    *   **B300 GPU**：总显存 **288GB**（显存容量提升）。

### 2. 超级芯片平台 (Grace Blackwell Superchip)
该平台采用统一内存架构（Unified Memory），结合了 Grace CPU 和 Blackwell GPU。

*   **GB200**：
    *   **总内存**：约 900GB。
    *   **组成**：~480GB LPDDR5X (CPU) + 384GB HBM3e (2x B200 GPU)。
    *   **带宽**：HBM 带宽 ~8 TB/s；CPU 内存带宽 ~500 GB/s。
*   **GB300**：
    *   **总内存**：约 1000GB。
    *   **组成**：~480GB LPDDR5X (CPU) + 576GB HBM3e (2x B300 GPU)。
    *   **带宽**：HBM 带宽 ~8 TB/s。

### 3. 互连与网络通信
*   **NVLink 5 (GPU 间通信)**：
    *   **端口配置**：每个 GPU 暴露 18 个端口，连接到 18 个 NVSwitch 芯片。
    *   **单机带宽**：单端口 100 GB/s，**单 GPU 聚合双向带宽 1.8 TB/s**。
    *   **机架域带宽**：NVL72 域内提供约 **130 TB/s** 的 GPU-to-GPU 总带宽。
*   **外部网络 (机架间通信)**：
    *   **节点配置**：每个计算节点配备 4 个 ConnectX-8 网卡 (800 Gb/s)。
    *   **总带宽**：单节点 3.2 Tbit/s，整机架约 57.6 Tbit/s。

### 4. 系统级架构 (GB200 NVL72 机架)
NVL72 是一个通过铜缆背板连接的巨型计算单元，可视作一个超大 GPU。

*   **算力与规模**：
    *   包含 **72 个 GPU** 和 36 个 Grace CPU。
    *   AI 算力：**1.44 exaFLOPS (FP4)**。
*   **机架布局 (42U)**：
    *   18 个计算托盘（每个含 2 个 Grace Blackwell Superchip）。
    *   9 个 NVSwitch 交换机托盘。
    *   电源与管理：4 个 33kW 电源托盘（上下各2），2 个 IPMI 托盘。
*   **系统内存总量**：
    *   **NVL72 标准版**：~13.5 TB HBM + ~18 TB DDR (总计 ~30 TB)。
    *   **NVL72 Ultra (B300版)**：~20.7 TB HBM + ~18 TB DDR。

### 5. 功耗、散热与物理规格
*   **功耗**：
    *   **整机架功耗**：约 **120 - 132 kW**。
    *   **计算节点**：单节点（2个Superchip）约 6 kW；18 个节点共 ~110 kW。
    *   **基础设施**：交换机、散热泵等约 20 kW。
*   **散热 (液冷)**：
    *   散热需求约 130 kW。
    *   流量需求：温升 10–12°C 时，需 **150–200 升/分钟**。
*   **重量**：
    *   满载（含冷却液）重量约 **3,000 磅 (1.3 – 1.4 吨)**。



第三章
1. 


















