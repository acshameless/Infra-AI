**译境** 已就位。

这部作品是技术与激情的结合，不仅探讨了冷冰冰的芯片与代码，更融入了作者对“机械共鸣”（Mechanical Sympathy）的深切感悟。译文将力求保留这种从底层硬件直达上层应用的通透感，以及作者字里行间流露出的工程浪漫。

以下是 **《AI 系统性能工程》** 核心内容的中文译本。

---

# AI 系统性能工程
**——基于 GPU、CUDA 和 PyTorch 优化模型训练与推理负载**

**作者：** Chris Fregly
**出版社：** O'Reilly

---

### 业界赞誉

> “AI 系统正处于极速狂飙的研发浪潮中，充斥着层层叠叠的复杂性。Chris 抽丝剥茧，将这些复杂性拆解得通俗易懂，为我们提供了一部**确立未来数年行业标准的权威指南**。”
> —— **Chris Lattner**，Modular CEO

> “理解大语言模型（LLMs）的原理仅仅是开始。能否通过模型与硬件的优化榨干其性能，才是平庸与卓越的分水岭。Chris 的这本书，正是那本教你如何跨越鸿沟、**缺失已久的实战手册**。”
> —— **Sebastian Raschka 博士**，ML/AI 研究员，畅销书《Build a Large Language Model from Scratch》作者

> “这本书的广度令人惊叹——CUDA 内核、分布式训练、PyTorch 编译器内部原理、分离式推理（Disaggregated Inference），尽收眼底。Chris 将那些通常散落在无数博客和论文中的碎片化知识整合在了一起。这简直是**机器学习系统的百科全书**。”
> —— **Mark Saroufim**，Meta PyTorch 工程师，GPU MODE 社区创始人

> “Chris 大师般地**从底层芯片到上层应用，完美串联起全栈知识图谱**，提供了每一位 AI 工程师将原始算力转化为高效、高性能模型所需的智慧。”
> —— **Harsh Banwait**，Coreweave 产品总监

---

### 前言 (Preface)

在旧金山充满活力的街头，创新之风盛行，恰如 101 国道上川流不息的自动驾驶车流，随处可见。我们置身于一个令人惊叹的人工智能世界。AI 的飞速发展正从各个维度重塑我们的日常生活。过去二十年，我们经历了推荐引擎（2000 年代）、AI 助手（2010 年代）和全自动驾驶（2020 年代）的洗礼。而 2030 年代将更加令人心潮澎湃，AI正以极快的速度演进，并产生巨大的社会影响力。

我投身于瞬息万变的 AI 系统性能工程领域，源于一种强烈的好奇心：我想探究那些驱动复杂系统和关键应用背后的力量——即尖端硬件、极致优化的软件与精妙算法之间，那种**微妙的平衡与协同设计 (Codesign)**。这种顿悟驱使我潜入“全栈” AI 性能工程的深海。我渴望理解处理器、内存架构、网络互连、操作系统以及软件框架等多个组件是如何和谐共奏的。这些交互的复杂性带来了挑战，也孕育了机遇，正是它们点燃了我深入探索这一独特技术组合的热情。

这本书，便是我多年来作为一名一线 ML 与 AI 性能工程师探索历程的结晶。

无论是工程师、研究员、从业者，还是渴望理解 AI 系统底层性能的狂热爱好者，这本书都为你而写。无论你是在构建 AI 应用，优化神经网络训练策略，还是设计管理可扩展的推理服务器，亦或仅仅是对现代 AI 系统的运作机制着迷，本书都将为你**架起一座连接多学科理论与实践的桥梁**。

本书的读者通常已具备神经网络的基础知识，并熟悉 Python 和机器学习。然而，即便没有这些基础，好奇的读者依然可以跟随书中关于硬件、软件和算法的第一性原理，踏上这场多维度的协同设计性能之旅。我承诺，任何类型的读者都能在本书中找到共鸣——我也保证，每一位读者都能在这些书页间通过学习获得新知。

贯穿全书，我们将审视硬件架构的演进，深潜入软件优化的细微之处，并剖析真实案例，以揭示构建高性能且低成本 AI 系统的模式与最佳实践。

---

### 致谢 (Acknowledgments)

虽由我一人执笔，但这本著作实则建立在无数致力于此领域的从业者和学者的心血之上。我对那些通过研究与创新为本书议题铺平道路的杰出人士深表亏欠。

特别感谢 Meta 的 Mark Saroufim。他和他的 **GPU Mode 社区**让我意识到，业界急需这种涵盖 PyTorch、CUDA 和 GPU 软硬件协同设计的深度技术内容。我深知维护和发展这样一个探讨前沿话题的大型社区需要付出多少心血。

感谢我的至亲（Ann Marie, Meredith, Tommy, 和 Conor），是你们持续的支持，陪伴我度过那些通宵达旦、笔耕不辍的日夜。如果过去一年我因为躲在地下室研究、写代码和写作而在探望你们时显得有些“心不在焉”，请接受我诚挚的歉意！

此外，以一种略显独特的方式，我想感谢我的爱车——**“小红帽” (Little Red Whiting Hood)**（图 P-1）。

你是一套集成了高度调优硬件与软件的复杂多学科系统。自从我十五年前开启这段旧金山之旅，你就一直陪伴着我。每一次驾驶，你都在提醒我为何要成为一名工程师。

**若论 GPU 代际，你便是 NVIDIA Ampere A100——那个时代的性能巅峰，永恒的经典，由跨学科工程师运用当时最顶尖的组件打造而成。**

虽然你原生设计为风冷 (Luftkühlung)，但你深知，唯有水冷 (Wasserkühlung) 方能释放极致性能。你的创造者顽固而不懈地推动着物理工程的极限，一代又一代。你的涡轮增压器就像 Tensor Cores（张量核心），当被推向极限时，既情绪化又火热。我永远不会离你而去。

正如 Phish 乐队经典歌曲《Contact》中的歌词所唱：

> *十一月的一个清晨我醒来*
> *忽然意识到我深爱着你*
> *爱的不是你车前的灯光*
> *不是你的排气管，也不是头顶的天窗*
> *而是当狂风试图推搡时*
> *你紧贴路面的倔强模样*
> *我绝不会独自远行*
> *归途若无你，何以为家*

谢谢你鼓励我拥抱**机械共鸣 (Mechanical Sympathy)**，小红帽！

---
*(图 P-1：一辆高性能、充满机械共鸣的保时捷 911 Turbo 的仪表盘，名为“小红帽”)*

随着我们继续拓展人工智能和超级计算的疆界，本书旨在通过知识来启发、教育并赋能每一位读者。这不仅是一场技术探索，更是对人类智慧的提醒——我们需要理解周遭的世界，并渴望通过技术创新不断进步。

世界上只有极少数人能真正通过软硬件与算法的协同设计，实现性能与效率的极致最大化。

**读完这本书，你将成为其中一员！**

**Chris Fregly**
加利福尼亚州，旧金山







**译境** 再次运转。

第一章是全书的基石，设定了从“暴力计算”转向“精细化工程”的宏大背景。作者以 DeepSeek 在 2024 年末的崛起为引子，极具时代感地阐述了性能工程的紧迫性。

译文将保持技术术语的精准，同时还原作者那种“透过现象看本质”的工程洞察力，特别是对于“机械共鸣”和“有效吞吐量（Goodput）”等核心概念的诠释。

以下是 **第一章：引言与 AI 系统概览** 的中文译本。

---

# 第一章：引言与 AI 系统概览

2024 年末，一家名为 **DeepSeek.AI** 的中国小型初创公司震惊了整个 AI 社区。他们在无法获取当时最新、最先进 NVIDIA GPU 的情况下，成功训练出了一个**前沿级 (Frontier)** 的大语言模型 (LLM)。受限于出口管制，DeepSeek 的工程师们无法获得顶级的 NVIDIA Blackwell (B200, B300 等) 或 Hopper (H100, H200 等) GPU，只能退而求其次，使用当时当地合规的替代品，包括 NVIDIA H800 GPU。他们通过编写自定义内核 (Kernels) 和采用模型蒸馏 (Model Distillation) 等先进优化技术，从这些性能受限的 GPU 中**“压榨”**出了极限性能。

尽管面临重重限制，DeepSeek.AI 训练出的 DeepSeek-R1 模型，其推理能力依然逼近了那些在最强 NVIDIA 芯片上训练出来的领先前沿模型。这一案例有力地证明了：**精通 AI 系统性能工程的从业者和研究人员，无论面对何种约束，都能最大程度地发挥硬件的潜力。**

例如，DeepSeek 的工程师将**通信带宽**视为一种**稀缺资源**，对线路上传输的每一个字节都进行了极致优化，从而在那样的基础设施上实现了许多人认为不可能的壮举。他们利用新颖的软件和算法优化克服了互连带宽受限的短板，将成千上万个此类受限 GPU 扩展为一个庞大的集群。

将 DeepSeek 的方法与美国和欧洲大型 AI 前沿实验室所采取的“暴力破解” (Brute Force) 路径进行对比，你会发现鲜明的差异。这些实验室继续追求更大的计算集群和更大的模型。模型规模已经从百万级爆炸式增长到十亿级，乃至现在的万亿级参数。虽然规模每扩大 10 倍都能解锁质的飞跃，但也伴随着巨大的成本和资源消耗。

例如，据报道，训练 OpenAI 的 GPT-4 (2023) 估算成本约为 1 亿美元，而训练 Google 的 Gemini Ultra (2023 年末) 估算成本更是高达惊人的 1.91 亿美元。这表明，随着模型在规模和成本上的不断攀升，未来的发展亟需资源效率的提升。

DeepSeek 声称，他们训练 DeepSeek-R1 模型的算力成本不到 **600 万美元**——比 GPT-4 和 Gemini Ultra 等模型低了一个数量级。与此同时，DeepSeek-R1 的性能却足以媲美那些成本高出数倍的竞争对手。

虽然外界对“600 万美元”这一说法的真实性仍存疑虑——比如它具体包含了什么（如仅指单次训练运行）或排除了什么（如实验和模型开发流水线）——但这则公告曾短暂地震动了美国金融市场，甚至导致 NVIDIA 股价单日**下跌约 17%**。这主要是因为市场担心 DeepSeek 的效率创新可能意味着未来对 NVIDIA 硬件的需求会减少。虽然这种市场反应有些过度——NVIDIA 股价在随后的交易日中也得以收复——但这足以证明，**AI 效率的突破对全球金融市场具有重大的财务影响力**。

除了模型训练，DeepSeek 还宣称，通过对支撑大多数现代前沿 LLM 的 **Transformer 架构**进行硬件感知的算法改进，他们在推理效率上也取得了显著收益。DeepSeek 清楚地表明，巧妙的 AI 系统性能工程优化可以彻底颠覆超大规模 AI 模型训练和推理的经济学逻辑。本书的其余部分将涵盖这些优化技术。

我们从中获得的最深刻的感悟是：在如此巨大的规模下，**从系统中挤出的每一滴性能，都可能转化为数百万甚至数十亿美元的成本节约。** 消除每一个瓶颈都能对训练吞吐量和推理延迟产生巨大的影响。这反过来又降低了成本，并提升了最终用户的整体满意度。简而言之，AI 系统性能工程不仅仅关乎速度——它关乎**将曾经的“不可能”变为“可能”且“可负担”**。

在**第一章**中，我们将深入探索 **AI 系统性能工程师**这一角色——在那个大规模人工智能时代，这已成为一个举足轻重的岗位。本章将作为一份综合指南，帮助你理解这一职业在现代 AI 系统中的多面责任及其关键影响。

我们首先追溯 AI 负载的演变，重点阐述从传统计算范式向现代 AI 应用需求的转变。这一背景将为我们理解 AI 领域为何需要专门的性能工程奠定基础。

随后，本章将深入探讨 AI 系统性能工程师所需的核心竞争力。我们将审视该角色必备的技术专长，包括对硬件架构、软件优化技术及系统级集成的深刻理解。此外，我们还会讨论解决问题、沟通与协作等软技能的重要性，这对于驾驭 AI 项目的跨学科特性至关重要。

本章很大一部分篇幅致力于该角色的实战层面。我们将探讨性能工程师如何分析系统瓶颈、实施优化策略，并确保 AI 系统的可扩展性和可靠性。我们会通过真实场景和案例研究来阐释这些概念，为读者提供该领域中遇到的挑战与解决方案的具体范例。

此外，我们还将讨论性能工程师常用的工具和方法论，提供关于性能测试、监控和基准测试实践的见解。这包括行业标准工具的概述，以及如何应用它们来评估和提升系统性能。

读完第一章，读者将对 AI 系统性能工程师的角色、胜任该职位所需的技能，以及性能工程在 AI 系统成功部署和运行中的至关重要性有一个透彻的理解。这些基础知识将为后续章节做好铺垫，届时我们将更深入地探讨那些定义了卓越 AI 性能工程的具体技术、科技和最佳实践。

---

## AI 系统性能工程师 (The AI Systems Performance Engineer)

AI 系统性能工程师是一个专门致力于优化 AI 模型及其底层运行系统性能的角色。这些工程师确保 AI 训练和推理流水线不仅快速、低成本、高性能，而且可靠且高可用。随着规模的增长，AI 系统性能工程师的重要性愈发凸显。

AI 系统性能工程师通常拥有顶薪——且物有所值。我们的工作直接影响公司的底线（利润）。我们需要融合硬件、软件和算法的专业知识。我们必须理解底层操作系统的考量、内存层级、网络基础，并掌握多种编程语言（如 Python 和 C++），以及不同的 AI 框架和库，如 PyTorch、OpenAI 的 Triton 和 NVIDIA 的 CUDA (Compute Unified Device Architecture)。

在日常工作中，一位 AI 系统性能工程师可能会检查底层 GPU 内核的效率、优化操作系统线程调度、分析内存访问模式、提升网络吞吐效率，或调试分布式训练算法。AI 系统性能工程师的关键职责包括基准测试、性能分析 (Profiling)、调试、优化、扩展以及高效地管理资源。虽然性能工程师可能专精于硬件、软件或算法的某种组合，但关键在于这些专业领域需要被**协同设计 (Codesigned)**（见图 1-1）。因此，理解它们之间的权衡及其相互影响大有裨益。

*(图 1-1：硬件、软件与算法的协同设计)*

### 基准测试与性能分析 (Benchmarking and Profiling)

基准测试和性能分析涉及在各种负载（包括训练和推理）下测量 AI 模型的延迟、吞吐量、内存使用率和其他性能指标。为了识别瓶颈，我们必须迭代地结合使用 **NVIDIA Nsight Systems**、**NVIDIA Nsight Compute** 以及 **PyTorch Profiler**。这些工具的组合能帮助我们定位瓶颈，并在我们持续改进 AI 系统整体性能的过程中，追踪不同层级栈的性能变化。

> **提示：** 建立自动化的性能测试非常重要，以便在开发周期的早期捕捉到“退化”（Regressions），即性能的下降。

### 调试与优化 (Debugging and Optimization)

调试和优化性能问题要求我们将性能问题追溯到其根本原因，无论是一个次优的 CUDA 内核、不必要的通信开销，还是训练或推理负载的不平衡。

在某种情况下，我们可能希望使用更高效的矩阵运算，以利用最新的 NVIDIA **Transformer Engine (TE)** 硬件，该硬件专门针对使用 Transformer 架构的现代 LLM 进行了优化。在另一种情况下，我们可以通过为“尴尬并行” (Embarrassingly Parallel) 的推理负载配置更高程度的并行性来改进软件框架。而在又一种情况下，我们可能会尝试改进 Transformer 的**注意力算法 (Attention Algorithm)**，通过实现更好的内存管理，降低相对于 GPU 计算量而言进出 GPU 显存的数据搬运量。

即使是微小的代码调整也能产生巨大的收益。例如，也许一个用 Python 编写的数据预处理步骤正拖慢整个训练流水线。你可以通过用 C++ 重写该代码来消除瓶颈，或者使用 **NVIDIA cuPyNumeric**——一个 NumPy 的直接替换品，它可以将数组运算负载分配到 CPU 和 GPU 上。

### 扩展分布式训练与推理 (Scaling Distributed Training and Inference)

将小型研究负载扩展到超大规模集群上的生产负载，确保当我们从 8 个 GPU 扩展到 80,000 个 GPU 时，系统能以最小的开销和效率损失进行扩展。这需要使用 **NVIDIA NCCL** (NVIDIA Collective Communications Library，发音为 "nickel") 来优化通信，处理训练运行中常见的 All-Reduce 等分布式集合通信。此外，**NVIDIA Inference Xfer Library (NIXL)** 为分布式推理提供了跨 GPU 内存和存储层的高吞吐、低延迟的点对点数据移动。这种通信既可以在单个节点的 GPU 之间进行，也可以跨越数千个节点。它们还优化了 All-Reduce、All-to-All 和 All-Gather 等集合聚合操作，这些操作在模型训练和推理中被广泛使用。

你可能想利用数据并行、张量并行和流水线并行，巧妙地将数据放置在不同节点上。或者，你可能需要重新设计负载以使用张量并行或流水线并行，因为模型太大，无法放入单个 GPU 中。也许你正在使用**混合专家 (Mixture of Experts, MoE)** 模型，并能利用专家并行 (Expert Parallelism)。

### 高效管理资源 (Managing Resources Efficiently)

优化模型如何利用 CPU 核心、GPU 内存、互连带宽和存储 I/O 至关重要。这可能涉及许多工作，例如确保 GPU 满负荷获取数据、将线程绑定到特定的 CPU 核心、减少上下文切换开销、编排内存使用，以及在训练和推理大模型时避免 GPU 出现内存溢出 (OOM) 错误。像 GPU 虚拟化（例如 **NVIDIA 的多实例 GPU [MIG]**）这样的技术可以对 GPU 资源进行分区，以便在作业不需要全部 GPU 算力时实现更好的整体利用率。

### 跨团队协作 (Cross-Team Collaboration)

跨团队协作对于 AI 系统性能工程师来说绝对是关键。与研究人员、数据科学家、应用开发人员以及包括网络和存储在内的基础设施团队携手合作非常重要。

提升性能可能需要修改模型代码，这涉及与研究人员的协调。或者你可能想部署一个新的 GPU 驱动程序以提高效率，这需要基础设施团队的支持。

通常，性能改进会跨越多个团队。例如，更新 CUDA 驱动或版本以修复 Bug 和提升效率，将涉及与 DevOps、基础设施和支持团队的仔细协调。而改进模型代码以提升性能则涉及与研究人员的紧密合作。性能工程师处于这些多学科领域的交汇点，说着 AI、计算机科学和系统工程的通用语言。

### 透明度与可复现性 (Transparency and Reproducibility)

在性能工程中，至关重要的是**测量一切并相信数据**，而不是假设。通过发布你的工作，其他人可以学习、复现并在你的发现基础上继续构建。

DeepSeek 故事中一个值得注意的方面是他们分享基础设施优化的开放程度。在 2025 年 2 月的 DeepSeek **“开源周 (Open-Source Week)”** 期间，他们发布了一套开源 GitHub 仓库，包括 **FlashMLA**、**DeepGEMM**、**DeepEP**、**专家并行负载均衡器 (EPLB)**、**DualPipe** 和 **Fire-Flyer 文件系统 (3FS)**。每个项目都经过生产环境验证，旨在从硬件中挤出最大性能。这些项目在《DeepSeek-V3 技术报告》中均有描述。

*   **FlashMLA** 是他们用 CUDA C++ 编写的优化版注意力内核。
*   **DeepGEMM** 提供了一个 FP8 优化的矩阵乘法库，据报道在稠密和稀疏运算上均优于许多厂商提供的内核。
*   **Deep Experts Parallelism (DeepEP)** 是他们为混合专家 (MoE) 模型高度调优的通信库。
*   **EPLB** 实现了一种冗余专家策略，通过复制负载过重的专家来处理额外负载。
*   **DualPipe** 是一种双向流水线并行算法，它重叠了前向/后向计算与通信阶段，以减少流水线气泡。
*   **3FS** 是他们的高性能分布式文件系统，提醒我们每一层都需要优化——包括文件系统——才能从 AI 系统中获得最大性能。

通过在 2025 年 2 月的“开源周”期间将这些项目在 GitHub 上开源，DeepSeek 不仅通过允许他人复现其结果证明了其主张的可信度，还回馈了社区。这种透明度允许其他开发者对他们的方法进行基准测试、复现和学习，包括使用 DeepEP/DualPipe 流水线并行重叠通信，以及使用 3FS 跑满 NVMe SSD/RDMA 带宽。

像 DeepSeek 的 **Open Infra Index** 这样的开放项目提供了宝贵的基准线和工具。它们提供了各种 AI 硬件设置上的真实性能测量值，并鼓励“苹果对苹果”（同类）的比较和可复现性。同样，**MLPerf** 开放基准测试套件也为跨硬件和软件设置可复现地比较训练和推理性能提供了标准。

行业基准测试如 MLPerf 已经量化了这种跨硬件代际的协同设计优化。在 MLPerf Training v5.0 (2025) 中，基于 Blackwell 的 **NVIDIA GB200 NVL72** 系统在每 GPU 训练吞吐量上比同等的 Hopper 系统高出达 **2.6 倍**，如图 1-2 所示。在 MLPerf Inference v5.0 (2025) 中，由于更高的单 GPU 性能和更大的 NVLink 域，Blackwell NVL72 在每 GPU 推理吞吐量上比同等的 Hopper 集群高出约 **3.4 倍**。结果显示在图 1-3 中。

*(图 1-2 & 图 1-3: NVIDIA GB200 NVL72 在 MLPerf Training/Inference v5.0 中相对于同等 Hopper 集群的吞吐量提升)*

在后续章节中，我们将引用其中一些开放基准测试来支持各种性能调优概念。例如，在讨论 GPU 内核优化时，我们将参考 DeepSeek 发布的分析数据，展示他们的自定义内核如何在 NVIDIA GPU 上实现接近峰值的内存带宽利用率。

> **注意：** 当我们引用 MLPerf 结果时，请记住 MLPerf 警告说**单 GPU (per-GPU)** 结果并非跨平台比较的主要指标。应使用系统级、端到端的吞吐量作为比较的正确基准。单 GPU 数据仅作为组件级指标参考。

实验的透明度和可复现性对于推动 AI 性能工程领域的发展至关重要。人们很容易陷入轶事般的“感觉流”优化陷阱（“我们做了 X，感觉变快了”）。相反，我提倡一种严谨、科学的方法：提出假设，通过可复现的基准测试测量结果，调整以改进结果，重新运行基准测试，并在每一步分享所有结果。

---

## 尽管受美国出口硬件限制，DeepSeek 在中国仍扩展至约 6800 亿参数模型

有时，系统性能的创新源于必要性。如前所述，由于美国的出口限制，DeepSeek 发现自己只能使用 NVIDIA 的 H800 GPU。H800 是 Hopper GPU 的出口合规变体。与 H100 相比，它降低了 **NVLink 互连带宽**和 FP64 性能，但保留了基本相似的 HBM 容量和带宽。

作为背景参考，一个 NVIDIA H100 每 GPU 提供约 900 GB/s 的 NVLink 互连带宽，而 H800 每 GPU 仅提供约 400 GB/s。这使得 GPU 间的数据传输变慢，最终限制了多 GPU 的可扩展性。此外，虽然 H100 提供 3.35 TB/s 的内存带宽，但 H800 受限的吞吐量意味着数据传输要慢得多。这威胁到了分布式训练作业的瓶颈，并限制了扩展效率。

DeepSeek 致力于在这个受到严格约束的环境中训练一个大规模的、约 6800 亿参数的 MoE 语言模型，名为 **DeepSeek-V3**。这个 MoE 模型每个输入 Token 仅使用约 370 亿个活跃参数——而不是一次性使用全部 6800 亿个。

通过这种架构，任何给定时间只有一小部分模型被激活。这有助于管理计算负载——即使是在紧凑的 H800 设置下。具体来说，对于每个 Token，DeepSeek-V3 使用 **1 个共享专家 (Shared Expert)** 加上 **8 个路由选定的专家 (Router-selected Experts)**（从 256 个专家中选出），共计 **9 个活跃专家**，如图 1-4 所示。

我们将在后续章节更详细地介绍 MoE。但现在只需知道，为了绕过环境限制，DeepSeek 实施了一种新颖的 **DualPipe** 并行算法，该算法精心重叠了计算和通信，以掩盖 H800 固有的弱点。

通过设计自定义 CUDA 内核来绕过一些默认的 NCCL 通信集合，DeepSeek 能够协调数据传输与正在进行的计算同步进行。这使得 GPU 即使在互连带宽减少的情况下也能保持高效利用。这种**通信/计算重叠 (Communication/Computation Overlap)** 是我们在本书其余部分将反复重申的主题。

这种创新工程获得了回报，DeepSeek-V3 的训练完成所用的 GPU 时间（和成本）仅为 OpenAI、Meta、DeepMind 等公司同等规模前沿模型的一小部分。这只是许多人认为使用更强集群训练此规模模型所需资源的一小部分。

*(图 1-4：DeepSeek-V3 的专家路由)*

DeepSeek 报告称，V3 的性能在多项标准化基准测试中接近 GPT-4，包括语言理解、阅读理解和推理。在某些测试中，它甚至持平或略微超过 GPT-4。这些比较是基于全行业通用的标准化测试进行的。这意味着一个开放的 MoE 模型可以与最好的闭源模型相抗衡——尽管使用的是较差的硬件。

在 DeepSeek-V3 模型的基础上，该团队随后创建了 **DeepSeek-R1**，这是其专门的推理模型，构建方式类似于 OpenAI 的 o1 和 o3 推理模型系列。DeepSeek 没有严重依赖昂贵的人类反馈循环进行微调，而是开创了一种“冷启动”策略，使用最少的监督数据，转而强调**强化学习**技术，将**思维链 (Chain-of-Thought)** 推理直接嵌入 R1 中。这种方法减少了训练成本和时间，并强调了**智能的软件和算法设计可以克服硬件瓶颈**。

吸取的教训是，即使在有限的内存和计算预算下，大型、稀疏激活的 MoE 模型也可以被有效地扩展。正如 DeepSeek 努力带来的巨大投资回报率 (ROI) 所证明的那样，新颖的训练调度和底层的通信/计算重叠优化可以克服硬件限制。

ROI 是显而易见的，DeepSeek 的非传统方法带来了巨大的效率，并以更低的训练成本和时间创造了一系列强大的模型。通过从 NVIDIA H800 GPU 中榨取每一盎司的性能——即使在通信带宽减少的限制下——该团队以数百万美元更低的成本交付了 GPT-4 级别的模型性能。此外，DeepSeek 在 R1 的推理微调阶段不需要那么多的人工标注数据，从而节省了更多资金。

简而言之，**智能软件和算法设计战胜了暴力硬件堆砌的局限**。这使得 DeepSeek 能够在紧张的成本和硬件预算下开发大规模 AI 模型。

---

## 迈向 100 万亿参数模型 (Toward 100-Trillion-Parameter Models)

100 万亿参数模型是 AI 的一个令人向往的里程碑，经常被拿来与人类大脑新皮层中估计的 100 万亿个突触连接进行比较。每个突触连接相当于一个模型参数。实现这种规模的模型在理论上是可能的，但它需要极其庞大的资源——和资金。对于除了绝对最富有的组织之外的所有人来说，通过暴力手段扩展到 100 万亿参数模型是不切实际的。

作为一个粗略的数量级估计，在一个约 29 万亿 Token 上训练的密集型 100 万亿参数模型将需要 $1.2 \times 10^{29}$ 次浮点运算 (FLOPS)。更大的 Token 数量会线性扩展这一需求，虽然使用稀疏 (MoE) 模型可以减少有效计算量，但这表明，仅靠暴力手段不足以满足超大规模模型的计算需求。我们需要新的方法，以便在合理的时间内使 100 万亿参数的训练变得可行。实现这种规模需要**硬件、软件和算法协同设计方面的突破性效率**——而不仅仅是继续扩展当前的方法。

> 虽然本书讨论的优化可以应用于较小的模型和集群规模，但我将继续重提 100 万亿参数模型，以强化这一理念：我们不能仅仅通过向扩展问题“砸硬件”来解决问题。我们需要与硬件（包括计算、网络、内存和存储）协同设计的巧妙软件和算法创新。

训练成本的爆炸式增长正在推动人们寻找新的 AI 系统和软件工程技术，以提高性能、降低成本，并在有限的计算资源、功率限制和资金下使极端规模的 AI 成为可能。研究人员一直在探索新技术以减少有效计算需求。

一个突出的想法是使用**稀疏性 (Sparsity)**，具体来说是 MoE 模型。像 MoE 这样的稀疏模型与传统的**密集模型 (Dense Models)** 形成对比，后者如 OpenAI 普及的常见 GPT 系列大语言模型 (LLM)。事实上，公众猜测一些前沿模型，如 OpenAI 专有的 GPT 系列和 o 系列推理模型，正是基于 MoE 架构。

像 MoE 这样的稀疏模型对于每个输入 Token 只激活模型的一部分。通过将每个输入 Token 仅路由到其众多内部“专家”的一个子集，即使总参数量增长，每个 Token 的 FLOPS 也大致保持不变。这种稀疏模型证明，扩展到数万亿参数模型可以在不引起计算成本同等爆炸的情况下完成。此外，由于这些模型在推理期间使用较少的活跃参数，与同等的密集模型相比，MoE 模型的请求-响应延迟通常要低得多。这是朝着训练和服务 100 万亿参数规模模型迈出的关键一步。

DeepSeek-V3（基础模型）和 -R1（基于强化学习的推理变体）是 MoE 效率的绝佳范例。它们包含约 6800 亿总参数，每个输入 Token 约有 370 亿个活跃参数。DeepSeek 的技术报告和公开文章描述了一个 MoE，每个 Token 有 1 个共享专家和从 256 个专家中选出的 8 个特定专家，这产生了每个 Token 约 9 个活跃专家和约 370 亿活跃参数。这使得 DeepSeek-V3 和 -R1 比类似大小的密集大语言模型更加资源高效。另一个例子是 Google 2021 年的 **Switch Transformer** MoE。这个 1.6 万亿参数的 MoE 模型以仅一小部分的计算量达到了与密集模型相同的精度。它的训练速度比同类密集方法快 7 倍。

除了巨大的计算需求外，**内存**也是一个主要瓶颈。例如，一个 100 万亿参数的模型将需要大约 **182 TB 的 GPU 显存**（182 TB = 100 万亿参数 $\times$ 16 位/权重 $\times$ 8 位/字节，注：此处原书计算似有简略，意指按参数量估算存储）来加载模型（如果每个参数以 16 位/2 字节精度存储）。这比单个 NVIDIA Blackwell B200 GPU 上的 192 (180 可用) GB GPU RAM 高出 3 个数量级 (1,000 倍)。

仅仅加载 100 万亿模型权重就需要接近 1,000 个 Blackwell B200 GPU——或 700 个 Blackwell Ultra B300 GPU（每个约 288 GB）。而这个估算仅仅是为了加载模型，还没包括激活内存 (Activation Memory)、优化器状态 (Optimizer States) 和推理输入。这些将进一步增加所需的总内存。

作为背景，一个典型的 B200 GPU 计算节点仅包含 8 个 B200 GPU。要使用这些，你需要约 125 个 GPU 节点仅用于加载模型（B200），或者使用 Ultra B300 GPU 计算节点集群（每节点 8 个 B300）需要约 86 个 GPU 节点。

此外，加载训练数据也变得极其困难，因为以足够快的速度向该模型提供数据以保持所有这些 GPU 忙碌并非易事。特别是，当 100 万亿参数模型被分割到 1,000 个 B200 GPU 或 700 个 B300 GPU 上时，GPU 之间的**通信开销**会显著增长。训练单个模型可能会消耗数百万 GPU 小时和数兆瓦时的能量。要扩展到足够大以训练和服务一个 100 万亿参数模型，这将是一笔巨大的成本——和能源消耗。

100 万亿参数 AI 的时代将迫使我们要彻底重新思考系统设计，以使这种规模的训练和部署变得切实可行。硬件、算法和软件都需要共同进化以迎接这一新前沿。

---

## NVIDIA 的“机架式 AI 超级计算机” ("AI Supercomputer in a Rack")

为了迎接超大规模计算的挑战，NVIDIA 构建了一类专门针对万亿参数级工作负载的新型 AI 超级计算机。例子包括 2024 年的 NVIDIA Grace Blackwell **GB200 NVL72** 和 **GB300 NVL72 Ultra**，后者使用带有 288 GB HBM3e 的 Blackwell Ultra GPU，并保持 72-GPU NVLink 域，拥有约 130 TB/s 的聚合带宽。

Vera Rubin **VR200** (2026) 和 **Feynman** (2028) 系统延续了这种将百亿亿次级 (Exascale) 超级计算机压缩到单个数据中心机架的趋势。实际上，NVIDIA 将像 NVL72 这样的机架系统称为**“机架上的 AI 超级计算机”**——这名副其实。

每个 GB200/GB300 NVL72 机架集成了 36 个 Grace Blackwell 超级芯片，通过 NVLink 与 **NVSwitch** 连接，提供机架级交换结构。每个 Grace Blackwell 超级芯片是一个 NVIDIA Grace CPU（拥有 72 个 CPU 核心）和两个 NVIDIA Blackwell GPU 的组合，总计 36 个 Grace CPU 和 72 个 Blackwell GPU——因此得名“NVL72”中的“72”。

> 如果你还没猜到，NVL72 中的 NVL 代表 **NVLink**。这有助于提醒你，该系统中的 GPU 是使用 NVLink 互连的。以防我们忘记！

每个 Grace Blackwell 板卡通过 NVSwitch（机架内 NVLink 交换网络）连接到其他板卡。这样，所有 72 个 GPU 都能以每个 GPU 1.8 TB/s 的双向全速 **NVLink 5** 带宽（18 $\times$ 100 GB/s 链路）相互通信。在整个机架上，NVLink 交换系统在一个 NVL72 域内提供了约 130 TB/s 的聚合 GPU 对 GPU 带宽。

实际上，NVL72 的内部结构将所有 72 个 Grace Blackwell GPU 统一为一个高速集群。因此，像用于训练的 PyTorch 和用于推理的 vLLM 等框架可以利用这个单一的 NVLink 域进行高效的数据张量流水线和专家并行。**CUDA 统一内存 (Unified Memory)** 可以在需要时通过 NVLink 迁移或远程访问页面。然而，远程内存具有不同的延迟和带宽，应被视为非一致的 (Non-uniform)。当依赖 Grace 和 Blackwell 之间的托管分配时，首选使用 `cudaMemPrefetchAsync` 和 `cudaMemAdvise` 进行显式预取，以减少缺页中断停顿。

关于这台超级计算机的计算、内存和互连硬件的更多细节将在后续章节提供。现在，让我们在现代生成式 AI 模型的背景下分析这台 AI 超级计算机的整体性能规格。

一个完整的 GB200 NVL72 机架在 2:1 结构化稀疏下，理论上可以达到约 **1.44 exaFLOPS** (FP4) 和约 **720 petaFLOPS** (FP8)。它在 72 个 GPU 上提供了大致 13.5 TB (13,824 GB = 192 GB/GPU $\times$ 72) 的 HBM3e 内存，算上同一 NVLink 域中的 Grace CPU 内存时，总计约 30 TB。

NVLink 提供了跨 72 个 GPU 的池化访问，CUDA 统一内存可以通过 NVLink 迁移或远程访问页面，但远程访问具有不同的性能，且该机架并不表现为单一的一致内存设备。

简而言之，GB200 NVL72 是一个自包含的 72 GPU、1.44 exaFLOPS、30 TB 内存系统，机架功率约为 120 到 132 kW（取决于供应商配置和冷却）。它确实是一台可以在单个机架内训练和服务多万亿参数模型的 AI 超级计算机。

通过组合这些机架形成超大规模集群，你可以支持巨大的多万亿参数模型。更好的是，你可以使用你喜欢的云提供商（包括 AWS, GCP, Azure, CoreWeave, Lambda Labs 等）点击几下（当然也要花不少钱！），就能配置这些机架和机架集群。

> 虽然本书重点关注 NVIDIA Grace Blackwell 这一代芯片，但讨论的优化原则源自 NVIDIA 许多前代硬件。这些优化将继续适用并演进到未来的许多 NVIDIA 芯片代际，包括 Vera Rubin (2026)、Feynman (2028) 及以后。这一路线图延续了每一代新 GPU 性能、内存和集成度翻倍的模式。

纵观全书，你将了解到每一代在计算、内存、网络和存储方面的创新如何以超大规模集群、多万亿参数模型、高吞吐训练作业和极低延迟模型推理服务器的形式，促进 AI 的进一步扩展。这些创新由**硬件感知算法**所驱动，这些算法严格执行下一节讨论的**机械共鸣**和**硬件-软件协同设计**原则。

---

## 机械共鸣：硬件-软件协同设计 (Mechanical Sympathy: Hardware-Software Codesign)

**机械共鸣 (Mechanical Sympathy)** 这个术语最初由软件工程师 Martin Thompson 提出，他以此类比英国一级方程式赛车冠军 **Jackie Stewart**，后者对他的赛车机械细节有着深刻的理解。在计算领域，它指的是编写对其运行硬件有深刻认知的软件。在 AI 语境下，它意味着算法与硬件能力的协同设计，以最大化性能。

现实世界的经验表明，即使是 GPU 内核或内存访问模式中的微小调整也能产生巨大的收益。一个经典的例子是 **FlashAttention**，这是一种以硬件感知方式重新实现 Transformer 注意力机制的新颖算法。

FlashAttention 对 GPU 计算进行“分块 (Tiling)”，最小化了对 GPU 内存的读写次数。FlashAttention 显著减少了内存移动并加速了注意力计算。用 FlashAttention 替换默认的 Transformer 注意力机制/算法，在长序列的训练和推理中产生了 **2倍–4倍** 的加速，同时也减少了整体内存占用。

这种改变将曾经的主要瓶颈（注意力）减少到仅占整体运行时间的一小部分。FlashAttention 几乎在一夜之间成为许多库的默认设置，因为它让模型能更快、更高效地处理更长的序列。自 FlashAttention 以来，许多新的注意力算法涌现出来，包括 DeepSeek 的 **Multi-Head Latent Attention (MLA)**。

DeepSeek 的 MLA 算法——作为 NVIDIA GPU 内核实现并于 2025 年开源——是硬件-软件协同设计或机械共鸣的另一个例子。与 FlashAttention 类似，MLA 重构了注意力计算，以更好地利用 NVIDIA 的内存层级结构和专用的 GPU **“Tensor Cores”**。这些优化使得 MLA 能够利用受限的 H800 GPU 架构，以一小部分的成本实现更高的吞吐量——甚至在那些 H800 系统上超过了 FlashAttention 的性能。

这就好比本书实际上就是一项关于机械共鸣的研究。我们将看到无数案例，其中新的硬件特性——或者像 DeepSeek 案例中的硬件限制——激发了新颖的软件和算法技术。反过来，我们也会看到新的软件算法如何鼓励新的硬件创新。

例如，Transformer 模型的兴起和低精度量化（如 FP8/FP4）导致 NVIDIA 添加了专门的硬件，如 Transformer Engine 和专用的低精度 Tensor Cores 以加快矩阵数学计算单元。这些硬件创新反过来又使研究人员能够探索新颖的数值优化器和神经网络架构。这进而推动硬件设计者走得更远，从而解锁更新的算法，以此类推。这是一个良性循环！

现代 GPU 使用支持 FP4 和微缩放 (Microscaling) 的最新 Transformer Engine。结合最新一代 NVLink，这些特性主要通过更快的 Tensor Core 数学运算、更高的内存带宽和改进的**特殊函数单元 (SFU)** 来增加注意力吞吐量。例如，**指数计算单元**是专门设计用来加速 Transformer 注意力算法中大量使用的 Softmax 操作——这是当今 LLM 模型的关键部分。随着 Transformer 的普及，Softmax 延迟已成为前代 GPU 的瓶颈，因为它对 Transformer 注意力机制至关重要。

> 通过提高现代 GPU 的特殊函数吞吐量和快速数学流水线，NVIDIA 展示了最纯粹、最好的机械共鸣和硬件-软件-算法协同设计。他们直接与研究人员和从业者合作，解决和减少在其硬件上运行的现代 LLM 算法（例如注意力）的瓶颈。

这种紧密的相互作用——GPU 和 AI 算法的共同进化——是 AI 机械共鸣的核心。这些协同设计创新只有在硬件公司（如 NVIDIA 和 ARM）、AI 研究实验室（如 OpenAI 和 Anthropic）以及 AI 系统性能工程师（如我们！）之间的紧密合作下才能发生。

---

## 测量“Goodput”：有效吞吐量 (Measuring "Goodput" Useful Throughput)

当运行包含数百、数千或数百万个 GPU 的集群时，重要的是要理解有多少理论硬件能力实际上在执行**有用**的工作。传统的吞吐量指标如 FLOPS 和设备利用率具有误导性，因为大部分时间可能花在了停滞的通信、空闲的计算或失败的作业重启上。这就引入了 **“Goodput” (有效吞吐量)** 的概念——Meta 在 2025 年的一篇论文中将其描述为“有效训练时间比率 (effective training-time ratio)”。

> NVIDIA 将理论硬件最大值称为**光速 (speed of light)**，正如你可能在 NVIDIA 的博客、文档、网络研讨会和会议演讲中看到的那样。

简单来说，Goodput 测量的是单位时间内完成的**有用工作**的吞吐量（处理的 Token 数或完成的推理请求数）——扣除了所有不直接贡献于模型训练或推理的内容。它实际上是从生产性模型训练或推理的角度来看的系统端到端效率。Goodput 可以通过集群的最大可能吞吐量进行归一化，得出一个百分比效率值。

例如，假设一个拥有 8 个 GPU 的节点可以在 10 秒内处理 100,000 个 Token。在这种情况下，其 Goodput 是每秒 10,000 个 Token。如果节点中的每个 GPU 都能达到每秒 1,500 个 Token 的峰值理论吞吐量，即 8 个 GPU 总共每秒 12,000 个 Token，那么该节点的效率为 **83.3%** (0.833 = 10,000 实现吞吐量 / 12,000 峰值吞吐量)。

Meta 的 AI 基础设施团队在“Revisiting Reliability”（重访可靠性）论文中强调了 Goodput 的重要性。该论文介绍了*有效训练时间比率*指标，并展示了即使在标题利用率很高的情况下，抢占、资源碎片化和故障是如何减少实际训练时间的。在这篇论文中，Meta 的团队分析了抢占、硬件故障和网络拥塞如何降低已实现的吞吐量。

换句话说，虽然集群看起来是 100% 被利用的，但 70%–75% 的算力因通信延迟、次优并行化、数据延迟或故障恢复等开销而丢失。此外，Meta 的分析表明，在大规模下，作业抢占、网络热点和不可恢复的故障等问题是造成 Goodput 损失的主要原因。

例如，想象一个训练作业，在理想硬件上理论上可以每秒处理 1,000 个样本，但由于输入流水线糟糕和过度的同步，它实际上只达到了每秒 300 个样本的训练吞吐量。我们会说这个作业以 **30% 的 Goodput** 运行。剩下的 70% 容量本质上是被浪费了。

识别这些差距并缩小它们是我们工作的核心部分。例如，如果 GPU 正在等待从存储加载数据，我们可能会引入缓存或异步预取。如果它们在模型训练过程的梯度同步步骤中空闲，我们可能希望将 GPU 计算（例如，计算梯度）与 GPU 之间的通信（例如，同步梯度）重叠。我们的目标是将浪费、低效的周期转化为有用的工作。

理论性能与实际实现性能之间的这种差距，正是 AI 系统性能工程师角色的价值主张所在。我们的使命是通过攻击栈的每一层（包括硬件、软件和算法）的低效和降低成本，将那个 Goodput 数字尽可能推高——理想情况下使其接近 100%。

> 对 AI 系统性能工程师的投资始终能产生远高于成本的回报。试想一位性能工程师帮助实现了 20% 的集群效率提升。在大型 AI 环境中，这可以减少数百万美元的硬件成本。

通过关注 Goodput，我们正在优化真正重要的东西——**每美元成本和每焦耳能量所完成的有用训练量**。Goodput 是成功的终极指标——比原始 FLOPS 或设备利用率更重要——因为它概括了硬件、软件和算法在实现“更快、更便宜地训练 AI 模型”这一最终目标上的协调程度。

提高 Goodput 需要对硬件（如 CPU、GPU、网络拓扑、内存层级、存储布局）、软件（如操作系统配置、分页内存、I/O 利用率）和算法（如 Transformer 架构变体、注意力机制替代方案以及不同的缓存和批处理策略）之间的交互有深刻的理解。

这种对多学科——包括硬件、软件和算法——的广泛而深刻的理解，正是 AI 系统性能工程师如今如此稀缺的原因。这也是我写这本书的原因！接下来是涵盖本书其余部分的路线图和方法论。

---

## 本书路线图与方法论 (Book Roadmap and Methodology)

我们将如何着手优化 100 万亿参数的 AI 系统？本书的组织结构旨在带你从硬件基础一直深入到软件栈和算法技术——重点在于每一层的动手分析。以下是本书其余部分的细分：

*   **第 2 章** 深入剖析 NVIDIA AI 系统硬件，包括 GB200/GB300 NVL72 “机架上的 AI 超级计算机”，它结合了 Grace Blackwell 超级芯片设计与 NVLink 网络，创造了 AI 超级计算机的性能/功耗特性。
*   **第 3–5 章** 涵盖基于 GPU 的 AI 系统的操作系统级、网络和存储优化。这些优化包括 CPU 和内存绑定 (Pinning)，以及 Docker 容器和 Kubernetes 编排考量，包括 GPU 环境的网络 I/O 和存储配置。
*   **第 6–12 章** 讨论 NVIDIA CUDA 编程基础和 CUDA 内核优化，这对于开发新颖的硬件感知算法至关重要。此类流行算法包括 FlashAttention 和 DeepSeek 的 MLA。这些算法针对的是 Transformer 架构中资源密集型的注意力机制，该机制主导了当今的生成式 AI 负载。
*   拥有 NVIDIA 硬件和 CUDA 软件背景后，我们将深入探讨**分布式通信优化**，包括高效地训练和服务超大模型。我们将研究最小化通信的策略，例如**计算与通信重叠**——这是一个适用于 AI 系统栈许多层的模式。
*   **第 13 和 14 章** 讨论 PyTorch 特定的优化，包括 PyTorch 编译器栈和 OpenAI 基于 Python 的 Triton 语言及自定义 GPU 内核编译器。这些编译器降低了开发新颖 CUDA 内核的门槛，因为它们不需要通常开发 CUDA 内核所需的深厚 C++ 理解。这些章节还讨论了模型训练的分布式并行技术，包括数据并行 (DP)、完全分片数据并行 (FSDP)、张量并行 (TP)、流水线并行 (PP)、上下文并行 (CP) 和混合专家 (MoE)。我们将展示如何高效地在许多 GPU 上拆分和训练多万亿参数模型。我们还将讨论超大规模模型训练期间的内存优化技术，包括激活检查点 (Activation Checkpointing)、分片优化器状态和卸载到更大的 CPU 内存。当模型大小超过物理 GPU 硬件限制时，这些技术至关重要。
*   **第 15–19 章** 聚焦于高吞吐、低延迟模型推理和代理 AI 系统 (Agentic AI) 的软件和算法创新。这包括**分离式预填充和解码 (Disaggregated Prefill and Decode)**，现在 NVIDIA Dynamo 和社区栈均支持通过 UCX 进行 Key-Value (KV) 缓存移动（利用 GPUDirect RDMA、NCCL 点对点或框架提供的传输）。我们还将讨论广泛使用的模型服务引擎，包括 **vLLM**、**SGLang** 和 **NVIDIA TensorRT LLM**。然后我们会介绍 NVIDIA Dynamo 分布式推理框架，它与这些引擎集成，并包含用于在分离式预填充-解码设置中进行低延迟 KV 缓存传输的 NIXL。
    我们还将通过案例研究，探讨利用 NVL72 中的 Grace CPU 进行预处理，共同运行较小的“草稿”模型以实现高性能推理算法（如**投机解码**），以及高效的请求路由和批处理以最大化推理系统的整体吞吐量。我们还将探索模型压缩和加速技术，如 **4-bit 量化**、**知识蒸馏**（教导较小的“学生”模型向更聪明的“老师”模型学习）、稀疏性和剪枝，以及使用专门的 TensorRT 内核。重点在于分离式预填充-解码和自适应技术，以便在运行时动态调优系统。
*   **第 20 章** 描述了使用 AI 辅助工具优化内核和 AI 系统性能的现代努力。本章还包括高效训练和服务数十亿及数万亿参数模型的案例研究。它还涵盖了自我改进 AI 系统优化和代理的新兴趋势。这有助于描绘 100 万亿参数级 AI 系统的发展方向——以及如何为 AI 系统性能工程的未来定位自己。
*   **附录** 提供了一份常见性能优化和成本节约技巧的清单，可应用于你自己的 AI 系统。这是本书讨论的可操作优化和效率收益的总结。

简而言之，本书实施了一种动手实践和经验主义的方法来应用性能优化。我们将频繁分析实际运行、案例研究、基准测试结果和分析数据，以理解瓶颈并验证改进。读完本书，你应该能掌握优化超大 AI 系统的原则——并获得一些使用工具的实践经验，将这些优化应用于超大规模、多 GPU、多节点和多机架 AI 系统，如 NVIDIA GB200/GB300 NVL72 机架式 AI 超级计算机——或现在及未来的类似 AI 系统。

---

## 关键要点 (Key Takeaways)

以下特质共同定义了 AI 系统性能工程师的角色，他们融合深厚技术知识与战略性、数据驱动优化的专长，将原始硬件转化为具有成本效益的高性能 AI 解决方案：

**测量 Goodput (有效吞吐量)**
> 不要只看原始 FLOPS 或利用率。相反，测量 GPU 执行有用工作（如前向/反向传播计算）的时间与等待数据或其他开销的时间之比。使用 NVIDIA Nsight Systems/Compute 或 PyTorch Profiler 来测量此比率。努力提高这一比率，因为 Goodput 关注的是有效的、有用的 GPU 利用率。

**首选巧妙的工程优化，而非暴力堆砌资金**
> 更多硬件并非银弹。巧妙的软件和系统优化可以在硬件受限时弥补差距，实现原本需要极其昂贵的基础设施才能达到的结果。我们在 DeepSeek 的成就中看到了这一点。通过优化通信和硬件使用，他们的工程师在受限的 H800 GPU（互连带宽有限）上以一小部分的成本训练出了前沿模型。DeepSeek 模型匹配了在更强大硬件上训练的前沿模型的性能。换句话说，巧妙的工程设计战胜了暴力消费。

**通过增量优化寻求数量级的影晌**
> 在大规模下，即使是很小百分比的效率提升也能节省数百万美元。换个说法，微小的低效，如冗余计算和缓慢的数据流水线，会随着系统扩展而悄无声息地增加巨额成本。

**以分析驱动 (Profile-Driven) 的思维方式进行性能调优**
> 使用数据和分析工具来指导优化。使用分析器 (Profilers) 识别真正的瓶颈——无论是计算利用率、内存带宽、内存延迟、缓存未命中，还是通信/网络延迟。然后针对该瓶颈应用定向优化。

**保持全局视野**
> 提升 AI 系统性能跨越硬件（包括 GPU、CPU、内存和网络）以及软件（如算法和库）。任何一层的弱点都可能成为整体的瓶颈。最好的性能工程师会考虑硬件-软件协同设计：有时算法的改变可以缓解硬件限制，有时新的硬件特性又能赋能新的算法。

**随时了解最新的硬件、软件和算法**
> 现代 AI 硬件和软件发展迅速。诸如统一 CPU-GPU 内存、更快的互连和新颖的数值精度格式等新能力可能会改变最优策略。优秀的性能工程师会关注这些变化并相应地更新他们的思维模型，以快速消除瓶颈。此外，MLPerf 基准测试套件是了解各种模型 AI 硬件性能的绝佳资源。

---

## 结论 (Conclusion)

这一介绍性分析强调，在大规模下，优化不是可选项——它是绝对必要的。它是“一个能工作的系统”与“一个完全不切实际的系统”之间的区别。传统方法，无论是在硬件还是算法上，在这样的规模下都会失效。为了向前推进，我们既需要先进的硬件，也需要智能的软件技术。

很明显，AI 模型正在推向物理资源的极限。硬件正在竞相跟上新模型架构和算法的步伐。而性能工程师正是那个坐在驾驶位上的人，确保所有这些昂贵的机器真正交付结果。

我们已经证明，AI 系统性能工程师的角色正变得越来越重要。仅仅向问题砸钱和砸硬件是不够的。我们需要协同优化一切——模型架构、算法、硬件和系统设计——以推动 AI 能力的下一次飞跃。

作为一名 AI 系统性能工程师，你的工作是多学科、复杂且动态的。你可能这一天钻研 GPU 性能分析，第二天研究网络拓扑，后天又得处理算法复杂度。这是一个属于“全栈”性能极客的角色，他们热衷于从硬件和软件中挤出每一滴可用性能。

本质上，AI 系统性能工程师的座右铭是**“机械共鸣”**。我们深刻理解机器——包括硬件和软件——以便我们可以量身定制高效的解决方案，将整个栈的性能能力发挥到极致。

正如我们早先看到的，100 万亿参数模型远远超出了当今硬件的能力范围——即使是百亿亿次级 (Exascale) 系统，单次训练运行也需要数千年的 GPU 时间。这显然是不切实际的，强调了为什么我们需要先进的硬件和巧妙的软件优化才能达到这种规模。

在接下来的章节中，我们将演示如何将 AI 系统的组件从处理器到内存再到互连和软件框架进行拆解——并学习如何以原则性的方式优化每个组件。我们将研究具体的案例研究，其中微小的改变带来了巨大的性能和成本改进。通过这样做，我们将帮助建立一个关于多维度性能优化的思维模型。

在这段旅程结束时，作为读者和从业者的你将装备好当今的最佳实践知识以及应对明天挑战的工程思维。你将拥有一个技术武库，将 AI 系统推向极限——无论是现在还是未来。对于 AI 系统性能工程师而言，使命是明确的。我们必须从这些创新中学习，并准备好在栈的每一层应用激进的优化。

现在，随着背景的确立，让我们深入探究现代 AI 系统的硬件组件，包括 CPU、GPU、内存技术、网络结构和存储机制。通过研究支撑当代 AI 超级计算机的组件，你将学到为后续章节深入探讨优化技术奠定基础的基本原理。









**译境** 再次启动。

第二章是全书的硬件基石。作者带我们从微观的芯片封装（Superchip）一路扩展到宏观的机架级超级计算机（NVL72），描绘了 NVIDIA 极其激进的垂直整合策略。

这一章充满了硬核参数和架构细节。译文将致力于在保持技术精确性的同时，传达出这种“暴力美学”带来的工程震撼感，特别是关于统一内存、NVLink 互连以及液冷散热系统的描述。

以下是 **第二章：AI 系统硬件概览** 的中文译本。

---

# 第二章：AI 系统硬件概览

想象一下，将原本需要占满整个数据中心的超级计算机 AI 硬件，浓缩进单个机柜里。

NVIDIA 的最新架构正是做到了这一点。在本章中，我们将深入探讨 NVIDIA 如何将 CPU 和 GPU 融合为强大的**超级芯片 (Superchips)**，然后通过超高速互连将数十个超级芯片编织在一起，创造出一种“**盒即超级计算机 (AI supercomputer-in-a-box)**”。我们将探索最基础的硬件构建模块——**Grace CPU** 和 **Blackwell GPU**——并了解它们紧密的集成和巨大的内存池是如何让 AI 工程师的生活变得更轻松的。

接着，我们将向外扩展，研究那张将 72 个 GPU 连接得如同单台机器一样的网络结构。沿途，我们将重点介绍计算性能、内存容量和能效方面的飞跃，正是这些赋予了该系统超能力。读完本章，你将领悟这套尖端硬件是如何使训练和服务那些曾被认为“不可能”的多万亿参数模型成为现实的。

---

## CPU 与 GPU 超级芯片 (The CPU and GPU Superchip)

NVIDIA 扩展 AI 的路径始于单个组合式 CPU + GPU 超级芯片模组的层面。从 Hopper这一代开始，NVIDIA 开始将基于 ARM 的 CPU 与一个或多个 GPU 封装在同一个单元中，并通过高速接口紧密连接。其结果是一个表现如同统一计算引擎的单一模组。

超级芯片的首次实现是 **Grace Hopper (GH200)**，它将一个 Grace CPU 与一个 Hopper GPU 配对。紧随其后的是 **Grace Blackwell (GB200) 超级芯片**，它将一个 Grace CPU 与**两个** Blackwell GPU 配对在同一个封装中。Grace CPU 位于模组中央，两侧各有一个 Blackwell GPU 裸片（Die），如图 2-1 所示。

*(图 2-1：NVIDIA Grace Blackwell 超级芯片模组，包含位于中央的 Grace CPU 和位于左上/右上的两个 Blackwell B200 GPU，它们处于同一个共享统一内存空间，并通过名为 NVLink-C2C 的定制高速链路连接)*

在传统系统中，CPU 和 GPU 拥有独立的内存池，并通过相对较慢的总线（如 PCIe）进行通信，这意味着数据必须来回拷贝。NVIDIA 的超级芯片通过使用一种名为 **NVLink-C2C (Chip-to-Chip)** 的定制高速链路连接 CPU 和 GPU，消除了这一障碍。

NVLink-C2C 在 GB200 超级芯片的 Grace CPU 和 Blackwell GPU 之间提供高达 **~900 GB/s** 的带宽。相比之下，PCIe Gen5 x16 (Blackwell B200) 单向约为 64 GB/s，PCIe Gen6 x16 (Blackwell Ultra B300) 单向约为 128 GB/s。NVLink-C2C 的互连速度比典型的 PCIe 快了一个数量级。而且，重要的是，它是**缓存一致的 (Cache-coherent)**。

**缓存一致性**意味着 CPU 和 GPU 共享一个一致的、统一的内存架构。因此，它们总是能看到相同的数值。在实践中，超级芯片上的 Grace CPU 和 Blackwell GPU 可以直接访问彼此的内存，就像访问一个巨大的内存池一样。GPU 可以读写 CPU 内存中的数据，反之亦然，而无需显式拷贝。这种统一内存架构通常被 NVIDIA 称为**统一 CPU-GPU 内存**或**扩展 GPU 内存 (EGM)**，它有效地模糊了 CPU 内存和 GPU 内存之间的界限。

每个 Grace Blackwell 超级芯片都搭载了海量的内存。Grace CPU 附带数百 GB 的 LPDDR5X DRAM，而每个 Blackwell GPU 都有其自己的高速、高带宽内存 (HBM) 堆栈。

在 GB200 超级芯片中，Grace CPU 提供高达 **~480 GB** 的 LPDDR5X（带宽高达 ~500 TB/s，此处原文可能有误，通常 LPDDR5X 带宽在 500 GB/s 级别，而非 TB/s），两个 Blackwell GPU 合计贡献高达 **~384 GB** 的 HBM3e 内存（每 GPU 192 GB）。总的来说，一个 GB200 超级芯片暴露了大约 **~900 GB** 的一致性统一内存，供 GPU 和 CPU 在统一地址空间中访问。

简单来说，**每个超级芯片都有近 1 TB 的快速统一内存可供支配。** 对于巨型 AI 模型来说，这是一个颠覆性的改变。在旧系统中，单个 GPU 可能受限于 < 100 GB 的内存，这意味着大于此容量的模型必须被分割或卸载到更慢的存储中。而在这里，GPU 可以无缝地利用 CPU 内存作为扩展。

如果神经网络的某一层或巨大的嵌入表（Embedding Table）无法放入 GPU 的本地 HBM，它可以驻留在 CPU 内存中，GPU 依然可以通过 NVLink-C2C 对其进行操作。从程序员的角度来看，统一虚拟地址空间和一致性简化了正确性。然而，为了性能，我们仍应使用异步预取和分级流水线等技术显式管理数据放置和移动。因为通过 NVLink-C2C 访问 LPDDR5X 的延迟较高，且带宽比直接访问 HBM 低大约一个数量级。

GPU 内存仍然比 CPU 内存快得多，也更靠近 GPU 核心——你可以将 CPU 内存视为一个巨大但稍慢的扩展。访问 LPDDR5X 中的数据不如 GPU 上的 HBM 快，带宽低约 10 倍且延迟更高。一个智能的运行时（Runtime）会将最常用的数据保留在 HBM 中，并使用 CPU 的 LPDDR5X 来处理溢出或对速度要求不那么高的数据。**关键点在于，内存溢出不再意味着必须求助于 NVMe SSD 或跨网络传输。**

GPU 可以以大约 900 GB/s（单向 450 GB/s）的速度从 CPU RAM 获取数据，这虽然比 HBM 慢，但比从 NVMe SSD 存储获取要快得多。这种灵活性至关重要，因为它意味着一个大小为（比如）500 GB 的模型（对于单个 GPU 的 HBM 来说太大了）仍然可以完全放置在一个超级芯片模组内，访问 HBM 中的 192 (180 可用) GB 和 CPU 内存中的 ~500 GB。该模型可以在无需跨多个 GPU 分割模型的情况下运行。GPU 只需在需要时透明地从 CPU 内存中提取额外数据。

本质上，只要整个模型能放入超级芯片的 CPU + GPU 组合内存中，内存大小就不再是容纳超大模型的硬性限制。许多研究人员都曾面临过当模型无法放入 GPU 时那令人恐惧的“内存溢出 (OOM)”错误——这种架构旨在大幅推后这一边界。

---

### NVIDIA Grace CPU

Grace CPU 本身绝非等闲之辈。它是由 NVIDIA 定制设计的基于 **ARM Neoverse V2** 的 CPU，专为带宽和效率打造。它在超级芯片中的工作是处理通用任务、预处理并向 GPU 输送数据，以及管理附带的海量内存。它运行在适中的时钟速度下，但通过巨大的内存带宽——高达 ~500 GB/s 的 LPDDR5X 内存——以及大量的缓存（包括超过 100 MB 的 L3 缓存）来弥补。

其设计哲学是：**在向 GPU 铲送数据时，CPU 永远不应成为瓶颈。** 它可以从存储流式传输数据或执行实时的转换操作（如 Tokenization 或数据增强）——通过 NVLink-C2C 非常高效地喂给 GPU。如果你工作负载的一部分更适合在 CPU 上运行，Grace 核心可以处理它，并让结果立即可供 GPU 访问。

这是一种和谐的耦合：CPU 扩展了 GPU 在弱项（如随机内存访问或重控制流代码）上的能力，而 GPU 则加速了 CPU 无法跟上的数字运算。

CPU 和 GPU 之间的低延迟链路意味着它们可以交换任务而没有通常的开销。例如，从 CPU 启动一个 GPU 内核可以比在传统系统上快得多，因为指令不需要穿越缓慢的 PCIe 总线。CPU 和 GPU 本质上是在同一块板子上。这类似于调用一个快速的本地函数与调用较慢的远程函数的区别。接下来，我们来谈谈超级芯片的暴力引擎——Blackwell GPU。

---

### NVIDIA Blackwell “双裸片 (Dual-Die)” GPU

Blackwell 是 NVIDIA 这一代 GPU 的代号，它代表了在计算马力和内存上相对于上一代 Hopper (H100) GPU 的巨大飞跃。Blackwell B200 和 B300 "Ultra" GPU 不是单芯片。相反，它们采用了**多芯片模组 (MCM)** 设计，在一个模组中放置了两个 GPU 裸片。因此，Blackwell 被称为**双裸片 (Dual-die) GPU**（见图 2-2）。

> **注意：** 虽然本节深入探讨了双裸片架构的细节，但本书其余部分将把 Blackwell 的两个组合 GPU 裸片统称为“Blackwell GPU”。

这种 **Chiplet (小芯片/芯粒)** 方法将原本是一个巨大的 GPU 分割成较小的 GPU 裸片——并用封装内的超高速裸片间互连将它们连接起来。为什么要这样做？因为单一的单片 (Monolithic) 裸片受限于制造工艺，硅片上的芯片尺寸是有物理极限的（光罩尺寸限制）。通过将两个物理 GPU 裸片组合成一个模组，NVIDIA 可以将该模组的总晶体管预算翻倍。

*(图 2-2: Blackwell 双裸片多芯片模组 (MCM) 设计)*

对于 Blackwell B200 MCM，每个 GPU 裸片拥有约 1040 亿个晶体管和 96 GB HBM3e 内存。组合后的 GPU 模组每个 B200 GPU 拥有约 2080 亿个晶体管和 192 (180 可用) GB 的总内存。相比之下，Hopper H100 GPU 拥有 ~800 亿个晶体管和 80 GB HBM3 内存。因此，Blackwell B200 的晶体管数量翻了一番以上，内存大小增加了约 **2.4 倍**。

Blackwell 的两个 GPU 裸片使用一种名为 **NV-HBI (High-Bandwidth Interface)** 的专用、高速 10 TB/s 裸片间互连进行通信。这使得模组中的两个 GPU 裸片能像**单个统一 GPU** 一样运行。运行在其之上的软件层只能看到一个 GPU。

从系统的角度来看，一个 Blackwell GPU 是一个单一的模组或设备，拥有巨大的内存池（192 [180 可用] GB HBM3e）和大量的执行单元，但在引擎盖下，它是两个协同工作的芯片。NVIDIA 的软件和调度确保工作在两个 GPU 裸片之间平衡，并且内存访问是一致的。这允许开发者在很大程度上忽略这种复杂性，因为正如 NVIDIA 所预期的那样，它们表现为一个 GPU。

每个 Blackwell B200 GPU 模组拥有 192 (180 可用) GB 的 HBM3e 内存，分布在两个 GPU 裸片上（每个 96 GB），并分为 8-Hi 堆栈。一个 8-Hi HBM3e 堆栈是通过垂直堆叠 8 个 DRAM 裸片构建的——每个 3 GB——每堆栈总计 24 GB。

B200 GPU 使用 8 个这样的堆栈（每裸片 4 个）来提供 192 (180 可用) GB 的封装内存。这增加了每 GPU 的堆栈数量和容量，相较于前代 Hopper GPU 提供了更多空间来容纳模型参数、激活值、梯度和输入数据。

> **注意：** 由于纠错码 (ECC)、系统固件使用、制造限制以及其他阻止芯片暴露全部 192 GB 的问题，每个 B200 只有 180 GB 的 HBM3e 内存可用。因此，我们将引用 180 GB 而非完整的 192 GB 作为 Blackwell B200 的可用内存。

内存速度也更快，Blackwell B200 的 HBM3e 每 GPU 提供高达约 **8 TB/s** 的聚合带宽。相比之下，Hopper 使用的是上一代 HBM3，每 GPU 提供 ~3.35 TB/s。因此，Blackwell 的内存带宽吞吐量大约是 Hopper 的 **2.4 倍**。

以每秒 8 TB 的速度输送数据，Blackwell GPU 核心得以忙碌地处理巨大的矩阵运算，而不会频繁因等待数据而停顿。NVIDIA 还加强了片上缓存，Blackwell 拥有总计 **126 MB 的 L2 缓存**（每裸片 63 MB）。这种缓存是 GPU 上的一种极快的小型内存，用于保存最近使用的数据。

通过将 L2 缓存大小相比 Hopper 的 50 MB 增加 2.5 倍以上，Blackwell 可以将更多的神经网络权重或中间结果保留在芯片上，避免了去 HBM 的额外行程。这再次有助于确保 GPU 的计算单元极少因缺乏数据而饥饿。

接下来，我们将展示 Blackwell GPU 如何与一组专用的低精度 Tensor Cores 配对——以及来自 NVIDIA 的名为 Transformer Engine 的针对 Transformer 优化的硬件和软件 API。像 PyTorch 这样的框架和 vLLM 这样的推理引擎通过使用 CUDA、CUTLASS 和 OpenAI 的 Triton 等库来支持这些优化，我们将在后续章节讨论这些内容。

---

### NVIDIA GPU Tensor Cores 和 Transformer Engine

说到计算单元，Blackwell 引入了专门针对 AI 工作负载的增强功能。其中的核心是 NVIDIA 的 **Tensor Core** 技术和 **Transformer Engine (TE)**。Tensor Cores 是 GPU 每个流式多处理器 (SM) 中的专用单元，可以极高速度执行矩阵乘法运算。

前代产品中已存在 Tensor Cores，但 Blackwell 的 Tensor Cores 支持更多的数值格式，包括极低精度的格式，如 8-bit 和 4-bit 浮点数。低精度背后的理念很简单：**通过使用更少的比特来表示数字，你可以同时执行更多的操作**——更不用说因为表示相同数字所需的比特更少，你的内存也更耐用了。这假设你的算法可以容忍一点点数值精度的损失。如今，许多 AI 算法在设计时就考虑到了低精度数值格式。

NVIDIA 开创了 TE，以便在深度学习中自动调整和使用**混合精度 (Mixed Precision)**，即关键层使用较高精度（FP16 或 BF16），而不那么关键的层使用 FP8。TE 自动优化精度的平衡，目标是在较低精度下保持模型的准确性。

在 Hopper 时代，TE 首次引入了 FP8 支持，使吞吐量相对于 FP16 翻了一番。Blackwell 更进一步，引入了 **NVIDIA FP4 (NVFP4)**，这是一种 4-bit 浮点格式，使用的比特数是 FP8 的一半。FP4 非常小，它可能使 FP8 的计算吞吐量再翻一番。图 2-3 显示了 FP8 和 FP4 相对于 FP16 的相对加速比。

一个完整的 NVL72 机架（72 个 GPU）在 4-bit 精度下拥有超过 **1.44 exaFLOPS**（即 $1.4 \times 10^{18}$）的理论 Tensor Core 吞吐量。这是一个令人咋舌的数字，使这单个机架跻身世界最快超级计算机的行列——尽管是在低 FP4 精度下。即使现实世界的工作负载并不总是能达到那个峰值，但这种能力的存在本身就令人震惊。

现代 GPU 使用的 TE 增加了 NVFP4 支持以及改进的缩放和校准功能。在实践中，你通过在 PyTorch 等框架中使用其内核和模块来采用 TE。这样，FP8 和 NVFP4 就会在能保持准确性的情况下被应用。这在所有框架中并非完全自动的逐层决策。

*(图 2-3: FP8 和 FP4 相对于 FP16 的相对加速比)*

先进技术包括在训练和推理期间动态改变神经网络每一层的精度。目标是为每一层使用能保持模型准确性的最低精度。例如，TE 可能会将神经网络的前几层保持在 FP16，因为早期层对噪声可能很敏感。但是，基于启发式算法，它可以决定对更具容忍度的后续层——或者对高精度并不关键的巨大嵌入矩阵——使用 FP8 或 FP4。

所有这一切都可以在 NVIDIA 库和像 PyTorch 这样的 AI 框架的底层发生。作为一个用户，你只需启用混合精度，结果就是一个巨大的加速，本质上是“免费”得来的。我们将在第 9 章讨论混合精度，但现在只需知道，许多 LLM 如今出于这个原因都在使用混合精度。这些降低的精度提高了相对于 FP16 和 FP32 的训练速度——并减少了准确性损失。Blackwell 的构建旨在让 FP8 和 FP4 变得易用且高效。

这些低精度格式也减少了内存使用。使用 FP4 每个参数所需的内存比 FP8 少一半（而 FP8 又是 FP16 内存使用的一半），这意味着你可以将更大的模型装入 GPU 内存中。

NVIDIA 实际上押注 AI 的未来在于低精度算术，并赋予了 Blackwell 在这方面表现出色的能力。这对于海量模型的推理服务尤其关键，因为在那里的吞吐量（每秒 Token 数）和延迟至关重要。

为了说明从 Hopper 到 Blackwell 的代际飞跃，NVIDIA 报告称，基于 H100 的系统对于一个大型 1.8 万亿参数 MoE 模型，每 GPU 每秒仅能生成约 **3.4 个 Token**——且首个 Token 的延迟超过 5 秒。这对于交互式使用来说太慢了。

基于 Blackwell 的系统 (NVL72) 运行相同的模型，每 GPU 每秒约为 **150 个 Token**，且首个 Token 延迟极低，约为 **50 毫秒**。这大约是 Hopper 代际实时吞吐量的 **30 倍**提升。NVL72 允许这个巨大的模型提供实时响应——为许多低延迟用例打开了大门。

这种加速来自于原始 FLOPS、更快的 GPU、低精度 (FP4) 的使用以及保持 GPU 数据充足的 NVLink 互连的组合。它强调了跨越计算和通信的整体设计如何转化为现实世界的性能收益。

本质上，Blackwell GPU 比它们的前辈更强大、更聪明，且数据“喂”得更饱。得益于 Tensor Cores、TE 和低精度，它们能更快地吞噬数学运算。此外，系统架构通过巨大的内存带宽、大缓存和 NVLink 确保数据能快速可用。

在继续之前，让我们快速讨论一下 GPU 内部的层级结构，这对理解后续的性能调优很有用。

---

### 流式多处理器 (SM)、线程 (Threads) 和 线程束 (Warps)

每个 Blackwell GPU，像它的前辈一样，由许多**流式多处理器 (SM)** 组成。把这些想象成 GPU 的“核心”，如图 2-4 所示。

*(图 2-4: CPU 核心与 GPU 核心的对比)*

每个 SM 包含一堆算术单元（用于 FP32, INT32 等）、用于矩阵运算的 Tensor Cores、用于内存操作的加载/存储单元，以及一些用于超越函数数学运算的特殊函数单元。GPU 还有自己的一小池超快内存，包括寄存器、共享内存和 L1 缓存。

一个 SM 以固定大小的组（称为 **Warps/线程束**）来执行线程，每个 Warp 恰好包含 32 个线程，它们步调一致地执行完全相同的指令。这被称为**单指令多线程 (SIMT)** 执行模型。

SM 并行执行许多活跃的 Warp，以帮助覆盖线程等待从全局内存访问数据时的延迟。想象一个拥有数十个 Warp（数百个线程）同时在飞行的 SM。如果一个 Warp 正在等待内存获取，另一个 Warp 可以运行。这被称为**延迟隐藏 (Latency Hiding)**。我们将在整本书中重访延迟隐藏。这是你的调优工具箱中一个非常重要的性能优化工具。

像 Blackwell 这样高端的 GPU 将拥有数百个 SM。每个 SM 能够并发运行数千个线程。这就是我们将数万个活跃线程放到单个 GPU 上的方式。所有这些 SM 共享一个 126 MB 的 L2 缓存（如前所述），并共享连接到 HBM 的内存控制器。内存层级结构包含：寄存器（每线程）→ 共享内存（每线程块，在每个 SM 上）→ L1 缓存（每 SM）→ L2 缓存（GPU 上所有 SM 共享）→ HBM 内存（片外），如图 2-5 所示。

*(图 2-5: GPU 内存层级结构)*

为了获得最佳性能，数据需要尽可能停留在该层级的高层。如果每个操作都要即使以 8 TB/s 的速度去访问 HBM，GPU 也会因访问片外内存的延迟增加而停顿太频繁。通过将可重用数据保存在 SM 本地内存或 L2 缓存中，GPU 可以实现巨大的吞吐量。Blackwell 架构缓存和带宽的翻倍正是旨在让 GPU 这头猛兽保持饱腹和快乐。

作为性能工程师，我们将看到许多案例，其中内核的性能既受限于计算，也受限于内存流量和吞吐量。NVIDIA 显然在设计 Blackwell 时，力求在 FLOPS 和内存带宽之间取得良好的平衡，以适应许多 AI 工作负载。

Blackwell 的设计平衡了计算和内存，使得对于许多 AI 内核，GPU 可以在最小停顿下保持计算。在实践中，优化良好的密集数学运算可以重用片上内存数据，从而在不受到严重内存限制的情况下接近峰值 FLOPS。

所有这一切意味着，在给定优化良好的代码下，GPU 通常会忙于计算而不是等待数据。请注意，某些操作（如巨大的归约或随机内存访问）仍然可能受限于内存，但更新后的 GPU、内存和互连硬件使这一问题有所缓解。

---

## 超大规模网络：将多个 GPU 视为一体 (Ultrascale Networking Treating Many GPUs as One)

将两个 GPU 和一个 CPU 打包进一个超级芯片给了我们一个极其强大的节点。下一个挑战是将许多这样的超级芯片连接在一起，以扩展到更大的模型训练。

NVIDIA 提供了一种名为 **NVL72** 的大型机架配置，使用 GB200/GB300 超级芯片。NVL72 代表一个拥有 72 个 Blackwell GPU —— 和 36 个 Grace CPU —— 全部通过 NVLink 互连的系统。这本质上是一个**单机架内的 AI 超级计算机**。

GB200/GB300 NVL72 构建为 18 个计算节点，其中每个节点包含两个 GB200/GB300 超级芯片，每个计算节点总共有 4 个 Blackwell GPU + 2 个 Grace CPU，如图 2-6 所示。

*(图 2-6: GB200/GB300 NVL72 机架内的一个 1U 计算托盘，包含两个 Grace Blackwell 超级芯片)*

在这里，每个超级芯片模组有一个 Grace CPU 和两个 Blackwell GPU（每个 B200 是双裸片 MCM）。NVL72 有 18 个这样的托盘连接在一起。通过连接这 18 个计算节点，GB200/GB300 NVL72 将 72 个 Blackwell GPU（18 节点 × 4 GPU）和 36 个 Grace CPU（18 节点 × 2 CPU）链接在一起，形成一个强大的、统一的 CPU-GPU 集群。

关于 NVL72 有趣的是，**每个 GPU 都可以通过 NVLink Switch 结构在单个 NVLink 域内以极高速度与任何其他 GPU 通话。** NVIDIA 通过结合 GPU 上的 **NVLink 5** 连接和一种名为 **NVSwitch** 的专用交换芯片实现了这一点。

---

### NVLink 和 NVSwitch

每个 Blackwell GPU 暴露 18 个 NVLink 5 端口。聚合双向 NVLink 带宽为每 GPU **1.8 TB/s**（18 个 NVLink 链路 × 100 GB/s 双向），NVL72 将所有端口连接到 NVLink Switch 系统。每个 NVLink 交换机托盘提供 144 个 100 GB/s 的 NVLink 端口。在 9 个托盘中，每个 GPU 的 18 个 NVLink 5 链路分别连接到一个 NVSwitch 芯片，因此 72 个 GPU 以**全对剖带宽 (Full Bisection Bandwidth)** 完全连接。聚合双向 NVLink 5 带宽为每 GPU 1.8 TB/s。

这是 Hopper GPU 使用的上一代每 GPU NVLink 带宽的两倍。Hopper H100 使用 18 个 NVLink 4 端口，但运行速度是 NVLink 5 的一半。通过 NVLink 的 GPU 间延迟在个位数微秒范围内。

GPU 通过 NVSwitch 芯片连接成网络。NVSwitch 本质上是一个类似于网络交换机的交换芯片，但它是专为 NVLink 构建的。这意味着任何 GPU 都可以通过 NVLink Switch 系统中的**一跳 (One switch stage)** 交换以全对剖带宽到达任何其他 GPU。这种单跳属性在单个 NVL72 机架内成立，因为每个 GPU 使用其 18 个 NVLink 链路连接到 18 个 NVSwitch 芯片，启用了一条通过单个交换机的路径。图 2-7 展示了 NVL72 中使用的 NVLink Switch 托盘。

*(图 2-7: NVL72 内部的一个 NVLink Switch 托盘)*

每个交换机托盘包含两个 NVSwitch 芯片和多个高速端口。NVL72 机架包含 9 个这样的交换机托盘和 18 个计算托盘，如图 2-8 所示。

*(图 2-8: NVL72 机架内由 9 个托盘组成的 NVSwitch 系统)*

由于 9 个交换机托盘各包含两个 NVSwitch 芯片，NVL72 系统中共有 18 个 NVSwitch 芯片。网络被布置为一个全交叉开关 (Full Crossbar)，使得每个 GPU 连接到每个 NVSwitch，每个 NVSwitch 连接到每个 GPU。这在任何一对 GPU 之间提供了一条高带宽路径。

每个交换机托盘暴露 144 个 NVLink 端口，以完全连接每个 GPU 上的 18 个 NVLink 链路。具体来说，每个 GPU 使用其 18 个 NVLink 链路连接到 18 个 NVSwitch 芯片（每个交换机一条链路）。这意味着任何 GPU 可以在一跳内到达任何其他 GPU（GPU → NVSwitch → GPU），且沿途带宽巨大。图 2-9 展示了完整的 NVL72 架构，包含 72 个全连接 GPU（36 个 GB200 超级芯片）和 18 个 NVSwitches。

*(图 2-9: 每个 GPU 连接到每个 NVSwitch [每个交换机一条链路])*

整个 72-GPU 网络在 NVL72 机架内的聚合对剖带宽约为 **130 TB/s**。作为参考，这比即使是同等规模的顶级 InfiniBand 集群也要高出许多倍。该设计暴露了一个全连接、高带宽的结构，并在 GPU 之间拥有**全局地址空间**。这允许高效的集合操作和单边操作 (One-sided operations)，同时保留了软件对同步和一致性的显式控制。

---

### 多 GPU 编程 (Multi-GPU Programming)

从编程模型的角度来看，一个 GPU 可以使用点对点 (Peer-to-Peer) 和分区全局地址空间 (PGAS) 模型（如 **NVIDIA SHMEM (NVSHMEM)**，NVIDIA 的 GPU 加速 OpenSHMEM 实现）通过 NVLink 直接访问另一个 GPU 的内存。存在一个全局地址空间，但 GPU 缓存并非跨 GPU 全局一致的。只有通过 NVLink-C2C 的 CPU-GPU 路径是缓存一致的。像 NCCL 和 NVSHMEM 这样的软件栈提供了正确多 GPU 访问所需的同步和排序。结合硬件缓存一致性和软件同步技术，使得 NVL72 在本质上可以被视为**一个大 GPU**。

**远程直接内存访问 (RDMA)** 是一种网络技术，它能够在主机之间通过 InfiniBand 和 RDMA over Converged Ethernet (RoCE) 传输实现直接、零拷贝的内存传输。InfiniBand 贸易协会 (IBTA) 为 InfiniBand 和 RoCE 定义了可选的远程原子操作。

**GPUDirect RDMA**，NVIDIA 的 RDMA 协议实现，允许网络接口卡 (NIC) 注册 GPU 内存，并使用 `nvidia-peermem` 驱动程序直接对 GPU 内存执行 RDMA。这允许 GPU 跨节点交换数据并执行原子操作，而无需涉及 CPU。这允许 NIC 直接对 GPU 内存执行 DMA，而无需通过主机 RAM 中转。

跨节点的远程原子操作和单边操作由上层库（如 NVSHMEM）提供，它们在 RDMA 传输之上实现了这些语义。请注意，GPUDirect RDMA 提供直接数据路径，而非原子 API 本身。分布式训练和推理工作负载需要频繁地在许多 GPU 之间同步和交换信息。

传统上，GPU 位于不同的计算节点和机架中。因此，同步可能发生在相对较慢的网络链路（如 InfiniBand 和以太网）上。在扩展到许多 GPU 以支持大型 AI 模型时，这通常是瓶颈。

而在 NVL72 系统中，这些交换通过 NVLink 和 NVSwitch 以超快速度发生。这意味着你可以将训练作业或推理集群扩展到 72 个 GPU，且通信开销极小。由于 GPU 等待彼此数据的时间大大减少，整体吞吐量在 72 个 GPU 范围内几乎呈线性扩展。

相比之下，考虑在同样规模的 72-GPU H100 集群（由 9 个独立的计算服务器组成，每个有 8 个 Hopper H100 GPU）上扩展相同的作业。这种配置需要 InfiniBand，这会产生网络瓶颈，极大地降低集群的扩展效率。

让我们用具体数字分析并比较 NVL72 和 72-GPU H100 集群。在单个 NVL72 机架内，GPU 到 GPU 带宽高达每 GPU 1.8 TB/s（双向聚合），对于千字节级别的小消息，延迟在 **1–2 微秒**量级。大消息花费时间更长，通常受带宽限制。在一个传统的 InfiniBand 网络中，每 GPU 带宽可能更像是 20–80 GB/s——取决于有多少 NIC 及其速度——且延迟可能在 **5–10 微秒**或更多。

NVL72 网络提供比主机-NIC 结构高得多的单 GPU 带宽和更低的延迟。具体来说，NVLink 5 提供约 1.8 TB/s 的每 GPU 聚合带宽，而现代主机 NIC 在 400–800 Gb/s 线速下提供约 50–100 GB/s 每端口。所有这些都将集合操作的开销从百分之几十降低到仅百分之几。

实际上，在 NVLink 连接的 NVL72 系统内，集合通信开销远低于传统的节点到节点结构，但迭代时间的确切比例取决于工作负载。例如，NVIDIA 报告称，一个 1.8 万亿参数的 MoE 模型从 H100 上的每 GPU 每秒约 3.4 个 Token（首字时间超过 5 秒）提升到 GB200 NVL72 上的每 GPU 每秒约 150 个 Token（首字时间约 50 毫秒）。这种加速很大程度上归功于消除了 NVL72 机架内的 GPU 间通信瓶颈，以及 Blackwell 更高的计算吞吐量。

在单个 NVL72 机架内，通信太快了，以至于通信瓶颈因几乎被完全消除而变得优先级较低，而在传统的 InfiniBand 和以太网集群中，通信往往是主要瓶颈，需要在软件层面进行仔细优化和调优。

简而言之，你应该设计和实现能利用 NVL72 配置的软件，尽可能将工作负载的通信保持在机架内部（“intra-rack”），以利用高速 NVLink 和 NVSwitch 硬件。只有在为了扩展超出 NVL72 计算和内存资源而绝对必要时，才使用机架间（“inter-rack”）较慢的基于 InfiniBand 或以太网的通信。

---

### 使用 NVIDIA SHARP 进行网内聚合 (In-Network Aggregations with NVIDIA SHARP)

另一个硬件启用的优化是 **NVIDIA 可扩展层次化聚合与归约协议 (SHARP)**。对于 NVLink Switch 系统机架，网内归约使用集成在 NVSwitch ASIC 中的 SHARP 引擎来**卸载 (Offload)** 归约和其他集合通信到网络内（见图 2-10）。

*(图 2-10: 使用 NVSwitch 中的 SHARP 归约引擎将计算卸载到 NVIDIA 网络硬件)*

NVSwitch 结构合并部分结果，而无需数据流经 GPU。通过将集合计算从 GPU 卸载到交换机硬件本身，SHARP 允许 GPU 专注于更复杂的计算，降低集合通信延迟，减少穿过网络的数据总量，并提高系统效率。

SHARP 提高的效率意味着在分布式训练期间，聚合梯度或同步参数的繁重工作由 NVSwitch 专用的 SHARP 引擎处理。结果是在机架内和机架间配置中都能实现更高效的扩展。

有了 SHARP，即使 GPU 数量增加，你也会看到近乎线性的性能提升。这种网内计算能力对于训练超大模型尤其关键，因为在集合操作上节省的每一微秒都能转化为显著的整体加速。

> **提示：** SHARP 是 NVIDIA 在 2019–2020 年收购 Mellanox 期间获得的最具影响力的创新之一。如果你目前没有使用它，应该去探索一下。SHARP 可以显著降低集合通信的延迟和流量，并经常能提高受限于通信的训练的扩展效率。

---

### 多机架与存储通信 (Multirack and Storage Communication)

接下来，我们讨论一个 NVL72 机架如何与另一个 NVL72——或像共享文件系统这样的外部存储系统——进行对话。如我们所示，在 NVL72 机架内部，NVLink 覆盖了所有 GPU 到 GPU 的流量。但在机架外部，它依赖于更传统的网络硬件。

NVL72 中的每个计算节点都配备了高速网络接口卡 (NIC) 和一个 **数据处理单元 (DPU)**。DPU 从主机 CPU 卸载、加速并隔离网络、存储、安全和管理任务。通过直接在 NIC 上运行这些操作，DPU 减少了 CPU 开销和延迟。

在 NVL72 设计中，**BlueField-3 DPU** 处理线速数据包处理、RDMA 和 NVMe over Fabrics (oF) 操作。NVMe-oF 是 NVMe 的一种协议变体，它将存储扩展到网络结构上。因此，DPU 直接在网络、存储和 GPU 内存之间移动数据，无需 CPU 参与。这最大化了整体系统吞吐量和效率。

GB200/GB300 NVL72 机架与 **Quantum-X800 InfiniBand** 或 **Spectrum-X800 以太网**结构集成。计算托盘通常每个节点使用四个 **ConnectX-8 800 Gb/s NIC** 以获得高外部带宽。BlueField-3 DPU 用于需要网内加速或卸载存储、安全和控制平面任务的地方。

每个计算节点有四个 800 Gb/s NIC，即每个节点 3.2 Tbit/s，每个机架约 57.6 Tbit/s（57.6 Tbit/s = 3.2 Tbit/s 每节点 × 18 节点）。虽然这个吞吐量令人咋舌，但请记住，当你走出机架时，你仍然需要一个超快的网络。这样，多机架扩展就不会在机架边界处遇到瓶颈。NVIDIA 将这些多机架部署称为 **AI 工厂 (AI Factories)**。他们确保 NVL72 可以使用每个节点的这四个 NIC 插入更大的网络结构中。

每个节点中的 BlueField-3 DPU 帮助卸载网络任务，如 RDMA、TCP/IP 和 NVMe SSD 存储访问。这确保了 Grace CPU 不会被管理网络中断所拖累。DPU 本质上充当智能网络控制器，使用 NVIDIA 的 GPUDirect RDMA 软件在 NIC 和 GPU 内存之间直接移动数据。这不需要通过主机内存暂存数据或使用任何 CPU 周期。

BlueField DPU 避免了 CPU 参与，这在为大规模训练作业从存储服务器流式传输大型数据集时特别有用。具体来说，DPU 可以处理传输并将数据直接存入 GPU 内存——而 CPU 专注于其他任务，如数据预处理。

除了提供性能卸载能力外，DPU 还支持安全的多租户。它隔离不同作业和用户的网络流量——充当节点上的智能防火墙/交换机。

当扩展到多个 NVL72 机架时，NVIDIA 使用 Quantum 系列 InfiniBand 交换机。多个 NVL72 机架可以使用这些 InfiniBand 交换机互连，形成一个庞大的 NVL72 机架集群。

例如，一个总共 576 个 GPU 的 8 机架 NVL72 使用 NVLink Switch 系统连接为一个 NVLink 5 域。然后使用 InfiniBand 或以太网将该 NVLink 域连接到其他域（例如，其他 NVL72 机架）或外部存储（尽管跨机架 InfiniBand 或以太网通信的性能将低于机架内 NVLink/NVSwitch 通信）。

简而言之，像 NVIDIA 的 ConnectX 和 BlueField DPU 这样的 InfiniBand 和以太网 NIC 通常与 NVLink 一起使用。它们提供机架间的高带宽连接，并利用 DPU 上的网内计算卸载协议。

---

### 预集成机架设备 (Preintegrated Rack Appliance)

由于 NVL72 是如此复杂的系统，NVIDIA 将其作为一个预集成的机架“设备 (Appliance)”在单个机柜中交付。它组装好了所有 18 个计算节点、所有 9 个 NVSwitch 单元、内部 NVLink 布线、配电和冷却系统。其理念是，组织可以订购这样一个单元，到货即用。你只需将机架连接到设施电源，接上水冷接口，将 InfiniBand 线缆连接到你的网络，然后开机。

该系统本质上是开箱即用的，只需极少的设置即可开始运行 AI 工作负载。不需要用 NVLink 单独连接 72 个 GPU，因为 NVIDIA 已经在机架内为你做好了。甚至液冷设置也是自包含的，我们稍后会讨论。

这种设备化方法加速了部署，并确保系统构建正确且经过 NVIDIA 验证。该机架还包括其 NVIDIA Base Command Manager 集群管理软件——以及用于集群作业调度和编排的 **SLURM** 和 **Kubernetes**。

简而言之，NVL72 机架设计用于直接放入你的环境，并开箱即可运行生产级 AI 工作负载。它不需要任何手动安装或复杂配置。

---

### 共封装光学器件 (Co-Packaged Optics)：网络硬件的未来

随着网络数据吞吐率攀升至 800 Gbit/s、1.6 Tbit/s 及更高，NVIDIA 已开始将**硅光子 (Silicon Photonics)** 和**共封装光学器件 (CPO)** 集成到其网络硬件中。这包括 Quantum-X800 InfiniBand 和 Spectrum-X800 以太网平台。这些平台出厂即具备 800 Gb/s 端到端连接和网内计算特性（如 SHARP）。有了 CPO，光发射器被集成在紧邻交换机芯片的位置。这极大地缩短了电路路径，实现了机架间更高的带宽连接，降低了功耗，并提高了整体通信效率。

在实践中，像 CPO 这样的技术正在铺平道路，将成百上千个机架（AI 工厂）连接成单一的统一结构，其中机架间带宽不再是瓶颈。这种光网络进步对于确保网络能跟上超大规模 GPU 所需的高性能、机架间带宽至关重要。

总结一下，在 NVL72 机架内部，NVIDIA 使用 NVLink 和 NVSwitch 在 72 个 GPU 之间创建了一个极快、全对全连接的网络。这些互连是如此之快且一致，以至于对于许多集合操作，GPU 实际上表现得像一个单元。在机架之外，高速 NIC（如 InfiniBand 或以太网）将机架连接到其他机架或存储，并配有 DPU 以高效管理数据移动。

NVL72 是一个极其强大的独立系统，也是更大的 AI 超级计算机或 **AI 工厂**的基本构建模块。AI 工厂——由多个此类机架组成的大规模 AI 数据中心——的概念现已成为现实。NVIDIA 与 HPE 和 Supermicro 等 OEM 和系统供应商合作，供应 GB200 NVL72 系统。NVIDIA 的硬件和网络路线图直指 AI 工厂愿景的实现。简而言之，NVL72 展示了协同设计能走多远：GPU、网络和物理机架硬件携手构建，旨在尽可能无缝且高效地扩展到数百万个 GPU。

---

## 计算密度与电力需求 (Compute Density and Power Requirements)

NVL72 机架在计算方面密度惊人，这意味着单个机架的功耗极高。一个满载的 NVL72 在最大负载下可消耗高达 **~130 kW** 的电力。这比 NVIDIA 上一代 AI 机架（消耗约 50–60 kW）高出 2 倍以上。将 72 个尖端 GPU——以及所有配套硬件——塞进一个机架，推向了数据中心基础设施所能承受的极限。

要向 NVL72 机架供应 130 kW，你不能只使用单一的标准供电馈线。数据中心通常会配置多条高容量电路来提供这种电力。例如，一个数据中心可以部署两条完全独立的供电馈线。在这种情况下，每条馈线的规格都要足以承载整个机架负载，以防其中一条发生故障。

如果一条馈线掉线，剩余电路可以支撑全部 130 kW 的消耗，以避免电路熔断。这种冗余是重要的保护措施。否则，电力中断可能会令你长达数月的训练作业中途夭折。

在机架内部，电力被分配到每个 1U 计算节点的电源。电力从交流 (AC) 转换为直流 (DC) 供本地电子设备使用。NVL72 中的每个计算节点包含两个 Grace Blackwell 超级芯片，它们合计消耗约 **6 kW**。对于 18 个计算节点，总功耗为 ~110 kW。NVSwitch 托盘、网络交换机、风冷和水冷泵占了 ~20 kW，整个 NVL72 机架总共消耗 130 kW。

典型数据中心使用的电压（例如 415 V 三相交流电）电流巨大，因此一切都要为高安培数设计。运营商必须仔细规划以托管这样的机架，通常需要专用的配电单元 (PDU) 和仔细的监控。电力瞬变也是一个考量因素，因为 72 个 GPU 从空闲爬升到全功率时，可能在几毫秒内迅速汲取数十 kW 的电力。好的设计将包括电容器或时序控制以避免巨大的电压降。

系统可能会以极小的时间间隔错开 GPU 的加速时钟，以免它们都在完全相同的微秒内出现峰值，从而平滑浪涌。正是这些电气工程细节让一个 130 kW 的机架变得可控。

称这个处于高密度计算前沿的 NVL72 机架为**小型变电站**一点也不夸张。8 个这样的机架组合起来共有 576 个 GPU，将消耗近 **1 MW** 的电力（8 机架 × 130 kW/机架），这相当于一个小数据中心的全部容量！好的一面是，虽然 130 kW 对一个机架来说很多，但你得到的**每瓦性能 (Work done per watt)** 也很高。

如果一个 NVL72 替换了几个旧设备的机架，整体效率会更好。但你绝对需要基础设施来支持那种集中的电力汲取。任何托管 NVL72 机架的设施都必须确保它们有足够的电力容量和冷却能力，我们接下来将讨论这一点。

---

### 液冷与风冷的对比 (Liquid Cooling Versus Air Cooling)

在一个机架中冷却 130 kW 超出了传统风冷的能力范围。对着 72 个每个能散发 ~1,200 瓦热量的 GPU 吹风，需要飓风般的气流，这会极其吵闹且低效——更不用说排出的热气将是残酷的。因此，**液冷 (Liquid Cooling)** 是 NVL72 机架在此功率密度下运行的唯一实际解决方案。

NVL72 是一个全液冷系统。每个 Grace Blackwell 超级芯片模组和每个 NVSwitch 芯片都附有一个**冷板 (Cold Plate)**。冷板是一块带有内部管道的金属板，直接贴合在组件上。水基冷却液流过管道带走热量。所有这些冷板通过软管、歧管和泵连接，使冷却液在整个系统中循环。

通常，机架的每个节点会有快速断开接头 (Quick-disconnect couplings)，这样你可以在不溢出冷却液的情况下滑入或滑出服务器。机架有供应和回流连接口，连接到外部设施的冷冻水系统。通常，有一个称为 **冷却液分配单元 (CDU)** 的热交换器，要么内置于机架中，要么紧邻其旁。CDU 将热量从机架的内部冷却回路转移到数据中心的水循环中。

设施提供 20–30°C 的冷冻水。水通过热交换器吸收热量。升温后的水随后被泵回冷水机组或冷却塔再次冷却。在现代设计中，它们甚至可能运行温水冷却，其中进入系统的冷冻水为 30°C，离开时为 45°C。水随后可以通过蒸发冷却塔冷却，无需主动制冷，从而提高整体效率。关键在于，水或液体冷却剂单位流量携带的热量远多于空气，因此在狭小空间内高功率运行时，液冷要有效得多。

通过将 GPU 和 CPU 的温度保持在远低于风冷环境下的水平，液冷减少了 GPU 的**热节流 (Thermal Throttling)**。GPU 可以维持其最大时钟频率而不会触及温度上限。此外，让芯片运行得更冷可以提高可靠性，甚至提高效率，因为在较低温度下运行时电力泄漏更低。

NVL72 在负载下将 GPU 温度保持在 50–70°C 范围内，这对于如此耗电的设备来说是非常出色的。冷板和冷却回路经过精心设计，允许每个 GPU 向系统排放 1,000 W，每个 CPU 排放 500 W。此外，冷却液流速必须足以迅速带走这些热量。粗略估计显示，每分钟需要 150–200 升的流量，水温上升 10–12°C，以耗散约 130 kW 的热量。

系统无疑拥有针对冷却液温度、压力和泄漏检测的传感器和控制装置。如果通过滴漏或压力损失传感器检测到泄漏，系统可以迅速关闭或隔离该部分。建议使用自密封连接——或许还有一个二级容纳托盘——以最大限度地减少流体泄漏的风险。

这种机架级液冷曾经是稀罕物，但现在已成为这些大规模 AI 集群的标准。Meta、xAI 和 Google 等公司正在为其 AI 集群采用液冷，因为风冷根本无法支持这些系统汲取的巨大电力。

因此，虽然 NVL72 需要更复杂的设施，包括液冷回路，但许多数据中心现在建设时都考虑到了液冷。NVL72 机架凭借其内置的内部液冷，可以直接连接到冷却回路。

内部液冷的一个副作用是机架的**重量**。NVL72 机架在装满硬件和冷却液时重约 **3,000 磅 (1.3–1.4 吨)**。对于一个机架来说这极其沉重，大约相当于一辆小型汽车的重量，却集中在几平方英尺的地板上。带有高架地板的数据中心必须检查地板是否能支撑此载荷（以磅/平方英尺衡量）。通常，高密度机架被放置在加固的楼板上或由额外的支柱支撑。移动这样的机架需要像叉车这样的特殊设备。这也是部署考虑的一部分，因为你正在安装一台 AI 超级计算机，它伴随着独特的物理和后勤挑战。

NVIDIA 还以机架管理控制器的形式集成了管理和安全功能，该控制器监督冷却泵、阀门位置和电力使用等事项，并监控每个节点的状态。管理员可以通过接口与之交互，执行诸如跨所有节点更新固件或安全关闭系统等操作。

所有这些考量说明，NVL72 是在考虑到数据中心基础设施的情况下协同设计的。NVIDIA 与搞定电力输送和冷却的系统工程师，以及指定如何安装和运行这些东西的设施工程师协同工作，共同开发计算架构。这不仅仅关于快速芯片——而是关于交付一个平衡、可用的系统。

这种复杂性的回报是巨大的。通过推高电力和冷却的极限，海量的计算被集中到单个机架中，并转化为巨大的**每瓦计算量**。是的，130 kW 是很大的功率，但分摊到每个 GPU 或每万亿次浮点运算 (TFLOP)，与将相同的 GPU 分散到多个冷却效率较低的机架相比，它实际上是高效的。

---

### 实践中的性能监控与利用率 (Performance Monitoring and Utilization in Practice)

当你拥有一台如此强大的机器时，你会想确保你能充分利用它。有效地操作 NVL72 需要仔细监控性能、利用率和电力。NVIDIA 提供了像 **Data Center GPU Manager (DCGM)** 这样的工具，可以追踪每个 GPU 上的指标，如 GPU 利用率百分比、内存使用量、温度和 NVLink 吞吐量。

作为一名性能工程师，你会在训练运行和推理工作负载期间关注这些指标。理想情况下，你希望你的 GPU 在训练作业期间的大部分时间都接近 **100% 利用率**。如果你看到 GPU 利用率为 50%，那意味着有一半的时间它们处于空闲状态。也许存在数据加载瓶颈或同步问题。

同样，你可以监控 NVLink 的使用情况。如果你的 NVLink 链路频繁饱和，通信很可能是罪魁祸首。BlueField DPU 和 NIC 有它们自己的统计数据可供监控，以确保你在读取数据时没有使存储链路饱和。像 NVL72 这样的现代系统暴露了这种遥测数据。

电力监控也至关重要。在 ~130 kW 下，即使是很小的低效或配置错误也会浪费大量电力和金钱。系统可能会让你监控每节点或每 GPU 的电力汲取。如果不需要全性能，管理员可能会限制 GPU 的功率或时钟，以节省能源。

NVIDIA GPU 允许设置功率限制。例如，如果你运行的一个较小的作业不需要每一滴性能，你可以调低 GPU 时钟以提高效率——以每瓦性能衡量——且仍能满足你的吞吐量需求。这在此过程中可以节省数千瓦的电力。经过数周的训练，这可以转化为显著的储蓄和成本效率。

### 共享与调度 (Sharing and Scheduling)

另一个方面是 NVL72 上工作负载的共享和调度。很少有单一作业需要全部 72 个 GPU。你可能有多个团队或多个实验在 GPU 子集上运行。使用像 **SLURM** 或 **Kubernetes** 这样的集群调度器配合 NVIDIA 的插件，你可以划分出（比如）8 个 GPU 给一个用户，16 个 GPU 给另一个用户，48 个 GPU 给又一个用户——全部在同一个机架内。

此外，NVIDIA 的 **多实例 GPU (MIG)** 特性允许你在硬件层面将单个物理 GPU 分割成更小的 GPU。例如，一个拥有 180 GB GPU 内存的 Blackwell GPU 可以被分割成更小的块，并发运行许多小型推理作业。

每个 Blackwell GPU 支持最多七个完全隔离的 MIG 实例。这允许一个物理 GPU 被分割成最多七个拥有专用内存和 SM 的较小 GPU。MIG 的大小由产品代际固定。我们将在下一章深入探讨 MIG 分区的细节。

在实践中，对于如此大的 GPU，MIG 可能用于推理场景，你想在一个 GPU 上服务许多模型。BlueField DPU 的存在也启用了安全的多租户，因为 DPU 可以充当防火墙和虚拟交换机。这隔离了不同作业和用户的网络流量。这意味着组织可以安全地让不同部门甚至外部客户使用系统的分区而互不干扰——类似于云提供商如何通过安全的多租户隔离为多个客户划分大服务器。

从成本角度来看，像 NVL72 这样的系统是数百万美元的资产，每月可能消耗数万美元的电费。所以你真的想尽可能多地做有用功，或 **Goodput**。如果它闲置，那是大量的资本和运营成本浪费。这就是为什么随时间监控利用率很重要。你可以追踪已使用 GPU 小时数与可用小时数的对比。

如果你发现系统利用率不足，你可能想合并工作负载或将其提供给更多团队用于更多项目。一些组织实施内部计费模型，内部团队使用自己的预算支付每 GPU 小时的使用费。这鼓励了高效使用，并核算了电力和折旧成本。这种透明度确保人们珍视资源。

---

### 升级硬件的投资回报率 (ROI of Upgrading Your Hardware)

有人可能会问，投资这种尖端硬件是否值得。在分析投资回报率 (ROI) 时，答案往往归结为**每美元性能**。如果 NVL72 可以做（比如）四个老一代机架的工作，长期来看它实际上可能会省钱，无论是在硬件还是电力上。在本章早些时候，我们讨论了一个 Blackwell GPU 在吞吐量上如何能取代 2–3 个 Hopper GPU。这意味着如果你升级，完成同样的工作可能需要更少的总 GPU 数。

让我们分析一个快速案例研究。假设你目前有 100 个 H100 GPU 处理你的工作负载。你可能可以用 50 个 Blackwell GPU 来处理它，因为每个都快两倍以上（如果使用 FP8/FP4 则更多）。所以你会购买 50 个而不是 100 个 GPU。即使每个 Blackwell 比 H100 贵，购买一半数量也可能是成本中性甚至更好的。电力方面，100 个 H100 可能消耗 70 kW，而 50 个 Blackwell 可能为同样的工作消耗 50 kW。这是一个显著的电力节省。

一年下来，这种电力差异能节省数万美元。此外，更少的 GPU 意味着更少的服务器需要维护，这意味着这些服务器的 CPU、RAM 和网络开销更少，提供了进一步的节省。总而言之，升级到新硬件在某些情况下可以在 1–2 年内回本——特别是如果你有足够的工作让它们全天候忙碌。

这笔账显然取决于确切的价格和使用模式，但重点是，对于大规模部署，采用最新 AI 硬件的 ROI 可能非常高。除了有形的 ROI，还有无形的利益，比如使用单一强大的系统代替许多较小的系统，可以简化你的系统架构。这种简化通过降低功耗和减少网络复杂性提高了运营效率。

例如，不必因为内存限制而将模型拆分到多个旧 GPU 上，可以简化软件并降低工程复杂性。此外，拥有最新硬件确保你可以利用最新的软件优化，并跟上同样升级的竞争对手的步伐。没人想以对手一半的速度训练和服务模型。升级将提升你的性能，同时启用更大的模型、更快的迭代和更迅速的响应。

有效地运行 NVL72 既是硬件壮举，也是软件和管理挑战。硬件给了你难以置信的潜力，但取决于工程师通过监控性能、保持高利用率和智能调度作业来驾驭硬件的全部力量。

好消息是 NVIDIA 提供了丰富的软件栈来监控和改进性能，包括驱动程序、分析器、容器运行时和集群编排工具。在本书的其余部分，我们将看到如何优化软件以充分利用像 GB200/GB300 NVL72 这样的系统。目前的要点是，**当你获得一个盒子里装有百亿亿次级 (ExaFLOPS) 性能的 AI 系统时，你需要同样先进的策略来让每一次浮点运算和每一个字节都发挥作用。**

---

### 瞥见未来：NVIDIA 的路线图 (A Glimpse into the Future: NVIDIA’s Roadmap)

在撰写本文时，Grace Blackwell NVL72 平台代表了 AI 硬件的最先进水平。但 NVIDIA 已经在准备下一次飞跃。简要了解 NVIDIA 未来几年的硬件路线图是值得的，因为它显示了清晰的扩展模式。NVIDIA 打算继续在性能、内存和集成度上加倍下注。

**Blackwell Ultra 和 Grace Blackwell Ultra**

NVIDIA 的 Blackwell Ultra (B300) 和相应的 Grace Blackwell Ultra 超级芯片 (GB300) 是 NVL72 架构的直接升级版。每个 Blackwell Ultra B300 GPU 拥有比 B200 (180 GB) 大约多 **50% 的内存容量 (288 GB)**——以及 **1.5 倍** 更高的 AI 计算性能和更大的片上加速器，专门为注意力操作和降低精度（例如 NVFP4）设计。这转化为 Blackwell B300 产生比 B200 高 45-50% 的推理吞吐量。

一个 GB300 的 72-GPU 机架由 36 个 Grace Blackwell Ultra 模组（每个 2 GPU + 1 CPU）、~20.7 TB 的 HBM (72 × 288 GB) 和 ~18 TB 的 DDR (36 × 500 GB) 组成。合计起来，每个 GB300 NVL72 机架拥有 **~38 TB** 的快速内存。GB300 NVL72 Ultra 中的机架内 NVLink 和 NVSwitch 网络使用与 GB200 NVL72 相同的 NVLink 5 代。

简而言之，GB300 是 GB200 的进化升级，因为它使用相同的架构。然而，它拥有更多的一切，包括更多 SM、更高内存和更快时钟。

**Vera Rubin 超级芯片 (2026)**

以发现暗物质证据的女天文学家命名，**Vera Rubin 超级芯片 (VR200)** 是下一个主要的架构步骤。Vera 是 Grace CPU 的基于 ARM 的继承者，Rubin 是 Blackwell 的 GPU 架构继承者。NVIDIA 继续沿用超级芯片概念，将一个 Vera CPU 与两个 Rubin GPU 结合在单个模组 (VR200) 中，类似于 Grace Blackwell (GB200/GB300) 的配置。

Vera CPU 使用台积电的 **3nm** 半导体工艺，拥有更多 CPU 核心和运行速度约为 1 TB/s 的更快 LPDDR6 内存。Rubin GPU 支持运行速度约为 **13–14 TB/s** 的更高 GPU 高带宽内存 (HBM)。

NVLink 也预计将升级到第六代 **NVLink 6**，这将使 CPU 到 GPU 和 GPU 到 GPU 的链路带宽翻倍。还有猜测称 Vera Rubin 可能允许每机架更多节点——或每个 NVLink 域更多机架——以扩展超出八机架 GB200/GB300 NVL72 集群的 576 GPU 限制。

底线是，Vera Rubin 世代在大多数指标上又是 **~2 倍** 的跳跃，包括更多核心、更多内存、更多带宽和更多 TFLOPS。Rubin GPU 将 SM 计数增加到每裸片 ~200 个 SM。这可能会进一步增加效率改进。它们还可能集成新特性，如第二代 FP4 甚至实验性的 2-bit 精度，尽管目前这只是猜测。

另一个特别有趣的可能性是，由于 Rubin 的 288 GB HBM RAM 对于大型 AI 模型来说仍然是瓶颈，NVIDIA 可能会直接在 GPU 模组中为 GPU 集成一些二级内存。例如，他们可能会在 GPU 模组的基座上放置一些 LPDDR 内存，作为 GPU 的一个更大但更慢的内存池——独立于 Vera 的 CPU DDR 内存。

如果发生这种情况，单个 GPU 模组可能拥有 **~550 GB** (288 GB HBM + 256 GB LPDDR) 的总缓存一致性统一内存。这将进一步模糊 CPU 和 GPU 内存之间的界限，因为 GPU 将拥有自己的多层内存层级结构。无论这是否会在 Rubin GPU 这一代发生，这都是一个值得关注的方向。

总体而言，Vera Rubin 和 Vera Rubin Ultra 机架提供 GB200/GB300 NVL72 **5 倍**的性能。它们的运行功率也是 5 倍——每机架近 600 kW。VR200/VR300 NVL 系统配备了海量的每机架总 GPU HBM（所有 Rubin GPU 合计，每 GPU 288 GB HBM）加上数十 TB 的 CPU 内存。并且机架内的 NVLink 6 产生的通信开销比 NVLink 5 更少。

**Rubin Ultra 和 Vera Rubin Ultra (2027)**

遵循这一模式，Rubin (R300) 和 Vera Rubin 的“Ultra”版本将在原始版本发布一年后到来。一份报告建议 NVIDIA 届时可能会转向**四裸片 (Four-die)** GPU 模组。这将结合两个双裸片 Rubin 封装，将它们放在一起产生一个四裸片 Rubin GPU。这个 R300 Rubin Ultra GPU 模组在一个封装上有四个 GPU 裸片和 16 个 HBM 堆栈，单个 R300 GPU 模组总计 **1 TB HBM 内存**。四个裸片合计将双裸片 B300 模组的核心数翻倍。

特别是，Vera Rubin **NVL144** 系统在整个机架中拥有 144 个这样的裸片。这是 36 个超级芯片模组，每个模组四个裸片。还有一个 Vera Rubin **NVL576** 配置，完整系统中将拥有 4 倍的 GPU 数量以及多裸片封装。

到 2027 年，每个机架可能推向 3–4 exaFLOPS 的计算性能和合计 165 TB 的 GPU HBM RAM（288 GB HBM 每 Rubin GPU × 576 GPU）。虽然这些数字仍带点推测性质，但迈向拥有海量 exaFLOPS 计算能力和 TB 级 GPU HBM RAM 的超大规模 AI 系统的轨迹是清晰的。

**Feynman GPU (2028) 和每年翻倍的“某样东西”**

NVIDIA 已将后 Rubin 世代代号定为 **Feynman**，计划于 2028 年发布。细节稀缺，但 Feynman GPU 很可能会转向更精细的台积电 **2nm** 工艺节点。它可能会使用 **HBM5** 并在模组内包含更多 DDR 内存。或许它会将裸片数量从四个翻倍到八个。

到 2028 年，预计**推理**需求肯定会主导 AI 工作负载——特别是随着**推理 (Reasoning)** 能力在 AI 模型中的不断演进。推理所需的推断时计算量是以前非推理模型的数百或数千倍。因此，芯片设计可能会针对大规模推理效率进行优化，这可能包括更新颖的精度、更多片上内存和封装上光学链路以进一步提高 NVLink 的吞吐量。

NVIDIA 似乎在每一代都在翻倍某样东西，如果可能的话甚至是每年。一年翻倍内存，下一年翻倍裸片数量，再下一年翻倍互连带宽，以此类推。几年下来，这种翻倍的复合效应是巨大的。NVIDIA 的激进轨迹可见一斑：Blackwell 引入了双 GPU 裸片（每模组两个裸片而非一个），每链路 NVLink 双向带宽从 ~900 GB/s 翻倍至 ~1.8 TB/s，每 GPU 内存从 Blackwell 的 180 GB 增加到 Blackwell Ultra 世代的 ~288 GB。Rubin 和 Feynman 将进一步增加计算、内存和带宽。

NVIDIA 反复谈论 **AI 工厂**，机架就是 AI 模型的生产线。NVIDIA 设想通过其合作伙伴提供**“机架即服务” (Rack-as-a-Service)**，这样公司可以租赁超级计算机的一部分，而不是自己构建一切。这一趋势可能会继续，因为尖端硬件将作为集成的 Pod 交付，你可以直接部署。每一代都允许你换入新的 Pod 以翻倍容量、提高性能并降低成本。

对于我们性能工程师来说，重要的是硬件将不断解锁新的规模水平。今天不可行的模型可能在几年内变得常规。这也意味着我们将不得不不断调整我们的软件以利用诸如新精度格式、更大的内存池和改进的互连等事物。这是一个激动人心的时刻，因为前沿模型的进步与这些硬件创新紧密相连。

---

## 关键要点 (Key Takeaways)

以下创新共同使 NVIDIA 的硬件能够以前所未有的速度、效率和可扩展性处理超大 AI 模型：

**集成超级芯片架构**
> NVIDIA 将基于 ARM 的 CPU (Grace) 与 GPU (Hopper/Blackwell) 融合为单个超级芯片，创建了统一的内存空间。这种设计通过消除 CPU 和 GPU 之间手动数据传输的需求简化了数据管理。

**统一内存架构**
> 统一内存架构和一致性互连降低了编程复杂性。开发者可以编写代码而无需担心显式数据移动，这加速了开发并帮助他们专注于改进 AI 算法。

**超高速互连**
> 使用 NVLink（包括 NVLink-C2C 和 NVLink 5）和 NVSwitch，系统实现了极高的机架内带宽和低延迟。这意味着 GPU 通信几乎就像它们是一个大处理器的一部分，这对于扩展 AI 训练和推理至关重要。

**高密度、超大规模系统 (NVL72)**
> NVL72 机架在一个紧凑系统中集成了 72 个 GPU。这种整合设计通过结合高计算性能和巨大的统一内存池来支持海量模型，使在传统设置上不切实际的任务成为可能。

**先进的冷却和电源管理**
> NVL72 依赖复杂的液冷和强大的配电系统，每机架运行功率约为 130 kW（130 kW = 18 节点 × 6 kW 每节点 + ~20 kW NVSwitch/冷却/开销）。这种量的冷却和电力对于管理高密度、高性能组件和确保可靠运行至关重要。

**显著的性能和效率收益**
> 与 Hopper H100 等前代产品相比，Blackwell GPU 在计算和内存带宽上提供了大约 2–2.5 倍的改进。这导致了训练和推理速度的显著提升——在某些使用 Blackwell FP4 Tensor Cores 和 Transformer Engine 的案例中推理速度快达 30 倍——以及通过减少 GPU 数量带来的潜在成本节约。

**现代软件栈支持**
> NVIDIA 的软件和框架继续演进以充分利用其最新硬件并支持最新的协同设计系统优化。这包括统一内存管理和原生 FP8/FP4 精度支持。因此，工程师可以用最少的代码更改利用系统的全部性能。

**面向未来的路线图**
> NVIDIA 的开发路线图（包括 Blackwell Ultra, Vera Rubin, Vera Rubin Ultra, 和 Feynman）承诺持续翻倍关键参数，如计算吞吐量和内存带宽。这一轨迹旨在支持未来更大的 AI 模型和更复杂的工作负载。

---

## 结论 (Conclusion)

NVIDIA NVL72 系统——凭借其 Grace Blackwell 超级芯片、NVLink 结构和先进的冷却——通过了 AI 硬件设计的顶峰。在本章中，我们看到了每个组件是如何为了加速 AI 工作负载这一单一目标而协同设计的。CPU 和 GPU 融合为一个单元以消除数据传输瓶颈并提供巨大的统一内存。

数十个 GPU 通过超快网络连接在一起，使它们表现得像一个巨大的 GPU，通信延迟极小。内存子系统被扩展和加速以喂饱 GPU 核心的贪婪胃口。甚至电力输送和热管理也被推向新高度，以允许这种计算密度。

结果是一个单个机架就能交付以前只能在多机架超级计算机中看到的性能。NVIDIA 采用了整个计算栈——芯片、板卡、网络、冷却——并对其进行了端到端的优化，以允许在超大规模下训练和服务海量 AI 模型。

但这样的硬件创新也带来了挑战，因为你需要专门的设施、仔细规划电力和冷却，以及复杂的软件来充分利用它们。但回报是巨大的。研究人员现在可以实验前所未有的规模和复杂度的模型，而无需等待数周或数月的结果。在旧基础设施上可能需要一个月训练的模型，在 NVL72 上可能几天就能训练好。曾经几乎无法交互（每查询数秒）的推理任务现在变成了实时（毫秒级）现实。这为以前不切实际的 AI 应用打开了大门，如多万亿参数的交互式 AI 助手和智能体。

NVIDIA 快速的路线图表明这只是开始。Grace Blackwell 架构将演变为 Vera Rubin 和 Feynman 及以后。正如 NVIDIA CEO 黄仁勋所描述的那样，“AI 正以光速前进，公司正在竞相建立 AI 工厂，以便能够扩展以满足推理 AI 和推理时间扩展的处理需求。”

NVL72 及其继任者是 AI 工厂的核心。它是将翻越数据大山以产出惊人 AI 能力的重型机械。作为性能工程师，我们站在这一硬件创新的肩膀上。它给了我们巨大的原始能力，而我们的角色就是通过开发能最大化硬件潜力的软件和算法来驾驭这一创新。

在下一章，我们将从硬件过渡到软件。我们将探索如何在像 NVL72 这样的系统上优化操作系统、驱动程序和库，以确保这些令人惊叹的硬件没有被未充分利用。在后续章节中，我们将通过内存管理和分布式训练/推理算法来补充软件架构。

本书的主题是**协同设计**。就像硬件是为 AI 协同设计的一样，我们的软件和方法也必须协同设计以利用硬件。现在我们对硬件基础有了清晰的理解，我们已经装备好深入软件策略以提高 AI 系统性能。AI 超级计算时代已经到来，充分利用它将是一次激动人心的旅程。

让我们深入其中！







**译境** 已就位。

第三章是实战性极强的一章，从“硬件”上浮到了“系统软件”层面。作者像一位经验丰富的老技师，手把手教你如何调校操作系统、容器和编排系统，以避免昂贵的 GPU 仅仅因为 CPU 喂数据太慢或 OS 调度不当而空转。

本章涵盖了从底层的 NUMA 亲和性、大页内存，到中间层的 Docker 优化，再到上层 Kubernetes 调度的全栈调优。

以下是 **第三章：针对 GPU 环境的操作系统、Docker 及 Kubernetes 调优** 的中文译本。

---

# 第三章：针对 GPU 环境的操作系统、Docker 及 Kubernetes 调优

即使拥有优化得极其完美的 GPU 代码和库，系统级的瓶颈依然可能限制大规模 AI 训练的性能。**最快的 GPU 也取决于向它输送数据和指令的环境。** 在本章中，我们将探讨如何调校操作系统 (OS) 和容器运行时，让 GPU 发挥出全部潜能。

我们将从探索基础的 GPU 软件栈开始。随后，深入研究关键的 CPU 和内存优化，如 **NUMA 亲和性 (NUMA Affinity)** 和**大页内存 (Hugepages)**。这些优化确保数据能高效地从存储流经 CPU，最终到达 GPU。与此同时，我们将讨论关键的 GPU 驱动设置，如**持久化模式 (Persistence Mode)**、**多进程服务 (MPS)** 和 **多实例 GPU (MIG)** 分区。这些功能通过减少开销和有效地同步资源，帮助维持最大的 GPU 利用率。

利用 **NVIDIA Container Toolkit**、容器运行时、**Kubernetes Topology Manager** 和 **Kubernetes GPU Operator** 等解决方案，你可以为 GPU 环境构建一个统一且高度优化的软件栈。这些解决方案实现了跨单节点和多节点 GPU 环境的高效资源分配和工作负载调度——并确保 GPU 能力被充分利用。

在此过程中，你将建立起对“为什么这些优化很重要”的直觉。本质上，它们**最小化了延迟，最大化了吞吐量，并确保你的 GPU 始终有数据可跑，并在峰值性能下运行。** 结果就是构建出一个稳健、可扩展的系统，为训练和推理工作负载带来显著的性能提升——以及极高的**有效吞吐量 (Goodput)** 百分比。

---

## 操作系统 (Operating System)

操作系统 (OS) 是万物运行的基石。GPU 服务器通常运行 Linux 发行版，如 Ubuntu Server LTS 或 Red Hat，并配备支持最新 GPU 硬件的更新内核。NVIDIA 驱动程序安装的内核模块会创建设备文件，如 `/dev/nvidia0`、`/dev/nvidia1` 和 `/dev/nvidia2`——每个 GPU 一个。驱动程序还会创建 `/dev/nvidiactl` 用于驱动控制操作，`/dev/nvidia-uvm` 用于统一虚拟内存，以及 `/dev/nvidia-modeset` 用于模式设置和缓冲区管理。

OS 管理着 CPU 调度、内存、网络和存储——所有这些都应针对高 GPU 吞吐量进行调优。因此，OS 的配置应避免干扰 GPU 任务。例如，GPU 节点应**禁用交换分区 (Swapping)** 或将 `vm.swappiness` 设置为 0，以避免任何操作系统发起的内存交换干扰 GPU 工作负载。作为性能工程师，我们的部分工作就是调整这些 OS 设置，让 GPU 处于最佳性能状态。

以 GPU 为核心的服务器可能需要运行额外的守护进程（后台进程），例如 **NVIDIA Persistence Daemon**，以保持 GPU 驱动和硬件上下文处于加载和就绪状态——即使当前没有 GPU 作业在运行。此外，**Fabric Manager** 管理 GPU 互连拓扑，而 **NVIDIA Data Center GPU Manager (DCGM)** 则负责监控 GPU 系统健康指标。

---

## NVIDIA 软件栈 (NVIDIA Software Stack)

运行一个多 PetaFLOP 的 GPU 集群不仅仅涉及编写高层的 PyTorch、TensorFlow 或 JAX 代码。支撑 GPU 运行的是一整套软件栈，每一层都会影响性能。图 3-1 展示了开发和生产化现代 LLM 工作负载常用的一套框架、库、编译器、运行时和工具，包括 PyTorch, cuDNN, cuBLAS, CUTLASS, CUDA C++, nvcc, 和 CUDA Runtime API（如 CUDA 工具、驱动等）。

此外，NVIDIA GPU 和 CUDA 生态系统拥抱 Python 库，允许你使用 **OpenAI 的 Triton** 领域特定语言 (DSL) 和 **NVIDIA 的 Warp** 框架在 Python 中创建 CUDA 内核——以及利用 NVIDIA 的 **CUDA Python**、**cuTile** 和 **CUTLASS** 库。

*(图 3-1: 用于开发和生产化现代 LLM 工作负载的常用框架、库、编译器、运行时和工具集)*

### GPU 驱动程序 (GPU Driver)

位于底部的是 NVIDIA GPU 驱动程序，它是 Linux OS 和 GPU 硬件之间的接口。驱动程序管理底层的 GPU 操作，包括设备上的内存分配、GPU 核心上的任务调度以及多租户使用的 GPU 分区。

GPU 驱动程序开启 GPU 的各项功能并保持硬件有活可干。保持 NVIDIA 驱动程序更新非常重要。新的驱动版本通常能解锁性能提升，并支持最新的 GPU 架构和 CUDA 特性。

随驱动程序附带的工具（如 `nvidia-smi`）允许你监控温度、测量利用率、查询纠错码 (ECC) 内存状态，并启用不同的 GPU 模式（如持久化模式）。

### CUDA Toolkit 和 Runtime

位于驱动程序之上的是 CUDA Runtime 和库，统称为 **CUDA Toolkit**。该工具包包含 CUDA 编译器 `nvcc`，用于编译 CUDA C++ 内核。编译后，CUDA 程序链接到 CUDA 运行时 (`cudart`)。CUDA 运行时直接与 NVIDIA 驱动程序通信，以在 GPU 上启动工作并分配内存。

此外，CUDA Toolkit 提供了许多优化库：用于神经网络原语的 **cuDNN**，用于线性代数的 **cuBLAS**，用于多 GPU 通信的 **NCCL** 等。因此，使用支持你 GPU 计算能力 (Compute Capability, CC) 的最新 CUDA Toolkit 版本至关重要，因为最新的工具包拥有针对你 GPU 的最新编译器优化和库。我们将在后续章节更详细地介绍 CUDA 编译器和编程模型——以及 CUDA（和 PyTorch）优化。

### 跨 GPU 硬件代际的 CUDA 向前和向后兼容性

NVIDIA GPU 编程模型的一个重要特性是其跨硬件代际的兼容性。当你编译 CUDA 代码时，生成的二进制文件包含虚拟的（或中间的）**PTX** 代码以及物理设备代码（如 ARM, x86, GPU 指令），如图 3-2 所示。

*(图 3-2: 使用 nvcc 将 CUDA 程序编译为 PTX——并最终编译为 GPU 目标设备的底层指令)*

这允许较新的 GPU **即时编译 (JIT compile)** PTX，使你的程序能在未来的架构上运行——同时也允许较新的 GPU 执行为旧架构编译的旧二进制代码。这种兼容性是通过 NVIDIA 的 **Fatbinary (胖二进制)** 模型实现的，该模型包含用于未来兼容性的 PTX 和用于已知架构的架构特定 CUDA 设备代码二进制 (**CUBIN**)。

**CUBIN** 是 `nvcc` 使用 `-cubin` 选项生成的二进制文件。它包含针对特定 NVIDIA 架构编译的 GPU 流式汇编 (SASS) 指令。它被打包进 Fatbinary 以供 CUDA 驱动在运行时加载。与作为中间、向前兼容表示的 PTX 不同，CUBIN 二进制文件允许在已知 GPU 架构上直接执行。当 CUBIN 与 PTX 一起包含在 Fatbinary 中时，它既支持为未来 GPU JIT 编译 PTX，也支持在较新硬件上运行旧的 CUBIN 代码。

简而言之，当嵌入 PTX 时，CUDA 提供了**向前兼容性**，因为驱动程序可以在运行时为较新的架构 JIT 编译 PTX。CUBIN 对象是特定于架构的，不具备对未来 GPU 架构的向前兼容性，因此你应该包含 PTX 或发布包含当前架构 SASS 和用于向前兼容的 PTX 的 Fat binaries（又称 "fatbinaries" 或简称 "fatbins"）。

### C++ 和 Python CUDA 库

虽然大多数 CUDA 工具包库是 C++ 的，但 NVIDIA 目前面向 Python 的选项包括 **CUDA Python**（例如，底层驱动和运行时访问）；**cuPyNumeric**、**CuTe DSL**、**cuTile** 和 **CuPy** 用于数组编程；以及用于在 Python 中编写 GPU 内核的 **NVIDIA Warp**。**CUTLASS** 是一个 C++ 模板库，由 cuBLAS 等库在底层使用，而非直接的 Python 库。

虽然大多数 CUDA Toolkit 库基于 C++，但越来越多的以 "Cu" 为前缀且构建于 C++ 工具包之上的 Python 库正从 NVIDIA 涌现。例如，**cuTile** 和 **cuPyNumeric** 是 2025 年初推出的 Python 库。它们旨在降低 Python 开发者使用 CUDA 构建 NVIDIA GPU 应用程序的门槛。

**cuTile** 是一个 Python 库，旨在通过将大矩阵分解为更小、更易管理的子矩阵（称为 Tiles/瓦片）来简化 GPU 上的大矩阵处理。它提供了一个高层的、基于 Tile 的抽象，使得执行块状计算、优化内存访问模式和高效调度 GPU 内核变得更加容易。通过将大矩阵划分为 Tiles，cuTile 帮助开发者充分利用 GPU 的并行性，而无需手动管理底层细节。这种方法可以提高缓存使用率，并在需要密集矩阵计算的应用中提升整体性能。

**cuPyNumeric** 是流行的 NumPy Python 库的**直接替换品 (Drop-in replacement)**（`import cupynumeric as np`），它利用了 GPU。它提供了与 NumPy 几乎相同的函数、方法和行为，因此开发者通常只需极少的代码更改即可切换到它。在底层，cuPyNumeric 利用 CUDA 在 GPU 上并行执行操作。这为计算密集型任务（如大规模数值计算、矩阵运算和数据分析）带来了显著的性能提升。通过将工作卸载到 GPU，cuPyNumeric 加速了计算并提高了处理海量数据集的应用效率。其目标是降低 Python 开发者利用 GPU 算力的门槛，无需学习全新的接口，使其成为高性能计算中 NumPy 的强大替代品。

另一个值得注意的基于 Python 的编程模型是 OpenAI 的开源 **Triton** 语言和编译器。Triton 是一种 Python DSL，允许在 Python 中编写自定义 GPU 内核。虽然不是 NVIDIA 的库，但 Triton 通过允许开发者直接在 Python 中编写高性能内核来补充 CUDA。

我们将在后面的章节中介绍 Triton 和各种基于 Triton 的优化，但现在只需知道 Triton 在许多情况下减少了对手写 CUDA C++ 的需求。并且它已集成到 PyTorch 的编译器后端，可以自动优化和融合 GPU 操作以获得更好的性能。现在让我们转向 PyTorch 的讨论。

### PyTorch 和更高级别的 AI 框架

一些建立在 CUDA 之上的流行 Python 框架包括 PyTorch、TensorFlow、JAX 和 Keras。这些框架提供了用于深度学习的高级接口，同时利用了 NVIDIA GPU 的能力。本书主要关注 PyTorch 的编译和图优化特性，包括 `torch.compile` 栈。

PyTorch 编译器栈由 **TorchDynamo**、**AOT Autograd** 和后端（如 **TorchInductor** 或 Accelerated Linear Algebra [XLA]）组成，它们自动捕获并优化你的模型。TorchInductor 是最常见的后端，它在底层使用 OpenAI 的 Triton。Triton 会针对你的特定 GPU 和系统环境融合内核并执行内核自动调优，我们将于第 14 章对此进行介绍。

当你使用 GPU 对 PyTorch 张量执行操作时，它们从 CPU 移动到 GPU，这看起来像是一个简单的 Python 调用。然而，这个单一调用实际上被转换为利用各种 CUDA 库的一系列 CUDA 运行时调用，如图 3-3 所示。

*(图 3-3: 从 PyTorch 代码到 GPU 设备的数据流)*

例如，当你执行矩阵乘法时，PyTorch 将这些任务委托给像 cuBLAS 这样的库。cuBLAS 是 CUDA Toolkit 的一部分，针对 GPU 执行进行了优化。在幕后，PyTorch 确保像前向和后向传播这样的操作是使用底层的、优化的 CUDA 函数和库执行的。

简而言之，PyTorch 抽象掉了直接 CUDA 编程的复杂性，允许你编写直观的 Python 代码，这些代码最终调用高度优化的 CUDA 例程，兼顾了开发易用性和高性能。我们将在第 4 章和第 5 章讨论 CUDA 编程和优化——以及在第 9 章讨论 PyTorch 优化。

所有这些组件——OS、GPU 驱动、CUDA Toolkit、CUDA 库和 PyTorch——必须协同工作，创造理想的基于 GPU 的开发环境。当研究人员提交训练作业时，调度器预留节点，OS 使用 NVIDIA 驱动程序提供 GPU 设备和内存分配，容器提供正确的软件环境（包括优化的、硬件感知的 CUDA 库）。用户代码（例如 PyTorch, TensorFlow, JAX）使用这些 CUDA 库，最终与驱动程序和硬件进行通信。

本章描述的优化旨在使该堆栈的每一层尽可能高效。它们将帮助 GPU 忙于实际有用的训练和推理工作——而不是让 GPU 等待 CPU、等待内存或磁盘 I/O，或者等待其他 GPU 同步。

一个调优良好的系统能确保分布在数十个 GPU 上的模型不会被 I/O 或 OS 开销所瓶颈。系统级调优常因模型优化而被忽视，但系统级优化可以产生可观的性能收益。在某些情况下，仅通过对 OS 级配置进行微小的调整，你就能获得两位数的百分比提升。在大型 AI 项目的规模下，这可以节省数万或数十万美元的计算时间。

---

## 为 GPU 环境配置 CPU 和 OS (Configuring the CPUs and OS for GPU Environments)

GPU 无法达到全利用率的最常见原因之一是 CPU 没有给它们喂食足够的有用工作。在典型的训练循环中，CPU 负责准备下一批数据，包括从磁盘加载数据、Token 化数据、转换数据等。此外，CPU 还负责分发 GPU 内核并协调线程和进程。

如果这些主机端任务很慢——或者 OS 调度得很糟糕——昂贵的 GPU 可能会发现自己处于空闲状态，无所事事地空转晶体管，等待下一个任务或下一批数据。为了避免这种情况，我们需要优化 CPU 和 OS 处理 GPU 工作负载的方式。

这些优化包括设置 CPU 亲和性以避免跨 NUMA 节点流量，确保正确的核心处理正确的数据；使用内存分配策略以避免 NUMA 惩罚；以及应用 OS 级更改以消除不必要的延迟。这样，GPU 就永远不会因为缺乏数据而饥饿。这其中一部分涉及将后台守护进程和 OS 任务隔离在它们自己的核心上——并远离那些为 GPU 供料的核心，我们要对此进行讨论。

### NUMA 感知与 CPU 绑定 (NUMA Awareness and CPU Pinning)

现代服务器 CPU 拥有数十个核心，并且通常被拆分为多个 **NUMA 节点**。一个 NUMA 节点是一组在物理上彼此靠近的 CPU、GPU、网络接口控制器 (NIC) 和内存的逻辑分组。意识到系统的 NUMA 架构对于性能调优至关重要。访问单个 NUMA 节点内的资源比访问其他 NUMA 节点中的资源要快。

例如，如果运行在 NUMA 节点 0 中的 CPU 上的进程需要访问 NUMA 节点 1 中的 GPU，它需要通过节点间链路发送数据，这会产生更高的延迟。事实上，跨越 NUMA 节点时，内存访问延迟几乎会翻倍。

> **注意：** 在基于 Grace 的超级芯片（如 GH200 和 GB200）上，CPU 和 GPU 通过 NVLink-C2C 连接，这提供了高达 ~900 GB/s 的 Grace 与其配对加速器之间的一致性 CPU-GPU 内存访问。Linux 仍然将 CPU DRAM 视为 CPU NUMA 内存，将 GPU HBM 视为设备内存。因此，尽管一致性减少了软件开销，你仍应继续将 CPU 线程绑定到本地 Grace CPU 并尊重数据局部性。

在许多双插槽系统上，远程内存访问延迟可能显著高于本地内存访问。在一个实验中，本地 NUMA 节点内存访问延迟约为 **80 ns**，而远程（跨节点）内存访问延迟约为 **139 ns**。这大约增加了 75% 的延迟，这是本地和远程 NUMA 节点内存访问速度之间的巨大差异。

通过将进程绑定到与其 GPU 处于同一 NUMA 节点的 CPU 上，我们可以避免这种额外开销。例如，你可以使用 `numactl --cpunodebind=<node> --membind=<node>` 将 CPU 线程和内存分配都绑定到 GPU 的本地 NUMA 节点。稍后你将了解到更多相关信息。核心思想是**让 CPU 执行和内存访问保持在它所服务的 GPU 的本地范围内。**

> 虽然 Linux 包含了基本的 NUMA 平衡机制，但这对于性能关键型的 AI 工作负载通常是不够的。默认情况下，进程可能会在 NUMA 节点间迁移。这将导致远程内存访问带来的额外延迟。因此，**显式地**将进程和内存绑定到与本地 GPU 相同的 NUMA 节点非常重要。你可以使用 `numactl`, `taskset`, 或 `cgroups` 来实现，我们稍后会展示。

要显式指定 NUMA 亲和性，你需要将进程或线程“钉 (Pin)”在连接到与 GPU 相同 NUMA 节点的特定 CPU 上。这种类型的 CPU 亲和性称为 **CPU Pinning (CPU 绑定)**。假设一个节点中有 8 个 GPU，其中 4 个 GPU 连接到 NUMA 节点 0，另外 4 个连接到 NUMA 节点 1。

如果你启动 8 个训练进程，每个 GPU 一个，你应该将每个训练进程绑定到一个 CPU 核心——或一组 CPU 核心——这些核心连接到与 GPU 相同的 NUMA 节点。在这种情况下，GPU 0–3 连接到 NUMA 节点 0，GPU 4–7 连接到 NUMA 节点 1 的核心，如图 3-4 所示。

*(图 3-4: 节点中的 8 个 GPU，其中 4 个连接到 NUMA 节点 0，另外 4 个连接到 NUMA 节点 1)*

这样，当一个 CPU 进程想要向 GPU 4 输送数据时，它应该运行在连接到 NUMA 节点 1 的 CPU 上，因为 GPU 4 连接在 NUMA 节点 1 上。Linux 提供了工具来做到这一点，包括 `numactl --cpunodebind=<node> --membind=<node>`，它启动一个绑定到给定 NUMA 节点的进程。

你也可以使用 `taskset` 将进程绑定到特定的核心 ID。以下是一个使用 `numactl` 将 `train.py` 脚本绑定到与 GPU 4 相同的 NUMA 节点 1 的 CPU 上的示例：

```bash
numactl --cpunodebind=1 --membind=1 \
python train.py --gpu 4
```

这假设我们知道 NUMA 节点 ID，并且我们只将脚本绑定到一个 GPU。将 `train.py` 绑定到未知 NUMA 节点上的多个 GPU 会稍微复杂一些。下面的脚本使用 `nvidia-smi topo` 动态查询拓扑结构，并使用本地 NUMA 节点将脚本绑定到 GPU：

```bash
#!/bin/bash
for GPU in 0 1 2 3; do
    # 查询此 GPU 的 NUMA 节点
    NODE=$(nvidia-smi topo -m -i $GPU \
           | awk '/NUMA Affinity/ {print $NF}')
    
    # 启动绑定到该 NUMA 节点的训练进程
    numactl --cpunodebind=$NODE --membind=$NODE \
    bash -c "CUDA_VISIBLE_DEVICES=$GPU python train.py --gpu $GPU"
done
```

在这里，我们使用 `topo -m` 获取 CPU 和 NUMA 亲和性。然后我们从 NUMA Affinity 列中提取单个节点 ID。最后，我们将 `--cpunodebind` 和 `--membind` 都绑定到该节点，以确保进程的线程和内存分配停留在 GPU 的 NUMA 域本地。

许多深度学习框架也允许你通过编程方式设置线程亲和性。例如，PyTorch 的 `DataLoader` 暴露了 `worker_init_fn`，因此你可以在初始化期间为每个 worker 进程设置 CPU 亲和性，如下所示：

```python
# (此处省略部分 Python 代码引用，核心逻辑是在 worker_init_fn 中使用 psutil 和 libnuma 绑定 CPU 和内存)
# 关键部分：
def set_numa_affinity(node: int):
    """将当前进程绑定到给定 NUMA 节点的 CPU 和内存。"""
    cpus = get_numa_cpus_for_node(node) # 重要：目标节点的 CPU 列表
    psutil.Process(os.getpid()).cpu_affinity(cpus)
    _libnuma.numa_run_on_node(node)
    _libnuma.numa_set_preferred(node)
    # ...
```

此脚本将主训练进程和每个 `DataLoader` worker 进程绑定到 GPU 的本地 NUMA 节点，以防止跨 NUMA 内存访问。在 `DataLoader` 中，我们传递了一个基于闭包的 `worker_init_fn`，它在每个 worker 内部重新应用预计算的 NUMA 绑定。我们在 worker 中这样做而没有触及任何 CUDA API。

在启动时，进程使用 NVML 将当前 GPU 映射到其 NUMA 节点和 CPU 亲和性掩码。如果可用，我们通过 `nvmlDeviceGetNUMANodeId` 直接读取节点。否则，我们从 GPU 的 CPU 亲和性掩码 (`nvmlDeviceGetCpuAffinity`) 推导出来。如果 NVML 不可用或未暴露节点，我们回退到内核的 sysfs 条目 `/sys/bus/pci/devices/<PCI_ID>/numa_node`。作为最后的手段，我们使用进程当前的首选节点。

然后我们从 `/sys/devices/system/node/node<N>/cpulist` 计算该节点的 CPU 列表，并使用 `psutil` 将 CPU 亲和性应用到这些核心。我们还使用 `libnuma` (`numa_run_on_node` + `numa_set_preferred`) 将所有未来的分配绑定到该节点。

因为某些启动器、容器运行时或内核不能可靠地将 NUMA 策略传播给子进程，我们在每个 fork 出来的 worker 中显式地重新应用并验证绑定。仅仅依赖继承是不安全的。

记得设置 `pin_memory=True` 并在主机到设备 (H2D) 拷贝上使用 `non_blocking=True`，以便页锁定 (Page-locked) 的主机缓冲区停留在正确的 NUMA 节点上。首选 `persistent_workers=True` 以避免重新 fork worker 并导致 epoch 之间丢失亲和性。并且**不要**在 `worker_init_fn` 中调用 `torch.cuda.*`。相反，使用闭包或环境变量传递 GPU 索引。

结果是数据准备和批量加载完全在本地内存中发生。这样，你的 GPU 保持忙碌，永远不需要因为远程 NUMA 跳转而暂停。有了这段代码，你在任何安装了 `libnuma` 和 `numactl` 的 Linux 服务器上都能获得稳健的、拓扑感知的亲和性。

默认情况下，`numactl` 将其 CPU 和内存策略应用于进程，并记录为将其策略继承给所有 fork 的子进程。实际上，由 Python 框架生成的线程或 exec 的子进程并不总是在每个内核或 Linux 发行版上都能继承相同的设置。当使用框架管理的 worker 进程时，你应该在每个 worker 内部显式地重新声明 CPU 和内存策略。

在实践中，绑定 (Pinning) 可以消除不可预测的 CPU 调度行为。它确保关键线程（例如 GPU 的数据加载线程）不会在训练或推理过程中突然被操作系统迁移到不同 NUMA 节点的核心上。实际上，仅仅通过消除跨 NUMA 流量和 CPU 核心迁移，就有可能看到 **5%–10%** 的训练吞吐量提升。这也倾向于减少性能抖动和方差。

许多高性能 AI 系统会评估 CPU **同步多线程 (SMT)**（或通常所说的**超线程**）——有时会禁用它以获得更可预测的单核性能，但收益取决于工作负载。这些系统还可能通过设置 `isolcpus` 内核参数为 OS 后台任务专门保留少数核心，将它们与通用调度器隔离。你也可以使用 Kubernetes CPU 隔离功能处理系统守护进程。这确保了剩余核心完全致力于训练和推理线程并做有用的工作。

值得注意的是，对于像 NVIDIA Grace Blackwell 这样的集成 CPU-GPU 超级芯片，许多关于 CPU 到 GPU 数据传输的传统担忧得到了缓解，因为 CPU 和 GPU 通过 NVLink-C2C 暴露了一致的共享虚拟地址空间，同时 CPU DRAM 和 GPU HBM 保持为不同的内存池。这意味着像跨 NUMA 延迟这样的问题被最小化了，数据可以在 CPU 和 GPU 之间更直接地流动。

> NVIDIA 通过将 CPU 和 GPU 结合到单个超级芯片（如 Grace Blackwell 架构）上来解决 CPU-to-GPU 瓶颈并非巧合。在该设计中，CPU 和 GPU 甚至使用 NVLink-C2C 以高达 900 GB/s 的速度共享统一、一致的内存，这最小化了数据传输开销。预计 NVIDIA 将继续通过更多此类与软件和算法需求协同设计的硬件创新来解决系统瓶颈。

### NUMA 友好的内存分配与内存绑定 (NUMA-Friendly Memory Allocation and Memory Pinning)

默认情况下，进程将从它当前运行的 CPU 所在的 NUMA 节点分配内存。所以如果你将一个进程绑定到 NUMA 节点 0，它的内存自然会来自 NUMA 节点 0 的本地 RAM，这是理想的。然而，如果 OS 调度器迁移了线程——或者如果在你进行绑定之前已经分配了一些内存——你可能会陷入非理想的场景：在 NUMA 节点 0 运行的进程正在使用来自 NUMA 节点 1 的内存。在这种情况下，每一次内存访问都要跳到另一个 NUMA 节点，抵消了 CPU 绑定的好处。

为了避免这种情况，`numactl --membind` 选项强制从特定的 NUMA 节点分配内存，如前一节所述。在代码中，也有 NUMA API 甚至环境变量可以影响此配置。一般规则是**保持内存靠近 CPU，而 CPU 靠近 GPU**。这样，从内存到 CPU 再到 GPU 的数据移动链条都在单个 NUMA 节点内。这里是与之前相同的示例，但增加了 `--membind=1` 以强制从包含 NUMA 节点 1 的首选 NUMA 节点分配内存：

```bash
numactl --cpunodebind=1 --membind=1 python train.py --gpu 5 &
```

重要的是要注意，当你在 `numactl` 下启动一个进程时，其 CPU (`--cpunodebind`) 和内存策略 (`--membind`) 都会应用于该进程，并被其所有子进程继承。因此，你的训练脚本 fork 出的任何 worker 子进程将自动使用相同的 NUMA 内存绑定。但是，它们必须使用基于 fork 的模型创建。如果你切换到 `spawn` 启动方法，或者 `exec` 一个新程序，那些子进程不会继承父进程的内存策略。

此外，**Pinned Memory (锁页内存)**，也称为页锁定内存 (Page-locked Memory)，对于高效和直接的 GPU 访问至关重要。当内存被锁页时，OS 不会交换或移动它。这导致更快的直接内存访问 (DMA) 传输。从锁页主机内存拷贝数据到 GPU 比从常规可分页内存快 **2–3 倍**，因为 GPU 或 NIC 可以直接执行 DMA。

> 你可以使用安装的 CUDA 实用程序中的 `bandwidthTest --memory=pinned` 或 `bandwidthTest --memory=pageable` 来测试 CPU 内存和 GPU 内存之间的数据传输带宽。

事实上，这正是 NVIDIA GPUDirect 技术（如 **GPUDirect RDMA**）的基础，该技术允许像 InfiniBand 这样的 NIC 直接与 GPU 内存交换数据。同样，**GPUDirect Storage (GDS)** 允许 NVMe 驱动器将数据直接流式传输到 GPU 内存中，而无需额外的 CPU 开销。

深度学习框架提供了为数据加载器使用锁页内存的选项。例如，PyTorch 的 `DataLoader` 有一个标志 `pin_memory=True`，当设置为 true 时，意味着加载的批次将被放置在锁页 RAM 中，如图 3-5 所示。

*(图 3-5: 锁页内存 [又称 Page-locked 或 Nonpageable] 是一种不能被交换到磁盘的内存类型)*

内存锁页加速了 `tensor.to(device)` 操作，因为 CUDA 驱动程序不需要在运行时临时锁页。当你使用大批量大小或在每次迭代中读取大量数据时，这尤其有益。许多从业者注意到，仅仅在 PyTorch 中开启 `pin_memory=True` 就可以通过减少数据传输瓶颈和增加主机到设备的传输吞吐量，将性能提高 **10%–20%**。

简而言之，你应该确保你的数据加载器使用锁页内存（例如，PyTorch DataLoader 中的 `pin_memory=True`），并且在支持的硬件上启用了 GPUDirect RDMA 和 GDS。这将降低数据传输延迟。

值得注意的是，操作系统对用户可以锁定（Pin）多少内存有限制。这是通过 `ulimit -l <max locked memory>` 命令设置的。在容器化环境中，你可以相应地调整容器的安全上下文和 Docker `--ulimit memlock` 设置。这样，容器就可以锁定足够的内存。

> **提示：** 如果你计划使用大型、锁页缓冲区，请确保 `ulimit` 值很高——或者将其设置为 `unlimited`。否则分配可能会失败。通常，对于利用大量内存的大型 AI 工作负载和高性能计算 (HPC) 应用程序，将其设置为 unlimited。

### 透明大页 (Transparent Hugepages, THP)

除了锁定内存和将其绑定到 NUMA 节点之外，我们还应该讨论**透明大页 (THP)**。Linux 内存管理通常使用 4 KB 的页面，但是当你有进程使用数十或数百 GB 内存（如深度学习数据集、预取批次、模型参数等情况）时，管理数百万个微小页面是低效的。

**Hugepages (大页)**——2 MB 甚至 1 GB 的页面——可以通过使内存块变大来减少虚拟内存管理的开销。主要好处是**减少缺页中断 (Page Faults)** 和减轻**转换后备缓冲区 (TLB)** 的压力。

TLB 是 CPU 用于将虚拟地址映射到物理地址的缓存。更少、更大的页面意味着 TLB 可以用相同数量的条目覆盖更多内存，从而减少未命中。

大页通常产生适度的收益——通常在 **~3%–5%** 的吞吐量改进量级。它们通过减少缺页中断开销和 TLB 压力来做到这一点。启用 THP 在大多数系统上是一个简单的胜利，因为内核会自动用 2 MB 页面支持大型分配。在具有非常大内存池的场景中（例如，用于 I/O 的预分配锁页缓冲区），你也可以考虑使用 `vm.nr_hugepages` 或 `hugetlbfs` 进行显式大页分配，以获得更确定性的性能。

> **注意：** 重要的是要注意，THP 的后台**内存规整 (Compaction)** 可能会引入不可预测的暂停，这对于**延迟敏感型 LLM 推理**工作负载来说是灾难性的。Linux 默认配置为使用 THP，只要可能就自动分配 2 MB 页面。这通常就足够了，但值得针对你的工作负载进行测试。

你可以禁用 THP，但你需要手动分配和控制大页。这会带来额外的复杂性，但这对于像推理这样的低延迟工作负载可能是必要的。禁用 THP 后，你的系统将避免由内核驱动的碎片整理引起的停顿。

> 现代的共识是：对于大多数重视吞吐量的基于 GPU 的**训练**工作负载**启用** THP；而对于像**推理**这样重视延迟的工作负载，**完全禁用** THP (`transparent_hugepage=never`)——或使用 `madvise`。对于许多 Rank (GPU) 同时分配内存的分布式训练工作负载也是如此。

除了 CPU/内存绑定和大页之外，还有其他一些值得一提的 OS 级调整。这些包括线程调度、虚拟内存管理、文件系统缓存和 CPU 频率设置，我们将在接下来的几节中介绍。

### 调度器和中断亲和性 (Scheduler and Interrupt Affinity)

在一个繁忙的系统中，你要确保重要线程（如数据流水线线程）不会频繁被中断。Linux 默认使用完全公平调度器 (CFS)，它在大多数情况下工作良好。

但是，如果你有一个非常延迟敏感的线程向 GPU 输送数据，例如，你可以考虑为该线程使用**实时先进先出 (FIFO)** 或**轮转 (RR)** 优先级调度。这将确保高优先级线程运行而不会被普通优先级线程抢占。

然而，请谨慎使用，因为如果不当管理，实时线程可能会使其他进程饥饿。但在实践中，如果你已经将线程绑定到了专用核心，你通常不需要折腾实时线程优先级，但值得留意。

另一个选项是**隔离核心**或创建单独的 CPU 分区，以进一步减少对这些专用计算资源的中断。要做到这一点，你可以使用 `cset`、内核参数如 `isolcpus` 和 `nohz_full`，或 cgroup `cpuset` 隔离。通过隔离，OS 调度器会将那些 CPU 核心留给你随心所欲地使用。

> 在生产环境中强烈推荐使用 cgroup CPU 和内存亲和性。使用这些，每个 AI 工作负载都被隔离在自己的物理核心和内存区域上。这将防止跨工作负载竞争和 NUMA 惩罚。应使用 `cpuset` cgroups 或容器运行时 (`docker --cpuset-cpus`) 来强制执行此操作。

你可以将每个设备的硬件中断分配给同一 NUMA 节点上的核心。这将防止跨节点中断处理，否则会导致额外的延迟并驱逐远程节点上有用的缓存行。例如，如果你的 GPU 或 NIC 在 NUMA 节点 0 上引发中断，你会将其绑定到节点 0 上的核心，以便没有其他节点处理它。如果没有这种绑定，不同 NUMA 节点上的 CPU 可能会处理该中断。这将强制缓存一致性流量和跨节点通信。

在实践中，性能敏感型系统通常禁用默认的 `irqbalance` 守护进程，或以定制规则运行它。另一个选项是使用 `/proc/irq/*/smp_affinity` 手动设置每个中断的亲和性掩码。通过将每个 GPU 和 NIC 中断绑定到最近的核心，你可以保证这些设备中断总是在最优的 NUMA 节点上得到服务。

简而言之，专用核心、适当的调度优先级和 NUMA 感知的硬件中断绑定的组合，可以帮助最小化向 GPU 输送数据的加载线程的抖动。

### 虚拟内存和交换 (Virtual Memory and Swapping)

不用说，你应该始终尝试避免内存交换。如果进程的任何部分内存被交换到磁盘，你将看到灾难性的、多个数量级的减速。GPU 程序倾向于分配大量主机内存用于数据缓存。如果 OS 决定将一些数据从内存交换到磁盘，当 GPU 需要访问该数据时，将经历巨大的延迟。

我们建议设置 `vm.swappiness=0`，这告诉 Linux 除非在极度内存压力下，否则避免交换。它有效地通过 cgroup 限制隔离了训练作业的内存，以防止任何交换。

你也可以使用 `sudo swapoff -a` 临时禁用所有交换设备和文件，直到下次重启。只需确保你有足够的 RAM 供工作负载使用——或设置限制以防止过度提交 (Overcommit)。否则，**OOM 杀手 (OOM Killer)** 可能会收割该进程。使用 `vmstat` 或 `free -m` 监控交换使用情况，确保 swap 保持为零。

另一个相关设置是 `ulimit -l`，如前所述用于锁页内存。如果你想防止内存被交换，你应该将该限制设置得很高，或者你可能会经历过度的内存交换。同样，对于利用大量内存的大型 AI 工作负载，通常将其设置为 `unlimited`。

### 文件系统缓存和回写 (Filesystem Caching and Write-Back)

大型训练作业的一个最佳实践是将频繁的检查点 (Checkpoints) 写入磁盘，以防你需要从已知良好的检查点重启失败的作业。然而，在检查点期间，巨大的数据爆发可能会填满 OS 页缓存并导致停顿。

对于存储，你可以调整 `vm.dirty_ratio` 和 `vm.dirty_background_ratio` 来调优用于缓冲写入的页缓存大小。例如，对于多 GB 的检查点，使用较高的脏页比率允许 OS 在刷新到磁盘之前在 RAM 中批处理更多数据。这将平滑大型检查点写入并减少训练循环中的停顿。

另一个选项是在单独的线程中执行检查点操作。PyTorch 中一个较新的选项是从集群中的节点写入分布式检查点分区。在这种情况下，检查点分区将在作业失败重启后加载检查点时合并。

在延迟敏感的训练工作流中，最好完全绕过页缓存。例如，使用 `O_DIRECT` 打开检查点文件，或使用 Linux 的 `io_uring` 进行异步 I/O 以避免页缓存停顿。写入每个检查点后，调用 `posix_fadvise(fd, 0, 0, POSIX_FADV_DONTNEED)` 以立即从缓存中丢弃这些页面，防止后续迭代出现内存压力。

### CPU 频率和 C-states (CPU Frequency and C-states)

默认情况下，许多计算节点以省电模式运行 CPU，这要么降低 CPU 频率，要么在空闲时将其置于睡眠状态。这有助于节省能源、减少热量并降低成本。在模型训练期间，当 GPU 正在处理数据集的最后一批数据时，CPU 可能并不总是 100% 被利用。然而，这些电源管理功能可能会在系统再次唤醒 CPU 以处理新工作时导致额外的延迟。

为了获得最大且一致的性能，AI 系统通常将 CPU 频率调节器配置为“性能 (Performance)”模式，这使 CPU 始终保持在最大频率。这可以使用 `cpupower frequency-set -g performance` 或在基本输入/输出系统 (BIOS) 中完成。

同样，禁用深度 **C-states** 可以防止核心进入低功耗睡眠状态。CPU C-states 是系统 ACPI 规范定义的省电模式。当 CPU 核心空闲时，它可以进入 C-state 以节省能源。C-state 越深，节省的电力越多，但在工作到达时核心唤醒所需的时间越长。禁用较深的 C-states 可以消除过度的延迟峰值。C0 是活跃状态；C0 以上的任何状态都代表更深的睡眠状态。

> 在实践中，许多服务器 BIOS/UEFI 提供了高性能配置文件，自动将 CPU 调节器设置为“Performance”并禁用深度 C-states。

本质上，我们可以用一点额外的电力消耗来换取更灵敏的 CPU 行为。在 GPU 是主要耗电大户的训练场景中，稍微多一点 CPU 功耗通常是可以接受的，只要它能让 GPU 保持饱腹。例如，如果一个数据加载器线程在等待数据时睡眠，CPU 进入深度 C6 状态，CPU 的大部分被断电以最大化节能。

如果 CPU 进入更深的睡眠状态，它可能需要几微秒才能唤醒。虽然这不是很长时间，但许多微秒累加起来可能会导致 GPU **气泡 (Bubbles)**，如果管理不当的话。气泡是指 GPU 等待 CPU 恢复数据处理的时间段。通过保持 CPU 就绪，我们减少了这种打嗝。许多服务器 BIOS 都有禁用 C-states——或至少限制它们的设置。

你应当始终关闭系统中任何可能引入不可预测延迟的东西，例如过多的上下文切换、CPU 频率缩放和内存到磁盘的交换。结果应该是你的 CPU 能够以 GPU 消耗数据的速度一样快地交付数据，而没有 OS 将任务调度到错误的核心或在错误的时间夺走 CPU 周期。

### 调优主机 CPU 内存分配器 (Tune Host CPU Memory Allocator)

在一个调优良好的 GPU 服务器上，CPU 使用率可能不会很高，因为 GPU 处理了大部分计算。然而，CPU 使用率应保持稳定并与 GPU 活动同步。CPU 必须忙于准备每个传入的批次，而当前批次正在由 GPU 处理。

正确的 CPU 到 GPU 交接对于维持高 GPU 利用率至关重要。通过调优主机的内存分配器（**jemalloc** 或 **tcmalloc**），你可以消除数据准备中不可预测的暂停。这将使 GPU 保持在峰值运行——除了有意的同步点之外。

调优后，你应该看到每个 GPU 的利用率徘徊在 100% 附近，并且仅在所需的同步屏障处下降。GPU 永远不应因 CPU 端延迟而停顿等待数据。使用 `jemalloc`，你可以将分配分片到每 CPU 竞技场 (narenas)，启用 `background_thread` 进行路径外清除，并延长 `dirty_decay_ms`/`muzzy_decay_ms`，以便释放的页面不会立即返回给 OS。这将最小化锁竞争和碎片化。

你可以使用 `MALLOC_CONF` 环境变量调优 `jemalloc`，如下所示：

```bash
export MALLOC_CONF="narenas:8,dirty_decay_ms:10000,muzzy_decay_ms:10000,background_thread:true"
```

同样，`tcmalloc` 受益于调优 `TCMALLOC_MAX_TOTAL_THREAD_CACHE_BYTES` 和 `TCMALLOC_RELEASE_RATE` 环境变量。这将提供更大的每线程缓存，以便小分配避免全局锁和系统调用——保持 CPU 线程以低且可预测的延迟准备好向 GPU 供料。你可以这样做：

```bash
export TCMALLOC_MAX_TOTAL_THREAD_CACHE_BYTES=$((512*1024*1024))
export TCMALLOC_RELEASE_RATE=16
```

简而言之，优化分配器可以减少分配器开销和碎片化。这将保持 CPU 线程持续快速，并避免向 GPU 供料时出现意外停顿。试验这些环境变量，并针对你的特定工作负载和环境进行调优。

---

## 针对性能的 GPU 驱动和运行时设置 (GPU Driver and Runtime Settings for Performance)

我们已经优化了 CPU 端，但 GPU 驱动和运行时也有一些重要的设置会影响性能——特别是在多 GPU 和多用户场景中。NVIDIA GPU 有几个旋钮，如果调优得当，可以减少开销并改善多个工作负载共享 GPU 的方式。

接下来，我们将介绍 GPU 持久化模式、MPS、MIG 分区，以及其他一些考量，如时钟设置、ECC 内存和内存溢出行为。

### GPU 持久化模式 (GPU Persistence Mode)

默认情况下，如果没有应用程序使用 GPU，驱动程序可能会将 GPU 置于低功耗状态并卸载部分驱动程序上下文。下一次应用程序想要使用 GPU 时，初始化它需要成本。这可能需要一两秒的时间让驱动程序启动一切。

GPU 初始化开销会对周期性释放和重新获取 GPU 的工作负载产生负面影响。例如，考虑一个训练集群，作业频繁启动和停止。或者一个低流量推理集群，每次有新的推理请求到达时都必须唤醒 GPU。在这两种情况下，开销都会降低整体工作负载性能。

**持久化模式**通过运行 `nvidia-persistenced` 守护进程启用。这保持 GPU 驱动程序加载且硬件处于就绪状态，即使没有应用程序处于活动状态。这请求系统在空闲时不要完全关闭 GPU 电源，从而防止**电源门控 (Power Gating)**。持久化使 GPU 保持唤醒状态，以便下一个作业具有零启动延迟。通常推荐用于长时间运行和延迟敏感的工作负载。你可以使用以下命令在启动时启用持久化守护进程：

```bash
systemctl enable nvidia-persistenced
```

> 在 Kubernetes 环境中，NVIDIA GPU Operator 可以配置为自动在所有 GPU 上启用持久化模式。

在 AI 集群上，通常在服务器启动时在所有 GPU 上启用持久化模式。这样，当作业开始时，GPU 已经初始化并可以立即开始处理。它不会让你的实际计算变得更快，因为它不加速数学运算，但它消除了作业启动延迟并防止冷启动延迟。

GPU 持久化模式还有助于交互式使用，因为如果没有持久化，你在一段空闲时间后进行的第一次 CUDA 调用可能会停顿，直到驱动程序重新初始化 GPU。开启持久化后，该调用会迅速返回。

持久化的唯一缺点是空闲时功耗略高，因为 GPU 保持在较高的就绪状态。但是，对于大多数数据中心 GPU 来说，这是为了更好的性能一致性而做出的可接受的权衡。一旦管理员使用 sudo 权限设置了 GPU 持久化模式，你就可以享受其好处并继续处理其他优化。

### MPS (多进程服务)

通常，当多个进程共享单个 GPU 时，GPU 的调度器在它们之间进行**时间分片 (Time-slices)**。例如，如果两个 Python 进程各自有一些内核要在同一个 GPU 上运行，GPU 可能会执行一个进程的内核，然后是另一个进程的内核，依此类推。如果这些内核很短并且它们之间存在空闲间隙，GPU 可能会利用率不足，因为它在做“乒乓”上下文切换而不是重叠工作。

NVIDIA 的 **MPS** 是一个特性，它创建了一种“保护伞”，在此之下多个进程可以在 GPU 上并发运行且没有严格的时间分片。有了 MPS，只要 GPU 资源（流式多处理器 [SMs], Tensor Cores 等）可用，GPU 就可以同时执行来自不同进程的内核。MPS 本质上将进程的上下文合并为一个调度器上下文。这样，你就不用支付在独立进程之间切换和空闲的全部成本。

MPS 何时有用？对于模型训练，如果你通常每 GPU 运行一个进程，你可能不需要 MPS。但如果你有像在一个大 GPU 上运行许多推理作业这样的场景，MPS 是一个游戏规则改变者。想象你有一个强大的 GPU 或 GPU 集群，但你的推理作业——或一组多个推理作业——并没有充分利用它。例如，考虑在一个 40 GB GPU 上运行四个独立的推理作业，每个使用 5–10 GB 和仅 30% 的 GPU 计算。默认情况下，每个推理作业获得一个时间片，因此在任何时刻，实际上只有一个作业的工作在 GPU 上运行。这导致 GPU 平均 70% 空闲。

如果你为这些推理作业启用 MPS，GPU 可以交错它们的工作，以便当一个作业等待内存时，另一个作业的内核可能会填充 GPU，等等。结果是更高的整体 GPU 利用率。在实践中，如果两个进程各使用 40% 的 GPU，有了 MPS，你可能会看到 GPU 以 80%–90% 的利用率同时服务两者。

例如，两个训练进程如果各自单独运行需要一小时——在同一个 GPU 上按顺序运行——使用 MPS 一起运行可以在总共一小时多一点的时间内并行完成，而不是按顺序的两小时。MPS 的加速在并发客户端的内核和内存带宽互补时可以接近翻倍。为了可视化，想象进程 A 和进程 B 各自周期性地启动内核而没有 MPS。GPU 调度可能看起来像 A-B-A-B，中间有间隙，因为每一方都在等待，如图 3-6 所示。

*(图 3-6: GPU 在运行进程 A 的内核和进程 B 的内核之间交替，产生了空闲间隙，其中一个进程在等待而另一个处于活动状态)*

有了 MPS，调度重叠了 A 和 B，以便每当 A 没有使用 GPU 的某些部分时，B 的工作可以同时使用它们，反之亦然。这种重叠消除了空闲间隙，如图 3-7 所示。

*(图 3-7: 使用 MPS 减少进程 A 和 B 的空闲间隙)*

设置 MPS 涉及运行一个 MPS 控制守护进程 (`nvidia-cuda-mps-control`)，它随后启动一个 MPS 服务器进程来代理 GPU 访问。在现代 GPU 上，MPS 更加精简，因为客户端（进程）可以直接与硬件通信，来自计算节点本身的干扰最小。

通常，你在节点上启动 MPS 服务器——通常每 GPU 一个或每用户一个——然后使用将它们连接到 MPS 的环境变量运行你的 GPU 作业。该服务器下的所有作业将并发共享 GPU。

MPS 的另一个特性是能够设置每客户端的**活跃线程百分比**。这限制了客户端可以使用多少 SM（本质上是 GPU 核心）。如果你想保证服务质量 (QoS)，例如两个作业各获得最多 50% 的 GPU 执行资源，这会很有用。在这种情况下，你可以设置 `CUDA_MPS_ACTIVE_THREAD_PERCENTAGE=50` 将客户端限制在约 50% 的 SM 执行能力。如果没有显式设置，作业将只是竞争并使用它们能用的任何 GPU 资源。

请注意，MPS **不**分区 GPU 内存，因此所有进程将共享完整的 GPU 内存空间。MPS 主要是关于计算共享和调度。问题在于，一个进程可能会请求大量的 GPU RAM，导致 GPU 发生 OOM 错误，并导致终止运行在 GPU 上的所有其他进程。这是非常具有破坏性的。此外，如果一个程序自己就使 GPU 100% 饱和，MPS 不会神奇地让它变快，因为你不能超过 100% 利用率。只有当个别作业留下一些他人可以填充的余量时，它才是有益的。

MPS 的另一个限制是，默认情况下，所有 MPS 客户端必须作为同一个 Unix 用户运行，因为它们共享上下文。在多用户集群中，这意味着 MPS 通常在调度器级别设置，以便一次只有一个用户的作业共享 GPU。否则，你可以配置一个所有用户共享的系统级 MPS，但要明白从安全角度来看，作业并不是隔离的。

现代 NVIDIA 驱动程序支持**多用户 MPS**，以便来自不同 Unix 用户的进程可以共享单个 MPS 服务器。这提高了可用性，但不提供内存隔离。当需要强隔离时，首选 **MIG**。MPS 的一个特定替代方案是 Kubernetes 中的 GPU 时间分片功能。Kubernetes 上的时间分片允许设备插件按时间在同一 GPU 上调度不同的 Pod。例如，如果你配置单个 GPU 的时间分片复制因子为 4，该 GPU 上的四个 Pod 可以各获得一个时间份额。

Kubernetes 时间分片是一种不需要 MPS 的自动分时算法。然而，这**并不**重叠执行。相反，它只是比默认驱动程序切换得更快。时间分片对于你宁愿牺牲一些空闲时间也要隔离的交互式工作负载可能很有用。对于高吞吐量作业，用 MPS 重叠或用 MIG 分割 GPU 通常比细粒度的时间分片更好，接下来我们将讨论这一点。

### MIG (多实例 GPU)

现代 GPU 可以在硬件层面被分区为多个实例，使用 **MIG**。MIG 是一种虚拟化形式，但在硬件中完成。这样，由于损失了一些灵活性，开销非常低——可能只有百分之几。

如果一个实例空闲，它不能将其资源借给另一个实例，因为它们是**硬分区 (Hard Partitioned)** 的。MIG 允许将一个 GPU 切割成多达七个更小的**逻辑 GPU**——每个都有自己专用的内存部分和计算单元或 SM，如图 3-8 所示。

*(图 3-8: 现代 GPU 上的七个 MIG 切片)*

按照惯例，NVIDIA 的 MIG 配置文件命名使用前缀 `<X>g` 来表示现代 GPU 上计算切片的数量，介于 1 (最小) 和 7 (最大) 之间。每个切片数字代表分配给该分区的 SM 组的数量。每个 SM 组大致是 SM 总数的 1/7 切片。

如果一个 GPU 有 132 个 SM，每个 1/7 切片代表 132 SMs × 1/7 = ~19 SMs 在一组中。因此，1g 代表 ~19 SMs，2g 代表 ~38 SMs，一直到 7g，代表全部 ~132 SMs。

相反，稍微令人困惑的是，后缀 `<Y>gb` 指定了为该配置文件保留的 HBM GPU RAM 的确切 GB 数。MIG 配置文件值对于每一代和类型的 GPU 都是固定的，并列在 NVIDIA 文档中。对于 Blackwell B200，表 3-1 显示了一些 MIG 配置文件值。

*(表 3-1: Blackwell B200 的 MIG 配置文件)*

该表还显示了每个配置文件的硬件单元、复制引擎和 L2 缓存比例。这些固定配置文件与 GPU 的硬件内存控制器对齐，使得每个内存切片映射到连续的 HBM 通道。

这种两部分方案将计算能力（SM 组数量）与内存容量（总 GB）分开。管理员可以选择 MIG 配置文件的组合。分配的 SM 和 HBM 的总和不需要完全匹配完整的 GPU 容量。然而，某些组合受限于硬件分区。例如，它们不能发明新的切片大小。

管理员只能使用 `nvidia-smi -mig` 或使用 **NVIDIA Kubernetes GPU Operator** 的 `nvidia.com/mig.config` config map 在每个 GPU 上启用或禁用受支持的 MIG 配置文件（例如，1g.23gb, 2g.45gb, 4g.90gb 等）。重新配置 MIG 需要排空工作负载并调用 MIG 的动态重新配置能力来应用更改。

一旦 GPU 处于 MIG 模式，现代 GPU 可以动态创建和销毁 MIG 分区，而无需重启整个系统。你可以在排空现有工作负载后即时调整 MIG 实例，但要在 GPU 上启用或禁用 MIG 模式本身，需要重置 GPU。

从软件的角度来看，每个 MIG 实例就像一个**独立的 GPU**，因为它有自己的内存、自己的 SM，甚至单独的引擎上下文。MIG 的好处是为每个作业提供强隔离和资源保证。如果你有多个用户或多个服务，每个只需要（比如）10 GB 的逻辑 GPU 内存，你可以将它们打包到一个物理 GPU 上，而不会干扰彼此的内存或计算。

作业可以专门请求 MIG 设备，但你必须小心调度，以使用所有切片。例如，如果你有一个 7 切片的设置，而一个作业只占用 1 个切片，其他 6 个应该被其他作业填充，否则你会留下大量闲置资源。可以配置集群中的某些节点使用 MIG 处理小型推理作业，配置其他节点用于大型训练作业的非 MIG 工作负载。

一个重要的操作注意事项是，为了使用 MIG，你通常在系统级别——或至少在节点级别——配置它。GPU 必须被置于 MIG 模式，切片被创建，GPU 被重置。一旦这些发生，切片作为单独的设备出现在系统中——每个都有自己唯一的设备 ID。

如果你因为缺乏前期规划而没有使用所有可用的 MIG 切片，你最终会因为让它们碎片化和闲置而浪费资源。提前规划分区大小以匹配你的工作负载并在工作负载变化时调整分区大小是很重要的。你需要重置 GPU 以获取更改。

对于跨越许多 GPU 的大规模模型训练作业和推理服务器，MIG 通常没用，因为我们需要访问全套 GPU。另一方面，对于多租户、小模型推理服务器，如果可以运行较小的 GPU 分区，MIG 及其隔离特性可能会很有用。

> 截至撰写本文时，当 GPU 处于 MIG 模式时，GPU 到 GPU 的点对点通信（包括 NVLink）是**禁用**的。这适用于跨 GPU 的 NVLink 和 PCIe P2P。MIG 实例不能与其他 GPU 进行 P2P。跨 MIG 实例的 CUDA IPC 也受到限制。这会降低分布式训练吞吐量。在启用 MIG 之前，请确认你的训练或推理拓扑不依赖于 GPU 对等路径。大规模训练作业（以及稀疏 MoE 专家推理系统）需要大量的 GPU 到 GPU 通信，通常不是 MIG 的好候选者。GPU MIG 实例之间的通信必须通过主机或网络结构传输。

简而言之，只有当你需要在同一 GPU 上以强隔离运行多个独立作业时，才启用 MIG。**不要**将 MIG 用于跨 GPU 的大规模分布式训练或推理，因为你希望访问 GPU 的全部能力及其快速互连。

在我们需要大型基于 Transformer 的模型训练和推理的背景下，我们将关闭 MIG。但知道这个特性的存在是很好的。也许一个集群可能会动态切换模式，在白天运行 MIG 进行大量小型训练或推理实验，然后在晚上关闭 MIG 运行使用整个 GPU 的大型训练作业。

> Kubernetes 设备插件会将 MIG 设备列为资源，例如在 1/7 GPU 切片且总共 23 GB 的情况下为 `nvidia.com/mig-1g.23gb`。

### GPU 时钟速度和 ECC (GPU Clock Speeds and ECC)

NVIDIA GPU 有一种叫做 **GPU Boost** 的东西，它会在功率和热限制内自动调整核心时钟。大多数时候，你应该让 GPU 自己处理。但有些用户喜欢锁定（Lock）时钟以获得一致性，以便 GPU 始终以固定的最大频率运行。这样，每次运行的性能是稳定的，不受功率或温度变化的影响。

在执行基准测试时，固定时钟极其重要，因为后续的运行可能会因过热而降频。如果你不考虑到这一点，你可能会错误地解释后续运行的糟糕结果，实际上是因为前一次运行导致的热量过高使这些后续运行的 GPU 被降频了。

具体来说，NVIDIA 的 GPU Boost 会上下调节核心时钟以保持在功率/热限制内。使用 `nvidia-smi -lgc` 锁定核心时钟和 `-ac` 锁定内存时钟在最大稳定频率。这将确保 GPU 以恒定频率运行——并防止 GPU Boost 的默认功能在后续运行中降频。

> 这主要是在**基准测试**期间为了获得确定性和可复现的结果。对于日常训练和推理，建议保留自动加速，除非你注意到显著的性能差异和 GPU 降频。

如果你追求最后一点确定性和一致性，锁定特定时钟是值得注意的。通常，让 GPU 处于默认的自动加速模式是没问题的。

一些团队故意**降频 (Underclock)** GPU 以减少热量——特别是如果他们运行非常长的作业并且不想随着时间的推移遭受最终的热减速。数据中心 GPU 通常有足够的温度余量——以及适当的风冷和液冷——所以你不需要这样做，但知道这是一个选项是很好的。

另一种方法是使用 `nvidia-smi -pl` 将功率限制设置为略低于 GPU 的最大热设计功耗 (TDP)。TDP 是 GPU 在持续负载下可能产生的最大热量（以瓦特为单位）。这规定了必须耗散以防止过热的热量。

如果你将功率限制设置在 TDP 以下，GPU Boost 将自动调整时钟低于热节流点。这可以减少峰值热量产生，防止节流，并产生最小的性能影响。

GPU 上的 **ECC (纠错码)** 内存是另一个考量。ECC 确保如果出现由宇宙射线等引起的单比特内存错误，内存可以被即时纠正。如果是双比特错误，错误会被检测到并向调用代码抛出错误。ECC 通常在 NVIDIA 数据中心 GPU 上默认启用。

禁用 ECC 可以释放少量内存，因为 ECC 需要额外的比特进行错误检查。这可能会通过减少与即时错误检查相关的开销产生边际性能增益，但通常只有几个百分点。然而，关闭 ECC 也会移除关键的内存错误保护，这可能导致系统不稳定或未检测到的数据损坏。

对于 NVIDIA 的数据中心 GPU，包括 Hopper 和 Blackwell，ECC 默认启用，并旨在保持启用状态以确保可靠、经纠错的计算和数据完整性。对于大型模型上的长训练或推理作业，单个内存错误可能会使作业完全崩溃，甚至更糟，在没有警告的情况下悄悄破坏你的模型。

**建议对任何严肃的 AI 工作负载始终保持 ECC 开启。** 你唯一可能考虑关闭它的时候是在研究设置中，你可以接受风险，因为你需要那额外的一丝内存来将模型塞进你受限的 GPU 集群中。

切换 ECC 模式需要重置 GPU，并且可能会重启当前在该 GPU 上运行的作业。所以这不是一个你想频繁切换的开关。为了稳定性和可靠性保持 ECC 开启。内心的安宁胜过关闭 ECC 带来的微不足道的加速。

### GPU 内存过载、碎片化和内存溢出处理 (GPU Memory Oversubscription, Fragmentation, and Out-of-Memory Handling)

与 CPU RAM 不同，默认情况下没有 GPU“交换”内存这回事。如果你试图分配比可用量更多的 GPU 内存，你会得到一个不友好的 **OOM (内存溢出)** 错误，以及一个更不友好的进程崩溃。有几种机制可以缓解这个问题：允许内存动态增长，拥抱跨 CPU 和 GPU 的统一内存，以及利用内存池和缓存分配器。

默认情况下，一些框架（例如 TensorFlow）在启动时抓取所有可用的 GPU 内存以避免碎片化并提高性能。如果你不知道这一点，在共享 GPU 的场景中这可能会非常糟糕。PyTorch 默认情况下只按需分配 GPU 内存。

TensorFlow 有一个选项 (`TF_FORCE_GPU_ALLOW_GROWTH=true`) 使其从小开始并根据需要动态增长 GPU 内存使用——类似于 PyTorch。然而，无论是 PyTorch 还是 TensorFlow 都不允许你分配比 GPU 可用量更多的内存。但这种惰性分配在多租户场景中表现更好，因为两个进程不会都试图从一开始就同时分配最大可用 GPU 内存。

CUDA 的**统一内存 (Unified Memory)** 系统允许你分配内存而无需预定义它驻留在 CPU 还是 GPU 上。CUDA 运行时处理按需移动页面。像 Hopper 和 Blackwell 这样的现代 NVIDIA GPU 包括对使用 **页面迁移引擎 (Page Migration Engine, PME)** 进行按需分页的硬件支持。

当 GPU 上的可用内存不足时，PME 自动在 GPU 内存和主机 CPU RAM 之间迁移内存页。然而，虽然 PME 提供了灵活性，但与其工作负载拥有足够的 GPU 内存相比，依赖它可能会引入性能惩罚。

这种 GPU 到 CPU 的内存卸载可能很慢，因为 CPU 内存 I/O 比 GPU 高带宽内存 (HBM) I/O 慢，正如我们在第二章中学到的。这种机制主要是为试图运行无法放入 GPU RAM 的模型的从业者提供的一种便利。

对于性能关键型工作负载，你通常希望尽可能避免依赖统一内存过载。它是作为安全网存在的，而不是让你的脚本直接崩溃，但当 GPU 内存过载时，你的作业运行速度会变慢。

像 PyTorch 这样的库使用**缓存分配器 (Caching Allocator)**，这样当你释放 GPU 内存时，它不会立即将内存返回给 OS。相反，它保留它以供将来的分配重用。这避免了内存碎片化以及重复请求 OS 分配同一块内存的开销。

你可以使用环境变量（如 `PYTORCH_ALLOC_CONF`，原 `PYTORCH_CUDA_ALLOC_CONF`）配置 PyTorch 的分配器以设置最大池大小。我们将在后面的章节中介绍对 PyTorch 内存分配机制的优化。

如果你遇到 GPU OOM 错误，你肯定会在某个时候遇到，这很可能是由内存碎片化或过多的内存缓存引起的。你可以尝试使用 PyTorch 的 `torch.cuda.empty_cache()` 清除缓存，但这几乎总是意味着你的工作负载确实需要那么多内存。

PyTorch 还提供了工具如 `torch.cuda.memory_stats()` 和 `torch.cuda.memory_summary()` 来帮助诊断碎片化，显示已分配内存与保留内存的对比。NVIDIA 的 **Nsight Systems** 也显示 GPU 内存使用模式，以帮助识别内存泄漏、与泄漏相关的长寿命分配、CPU-GPU 互连活动和 GPUDirect Storage 时间线跟踪。此外，**Nsight Compute** 分析器提供了低级内核分析，包括占用率、吞吐量和 NVLink 使用情况。我们将在接下来的章节中涵盖所有这些内容。

Docker 提供了 `--gpus` 标志来选择并将 GPU 暴露给容器，但它不支持设置 GPU 内存限制。如果你需要 GPU 内存或计算的硬隔离，请使用 MIG 对设备进行分区，或使用具有活跃线程百分比的 Multi-Process Service (MPS) 进行公平共享。当你需要严格分区时，在 Kubernetes 中使用 MIG 资源（如 `nvidia.com/mig-2g.45gb`）配置限制。

在多租户节点中，这对于隔离作业很有用。在单作业单 GPU 的情况下，通常不设置内存限制，因为你想让作业尽可能多地使用 GPU 内存。

一般来说，耗尽 GPU 内存是你可以在应用程序级别管理的事情。例如，你可以减少数据批次大小、模型权重精度，甚至模型参数数量（如果这是一个选项）。

一个最佳实践是在模型训练和推理期间使用 `nvidia-smi` 或 NVML API 监控 GPU 内存使用情况。如果你接近内存限制，考虑变通方法，如减少批次大小，训练时使用**激活检查点 (Activation Checkpointing)**，或其他降低内存使用的技术。

此外，你应该确保你的 CPU 内存没有被交换，因为这将间接损害你的 GPU 利用率和有效吞吐量，因为每次你的 GPU 试图从 CPU 主机获取东西，但主机内存页已被交换到磁盘时，你的性能将受到慢得多的磁盘 I/O 的瓶颈限制。因此，将这些内存减少最佳实践与之前关于绑定内存、增加 `ulimit` 和禁用 `swappiness` 等建议结合起来非常重要。

简而言之，建议始终保持 GPU 驱动程序加载，而不是在作业之间卸载 GPU 驱动程序。这类似于 GPU 持久化模式，但在更深层次上。有些集群配置为在没有作业运行时卸载驱动程序以释放 OS 内核内存和出于安全考虑。然而，如果你这样做，下一个作业必须支付重新加载 GPU 驱动程序的成本，如果使用了 MIG，还要重新配置 MIG 切片。

> 建议保持驱动程序和任何 MIG 配置在作业之间持久化。你唯一想卸载 GPU 驱动程序的时候是为了故障排除或升级驱动程序。因此，集群管理员通常设置系统，以便一旦机器启动，NVIDIA 驱动程序模块始终存在。

---

## 容器运行时优化 (Container Runtime Optimizations for GPUs)

许多 AI 系统使用编排工具和容器运行时来管理软件环境。Kubernetes 和 Docker 在 AI 基础设施中很受欢迎。使用容器确保所有依赖项（包括 CUDA 和库版本）是一致的。这避免了“但在我的机器上能跑”的问题。容器引入了一点复杂性和微量的开销，但在正确的配置下，你可以使用容器获得接近裸机的 GPU 工作负载性能。

在节点上运行的容器不是传统的虚拟机 (VM)。与 VM 相比，容器共享主机 OS 内核，以便 CPU 和内存操作以接近原生的速度执行。并且有了 NVIDIA Container Toolkit，从 Docker 容器内部访问 GPU 是直接的，不会产生开销。

> 对于运行最新 NVIDIA Container Toolkit 的现代 GPU，在配置得当的环境中，GPU 性能与直接在容器外的裸机主机上运行代码几乎相同（差异 < 2%）。实际上，MLPerf Inference v5.0 结果中使用了 Red Hat OpenShift 和 Kubernetes，这证明了现代容器和编排配置不会损害效率或延迟。

### NVIDIA Container Toolkit 和 CUDA 兼容性

在容器中使用 GPU 的一个挑战是确保容器内的 CUDA 库与主机上的驱动程序匹配。NVIDIA 通过他们的 Container Toolkit 和基础 Docker 镜像解决了这个问题。主机提供 NVIDIA 驱动程序，记住，它与内核和硬件紧密集成。在容器内部，你通常会找到特定版本的 CUDA 运行时库。

一般规则是，主机的 NVIDIA 驱动程序版本必须至少与容器内 CUDA 版本所需的最低驱动程序版本一样新。对于 CUDA 13.x，所需的最低 Linux 主机驱动程序分支是 R580 或更新。对于 CUDA 12.x，所需的最低 Linux 主机驱动程序分支是 R525 或更新。使用较新的 CUDA 运行时搭配较旧的驱动程序将导致 CUDA 初始化失败。

> 每个新 CUDA 版本都需要最低 NVIDIA 驱动程序版本。始终查阅 NVIDIA 的官方兼容性矩阵，并在更新 CUDA 工具包时升级主机驱动程序。

对于 Docker 和 Kubernetes 环境，最简单的方法是使用 NVIDIA GPU Cloud (NGC) 或 DockerHub 镜像存储库中的 NVIDIA 官方基础 Docker 镜像。这些镜像（例如 `nvcr.io/nvidia/pytorch` 或类似）捆绑了正确版本的 CUDA 运行时、cuDNN、NCCL 等。此外，这些 Docker 镜像列出了所需的最低 CUDA 驱动程序，具体取决于 CUDA 版本。这样，你就获得了对最新硬件的支持，而没有依赖项的头痛。

### NVIDIA 容器运行时 (NVIDIA Container Runtime)

或者，NVIDIA 的容器运行时实际上可以在运行时将主机驱动程序库注入容器，所以你甚至不需要在镜像中发布 NVIDIA 驱动程序。相反，你只依赖主机的驱动程序。同样，这之所以有效，是因为容器不像传统 VM 那样完全隔离。Docker 容器被允许使用主机设备、卷和库。

在容器内部，你的应用程序使用 CUDA 运行时库，如容器镜像中的 `libcudart.so`，而 NVIDIA Container Toolkit 在容器启动时注入主机的驱动程序库，如 `libcuda.so` 和 `libnvidia-ml.so`。主机驱动程序库直接在主机上调用，所以一切正常工作。

CUDA 运行时库（容器）和 NVIDIA Container Toolkit（主机）之间的分离是受支持的，只要主机驱动程序满足镜像中 CUDA Toolkit 所需的最低版本。如果你不匹配并试图在容器中使用较新的 CUDA 版本而在主机上使用旧驱动程序，你很可能会得到错误。匹配 CUDA 和驱动程序版本很重要。

关键要点是，当使用容器运行 GPU 时，不涉及管理程序 (Hypervisor) 或虚拟化层。容器直接共享主机内核和驱动程序，因此当内核在 GPU 上启动时，就像它从主机启动一样。

换句话说，你不会因为基于 Docker 的虚拟化而损失性能——除非你使用的是像 VMware 或单根输入/输出虚拟化 (SR-IOV) 虚拟 GPU 这样的东西，这是一种需要一些调优的特殊场景。有了 Docker 加 NVIDIA，它基本上相当于裸机性能。

> NVIDIA Container Toolkit 也适用于 **containerd** 和 **Podman**，不仅仅是 Docker。这对于使用 containerd 作为默认容器运行时的现代 Kubernetes 环境非常相关。

### 避免容器叠加文件系统开销 (Avoiding Container Overlay Filesystem Overhead)

在 Docker 容器中运行与直接在主机上运行的主要区别可能在于 I/O。容器通常使用**联合文件系统 (Union Filesystem)**，它透明地将多个底层文件系统（如主机文件系统和容器文件系统）叠加成一个单一、统一的视图。

在像 OverlayFS 这样的联合文件系统中，来自多个源的文件和目录将显示为它们属于同一个文件系统。这种机制对容器特别有用，其中来自基础镜像层的只读文件系统与可写容器层相结合。

然而，使用叠加文件系统时会有一些开销。产生这种额外延迟是因为文件系统必须检查多个底层层——既有只读的也有可写的——以确定应返回哪个版本的文件。与从单个、简单的文件系统读取相比，额外的元数据查找和合并这些层的逻辑可能会增加少量开销。

此外，当写入叠加层使用的**写时复制 (CoW)** 机制时也有开销。CoW 意味着当你修改只读层（例如基础镜像）中的文件时，该文件必须首先被复制到可写层。然后写入发生在复制的可写文件上——而不是原始的只读文件。如前所述，读取修改后的文件需要查看只读和可写层以确定哪个是正确的版本。

模型训练通常涉及繁重的 I/O 操作，如读取数据集、加载模型和写入模型检查点。为了解决这个问题，你可以使用**绑定挂载 (Bind Mounts)** 将主机目录——或网络文件系统——挂载到容器中。

绑定挂载绕过了叠加层，因此性能类似于直接在主机上进行磁盘 I/O。如果主机文件系统是 NVMe SSD 或 NFS 挂载，你将获得该底层存储设备的全部性能。我们特意不将多 TB 的数据集打包在镜像内。相反，我们通过挂载引入数据。

例如，如果你的训练数据在主机的 `/data/dataset` 上，你会运行容器并带上 `-v /data/dataset:/mnt/dataset:ro`，其中 `ro` 意味着只读挂载。然后你的训练脚本从 `/mnt/dataset` 读取。这样，你就直接从主机文件系统读取了。

事实上，避免对容器可写层进行大量数据读写是一个最佳实践。相反，将你的数据目录和输出目录从主机挂载到容器中。你要确保 I/O 不会被容器 CoW 机制的开销所瓶颈。

### 减小镜像大小以加快容器启动

如果镜像巨大且需要通过网络拉取，容器启动时间可能会相当慢。但在典型的长时间运行的训练循环中，几分钟的启动时间与数小时、数天或数月的训练时间相比微不足道。通过不包含不必要的构建工具或临时构建文件来保持镜像合理精简仍然是值得的。这节省了磁盘空间并改善了容器启动时间。

一些 HPC 中心更喜欢 **Singularity (Apptainer)** 而不是 Docker，因为它可以在用户空间运行镜像而无需 root 守护进程。它还直接使用主机文件系统，并且除了 OS 已有的开销外，几乎没有任何开销。

无论哪种情况，Docker 或 Apptainer（以前的 Singularity），研究和基准测试表明，一旦配置得当，这些容器解决方案在运行容器与直接在主机上运行之间的差异仅为百分之几。本质上，如果有人给你一份 GPU 利用率和吞吐量的日志，仅凭日志很难判断作业是否在容器中运行。

---

## 用于拓扑感知容器编排和网络的 Kubernetes (Kubernetes for Topology-Aware Container Orchestration and Networking)

Kubernetes (也称为 K8s) 是用于 AI 训练和推理的流行容器编排器。**NVIDIA Kubernetes 设备插件 (Device Plugin)** 是一个轻量级组件，它向调度器通告 GPU 硬件 (`/dev/nvidia0`, `/dev/nvidiactl` 等)。当你请求 `nvidia.com/gpu` 资源限制 (`resources.limits`) 且可选请求资源请求 (`resources.requests`) 时，它会将这些设备节点挂载到你的 Pod 中。这样，当你在 Kubernetes 上使用此设备插件部署容器时，Kubernetes 会负责让 GPU 对容器可用。设备插件也是**拓扑感知**的。这意味着它可以倾向于为给定的 Pod 分配来自同一 NVLink Switch 或同一 NUMA 节点的多个 GPU。

**NVIDIA Kubernetes GPU Operator** 自动化了所有 NVIDIA 软件的安装和生命周期，包括驱动程序库、前面提到的 NVIDIA Kubernetes 设备插件和 NVIDIA Container Toolkit。它还负责使用 **NVIDIA GPU Feature Discovery** 进行节点标记，用 GPU 的 NUMA 节点和 NVLink/NVSwitch ID 标记每个 GPU。调度器随后可以使用这些标签智能地将 GPU 分配给作业。GPU Operator 还实现了使用 DCGM 的 GPU 监控。

当使用 Kubernetes 编排基于 GPU 的容器时，你希望它以感知硬件拓扑的方式将资源分配给容器，包括 NUMA 节点和网络带宽配置。然而，默认情况下，Kubernetes **不是**拓扑感知的。它将每个 GPU 视为一种资源，但不知道 GPU 0 和 GPU 1 是否在同一个 NUMA 节点上，或者它们是否使用相同的 NVLink 互连。这可能会造成巨大的差异。

考虑一个拥有两组 4 GPU 的 8 GPU 服务器——每组通过 NVLink 连接。如果你向 Kubernetes 请求 4 个 GPU 用于作业，理想情况下 K8s 会给你 4 个全部通过 NVLink 互连的 GPU，因为它们可以更快地共享数据。然而，如果 Kubernetes 在系统中随机挑选 4 个 GPU，你的作业可能会被分配到来自一个 NVLink 域的两个 GPU 和来自另一个 NVLink 域的两个 GPU。

在没有拓扑感知的情况下分配 GPU 会在 GPU 到 GPU 的路径中引入慢速互连（例如 InfiniBand 或以太网）。这可能会将你的 GPU 间带宽削减一半。在这种情况下，Kubernetes 理想情况下应该分配四个全部通过 NVLink 互连且使用相同 NUMA 节点的 GPU，而不是跨越不同机架和 NUMA 域的四个 GPU。

例如，考虑基于 NVLink 5 互连构建的 NVIDIA NVL72 机架，它将 72 个 GPU 连接到一个单一的高带宽域中，机架内拥有合计 **~130 TB/s** 的吞吐量（72 GPUs * 1.8 TB/s 每 GPU）。在此配置中，如果 Kubernetes 调度器不是拓扑感知的，它可能会将多 GPU 作业放置在不同的 NVLink 域上——甚至是在 NVL72 组之外。在不尊重系统拓扑的情况下为作业分配 GPU 将抵消 NVL72 巨大的机架内带宽优势。

为了避免资源竞争，你应该尝试预留你需要的资源或为你的作业请求整个节点。对于容器/Pod 放置，你应该使用 **Kubernetes Topology Manager** 组件将 Pod 与 CPU 亲和性和 NUMA 节点对齐，以将容器的 CPU 绑定到与容器分配到的 GPU 相同的 NUMA 节点。我们接下来讨论这个。

### 使用 Kubernetes Topology Manager 编排容器

Kubernetes Topology Manager 可以提供详细的拓扑信息。例如，它可以检测到 GPU 0 连接到 NUMA 节点 0、NVLink 域 A 和 PCIe 总线 Z。Kubernetes 调度器随后可以使用这些信息以最佳方式将容器分配给 GPU，以实现高效的处理和通信。

拓扑感知的 GPU 调度仍在成熟中。在许多集群中，管理员使用 Kubernetes 标签显式标记节点以捕获 GPU 和系统拓扑。这些标签确保多 GPU Pod 落在 GPU 共享同一 NVLink 互连或驻留在同一 NUMA 域的服务器上。

为了我们的目的，如果你在 Kubernetes 中运行多 GPU 作业，请确保启用拓扑感知调度。这通常涉及将 `--topology-manager-policy` 配置为 `best-effort`、`restricted`，或在某些情况下为 `single-numa-node`。此策略配置通过避免远程内存访问，帮助多 GPU 和 CPU + GPU 工作负载实现更低的延迟。这补充了 OS 级的 NUMA 调优。

此外，确保存用最新的 NVIDIA GPU 设备插件和 NVIDIA Kubernetes GPU Operator（前一节提到过），因为这些是拓扑感知的，并支持将多 GPU Pod 打包到连接到同一 NUMA 节点的 GPU 上。这些有助于通过最小化跨 NUMA 节点通信和减少多节点 GPU 工作负载中的延迟来优化性能。

在 NVLink-5 NVL72 系统上，单个机架级 NVLink 域提供高达 130 TB/s 的聚合双向 GPU 到 GPU 带宽，相当于每 GPU 约 1.8 TB/s。在调度重集合通信的训练时，优先选择将流量保留在快速 NVLink 域内的放置，然后再跨越较慢的网络结构。

### 使用 Kubernetes 和 SLURM 进行作业调度

在多节点部署中，作业调度器对于最大化所有节点的资源利用率至关重要。通常，**SLURM** 用于训练集群，而 Kubernetes 通常用于推理集群。然而，已经出现了集成 SLURM 与 Kubernetes 的混合解决方案。开源的 **Slinky** 项目就是一个例子，旨在简化跨训练和推理工作负载的集群管理。

这些系统处理将 GPU 分配给作业并协调跨节点启动进程。如果一个训练作业请求 8 个节点，每个节点 8 个 GPU，调度器将识别符合条件的节点并使用 `mpirun` 或容器运行时（如 Docker）启动作业。这样，每个进程都知道作业中所有可用的 GPU。许多集群还依赖经过良好测试的 Docker 存储库（如 NVIDIA 的 NGC Docker 存储库）来保证跨所有节点的一致软件环境——包括 GPU 驱动程序、CUDA 工具包、PyTorch 库和其他 Python 包。

对于 SLURM，存在类似的问题。SLURM 有 GPU“通用资源”的概念，你可以定义某些 GPU 附属于某些 NUMA 节点或 NVLinks/NVSwitches。然后在你的作业请求中，你可以请求（比如）连接到同一 NUMA 节点的 GPU。

如果设置不当，调度器可能会将所有 GPU 视为相同，并为你的多 GPU 容器请求提供非理想的分配。正确的配置可以避免不必要的跨 NUMA 节点和跨 NVLink GPU 通信开销。

SLURM 也支持将 MIG 分区调度为不同的资源。这对于在一个 GPU 上打包多个作业很有用。这类似于 Kubernetes 如何使用 Kubernetes 设备插件调度 GPU 切片。接下来，我们将讨论如何在 Kubernetes 中使用 MIG 切片。

### 使用 MIG 切分 GPU

当你启用 NVIDIA 的 MIG 模式（在前面章节介绍过）时，单个物理 GPU 被切分为更小的、固定的且硬件隔离的分区，称为 MIG 实例。下面是一个 Kubernetes Pod 配置示例，用于两个 `nvidia.com/mig-2g.45gb` MIG 切片（此配置假设 NVIDIA Kubernetes 设备插件已配置为识别每个节点上的 MIG 设备）：

```yaml
resources:
  limits:
    nvidia.com/mig-2g.45gb: "2"
```

这里，配置指定在一个节点上运行 Pod，该节点至少有两个空闲的 `2g.45gb` 实例在一个 GPU 上；换句话说，2 个切片，每个切片是 SM 的 2/7 (2g)。如果一个 GPU 总共有 132 个 SM，每个是 2/7 × 132 SMs = ~38 SMs。乘以 2，Pod 分配了总共 ~76 个 SM。总内存分配是 45 GB 的 GPU RAM。

请注意，调度器不能跨 GPU 或节点拆分这些资源。Kubernetes 只有在**单个节点**能提供两个分区时才会调度该 Pod。这是因为 Pod 不能跨越多个节点。如果没有任何单个节点有两个可用的 `2g.45gb` 切片提供总计 76 SMs 和 45 GB GPU RAM（如前计算），Pod 将保持在 Kubernetes Pending（未调度）状态，因此不会运行——即使其他节点合计有足够的 MIG 容量。

此约束突显了根据典型工作负载需求规划 MIG 大小的重要性。例如，如果许多作业请求 `2g.45gb` 切片，你可能会配置每个 GPU 托管三个 `2g.45gb` 实例——在其七个可能的切片中——以便两个这样的实例可以在一个 GPU 上共存以供单个 Pod 使用。

> 这种单节点约束可能会导致 Pod 永远无法运行——即使可以在集群的不同节点上找到组合的 MIG 资源。只有在单个节点上可获得请求的 MIG 资源时，请求才能得到满足。

MIG 的一个管理缺点是，在 MIG 模式和正常（非 MIG）模式之间切换 GPU 需要重置 GPU——或重启计算节点。所以这不是调度器可以轻易地为每个作业动态做的事情。然而，你通常会提前创建 MIG 分区，并让配置运行一段时间。

在 Kubernetes 环境中，NVIDIA Kubernetes GPU Operator 的 **MIG Manager** 可以自动在节点上配置和保留 MIG 分区。这样，MIG 切片在重启和驱动重新加载后保持活动状态。

你可以给一个 K8s 节点打上 "mig-enabled" 标签，给另一个打上 "mig-disabled" 标签，并让调度器相应地放置作业/Pod。这更多是一个操作细节，但知道 MIG 是一个真正的静态分区——而不是动态调度器的产物——是很好的。

> 使用 MIG 时建议启用持久化模式，以便即使没有作业运行，MIG 配置也保持在 GPU 上。这样，GPU 不必在运行每个周期性作业之前重建切片。

### 为 Kubernetes 优化网络通信

当你使用 Kubernetes 容器运行多节点 GPU 工作负载时，Pod 需要相互对话。在 Kubernetes 中，默认情况下，Pod 拥有自己的 IP，并且不同节点上的 Pod 之间可能存在覆盖网络 (Overlay Network) 或网络地址转换 (NAT)。这可能会引入复杂性和额外的开销。

通常，GPU 集群最简单的解决方案是为这些性能敏感型作业使用**主机网络 (Host Networking)**。这意味着容器的网络不是隔离的，因为它直接使用主机的网络接口。要在 Kubernetes 中启用此功能，你在 Pod 规范中设置 `hostNetwork: true`。在 Docker 中，你可以使用 `--network=host` 运行。

使用主机网络允许容器完全像主机一样访问 InfiniBand 互连——没有任何额外的转换或防火墙层。这对于 MPI 作业特别有用，因为它消除了为每个 MPI rank 配置端口映射的需要。

然而，如果由于安全策略主机网络不是一个选项，你必须确保你的 Kubernetes 容器网络接口 (CNI) 和任何覆盖网络能够处理所需的流量。在这种情况下，你可能需要打开特定端口以支持 NCCL 的握手和数据交换，使用 `NCCL_PORT_RANGE` 和 `NCCL_SOCKET_IFNAME` 等环境变量来帮助建立连接。

当在覆盖网络上运行时，保持内核空间中的低延迟至关重要。此外，确保没有用户空间代理限制节点间的流量。这些因素会显著影响性能。

当使用 Kubernetes 环境并希望启用 RDMA 时，考虑安装来自 Mellanox 的 **Kubernetes RDMA 设备插件**。此插件在 Pod 接口上暴露 InfiniBand 和 GPUDirect RDMA 端点，以启用低延迟、零拷贝网络。

> 如果你有 InfiniBand 或 RoCE 网络，记得如果你的 NIC 支持，就在 NVIDIA 驱动中启用 GPUDirect RDMA。这允许 GPU 直接与 NIC 交换数据——绕过 CPU 进行节点间通信。这对于在多节点环境中维持高性能至关重要。

### 减少 Kubernetes 编排抖动

运行像 Kubernetes 这样的编排器意味着每个节点上都有一些后台进程在运行（例如 Kubernetes "kubelet"），容器运行时守护进程，以及（理想情况下）监控代理。虽然这些服务消耗 CPU 和内存，但消耗量大约是一个核心的百分之几。因此它们不会从基于 GPU 的训练作业中窃取明显的时间，后者使用这些核心进行数据加载和预处理。

然而，如果训练作业运行在一个同时也运行推理工作负载的节点上，你可能会经历一些**抖动 (Jitter)**，即执行时间和吞吐量的不可预测变化。这在任何多租户情况下都很常见。如果同一台机器上的另一个容器意外地使用了大量 CPU 或 I/O，它将通过竞争相同的资源影响你的容器——无论是训练还是推理。

> 同构工作负载（如全训练或全推理）比混合了训练和推理的异构混合更容易从系统角度进行调试和调优。

### 改善资源保证 (Improving Resource Guarantees)

为了防止资源竞争，Kubernetes 允许你为 Pod 定义资源请求 (`requests`) 和限制 (`limits`)。例如，你可以指定你的训练作业需要 16 个 CPU 核心和 64 GB RAM。Kubernetes 随后将为你作业独占保留这些资源，并避免在相同的 CPU 上调度其他 Pod。

这些限制使用 Linux cgroups 强制执行，所以如果你的容器超过其分配，它可能会被 OOM 杀手节流甚至终止。通常的做法是使用资源请求——并可选地使用 CPU Manager 功能来绑定核心——以确保性能关键型作业获得对必要 CPU 资源的独占访问权，从而其他进程无法从你的保留核心窃取 CPU 时间。

另一个抖动来源是后台内核线程和中断，正如我们在第 2 章关于使用中断请求 (IRQ) 亲和性的背景下讨论的那样。类似于 Kubernetes，如果其他 Pod 正在使用与你的作业相同的网络或磁盘，其他 Pod 可能会在托管你作业的计算节点上引起大量中断和额外的内核工作。这将导致抖动并影响你的作业性能。

理想情况下，GPU 节点完全专用于你的作业。然而，如果不是，你应该确保使用 Linux cgroup 控制器对 I/O 和 CPU 进行仔细分区，以便其他工作负载不会干扰。

幸运的是，Kubernetes 支持 CPU 隔离，这确保 Pod 获得它们请求的专用 CPU 核心和内存——并防止其他 Pod 被调度到与你相同的 CPU 核心上。这避免了上下文切换和资源竞争带来的额外开销。

> 在实践中，性能敏感型 Kubernetes 作业应该请求给定节点的**所有** CPU 和 GPU，以便没有其他东西干扰或竞争作业的资源。说起来容易做起来难，但从性能和一致性的角度来看，这是理想的作业配置。

### 内存隔离与避免 OOM 杀手

如果没有适当限制，内存干扰也可能发生。Kubernetes 提供了原本的内存隔离支持（使用 Linux cgroups）。然而，一个贪婪的容器，如果不加约束，可能会分配主机上过多的内存。这将导致主机将其部分内存交换到磁盘。

如果一个无约束的容器使用了主机上过多的内存，臭名昭著的 Linux **“OOM 杀手”** 将开始杀死进程——并且可能是你的 Kubernetes 作业——即使你的作业不是使用过多内存的那个。

OOM 杀手在决定杀死哪个 Pod 时使用启发式算法。有时它决定杀死最大的运行 Pod，这很可能是你的大型训练或推理作业，因为它在 CPU RAM 中持有大量数据以供给 GPU。为了避免这种情况，你可以特意不对训练或推理容器设置严格的内存限制。这样，如果需要，它们可以使用所有可用内存。

通过适当的监控和警报，你可以确保作业不会尝试过度分配超出你预期的内存。如果你确实设置了内存限制，请确保它高于你实际预期使用的值。这提供了一点余量，以避免在长期运行的训练作业进行了三天后被 OOM 杀手杀死。

> 在 Kubernetes 中，没有 requests/limits 的 Pod 被视为 **BestEffort (尽力而为)**，最有可能被驱逐。为了获得 **Guaranteed (保证)** QoS，每个容器必须为 CPU 和内存设置 `requests == limits`。仅设置高限制将导致 **Burstable (可突发)** QoS，而非 Guaranteed。

### 处理 I/O 隔离

不幸的是，截至撰写本文时，Kubernetes 开箱即不提供原生的、一等公民的 I/O 隔离。虽然 Linux 确实支持使用 cgroup 控制器的 I/O 控制，但 Kubernetes 本身并不会像对 CPU 和内存那样自动强制执行 I/O 限制。

如果你需要确保 GPU 节点上的繁重 I/O 工作负载不相互干扰，你可能需要在节点级别手动配置 I/O 控制。这可能涉及调整 cgroup v2 I/O 控制器或使用其他 OS 级配置来分区 I/O 资源。简而言之，虽然 Kubernetes 通过调度和资源请求防止 CPU 竞争，但 I/O 隔离通常需要对底层 Linux 系统进行额外的、手动的调优。

值得注意的是，在容器内部，一些系统设置是从主机继承的。例如，如果主机将 CPU 频率缩放设置为性能模式，容器将继承该设置。但如果容器运行在虚拟化环境（如云实例）中，你可能无法更改这些设置。

始终确保主机机器经过调优是个好主意，因为容器无法更改像大页设置或 CPU 调节器限制这样的内核参数。通常，集群管理员通过基础 OS 镜像设置这些参数和设置。或者，在 Kubernetes 环境中，他们可能会使用像 NVIDIA GPU Operator 这样的东西来在每个节点上设置持久化模式和其他 sysctl 旋钮。

---

## 关键要点 (Key Takeaways)

以下是本章的关键要点列表，包括跨操作系统、驱动程序、GPU、CPU 和容器层的优化：

**数据和计算局部性至关重要**
> 确保数据存储和处理尽可能靠近计算单元。使用本地、高速存储（如 NVMe SSD 缓存）来最小化延迟并减少对远程文件系统或网络 I/O 的依赖。

**实施 NUMA 感知配置和 CPU 亲和性**
> 通过在同一 NUMA 节点内对齐进程和内存分配来优化 CPU 到 GPU 的数据流。使用 `numactl` 和 `taskset` 等工具绑定 CPU 可防止跨节点内存访问。这将导致更低的延迟和更高的吞吐量。

**最大化 GPU 驱动和运行时效率**
> 微调 GPU 驱动程序设置，例如启用持久化模式以保持 GPU 处于就绪状态。考虑使用多进程服务 (MPS) 等功能在单个 GPU 上重叠来自多个进程的工作。对于多租户环境，探索 MIG 分区以有效地隔离工作负载。

**有效地预取和批处理数据**
> 通过提前预取数据并将小 I/O 操作批处理为更大、更高效的读取来保持 GPU 饱腹。利用 PyTorch `DataLoader` 的预取机制 (`prefetch_factor` 连同 `num_workers`) 提前加载多个批次。

**加载数据时锁定内存**
> 结合数据预取与内存锁定（使用 PyTorch `DataLoader` 的 `pin_memory=True`）使用锁页 CPU 内存（页锁定，不可交换到磁盘），以实现更快、异步的数据传输到 GPU。结果是，数据加载和模型执行可以重叠，空闲时间减少，CPU 和 GPU 资源都得到持续利用。

**优化内存传输**
> 利用锁页内存和大页等技术加速主机和 GPU 之间的数据传输。这有助于减少拷贝开销，并允许异步传输与计算重叠。

**重叠通信与计算**
> 通过将梯度同步和数据暂存等内存操作与正在进行的 GPU 计算重叠，减少数据传输的等待时间。这种重叠有助于保持高 GPU 利用率和更好的整体系统效率。

**调优和扩展网络栈**
> 在多节点环境中，使用支持 RDMA 的网络（例如 InfiniBand/Ethernet），并调优网络设置（如 TCP 缓冲区、MTU 和中断亲和性）以在分布式训练和推理期间维持高吞吐量。

**使用容器化和编排保持一致性**
> 使用像 Docker（配合 NVIDIA Container Toolkit）这样的容器运行时和像 Kubernetes（配合 NVIDIA GPU Operator 和设备插件）这样的编排平台，以便整个软件栈——包括驱动程序、CUDA 库和应用程序代码——在所有节点上保持一致。这些解决方案有助于对齐 CPU-GPU 亲和性并基于硬件拓扑管理资源分配。

**消除容器运行时开销**
> 虽然容器增加了可复现性和部署的便利性，但要确保 CPU 和 GPU 亲和性、主机网络和资源隔离已正确配置，以最小化任何容器开销。

**使用编排和调度最佳实践**
> 像 Kubernetes 这样强大的容器编排器对于确保高效的资源分配至关重要。先进的调度技术——如 Kubernetes Topology Manager——有助于确保拥有快速互连的 GPU 被聚集在一起。

**通过动态适应性和扩展性追求灵活性**
> 编排层跨节点分发工作并动态管理工作负载分割。这种灵活性对于扩缩训练任务和确保推理场景（其中数据负载和请求模式变化很大）中的高效运行时都至关重要。

**持续且增量地调优**
> 系统级优化不是一劳永逸的。定期监控性能指标；随着工作负载的发展调整 CPU 亲和性、批次大小和预取设置；并累积使用这些小的改进来实现显著的性能增益。

**减少整个栈的瓶颈**
> 最终目标是确保从 OS 和 CPU 到 GPU 驱动程序和运行时的所有组件和谐工作。消除一层的瓶颈（如 CPU 内存分配或驱动程序初始化）能解锁 GPU 的全部潜能，这直接转化为更快的训练、更低的成本和更高效的资源使用。

总之，这些策略致力于**最小化数据传输摩擦，减少等待时间，并确保你的硬件在高效训练和推理中发挥最大潜能。**

---

## 结论 (Conclusion)

本章证明，即使是最先进的 GPU 也会受到其周围环境低效的阻碍。调优良好的操作系统、容器运行时、集群编排器和软件栈构成了高性能 AI 系统的骨脊。通过利用 NUMA 感知绑定和本地存储解决方案对齐数据与计算，重叠通信与计算，以及微调主机系统和 GPU 驱动程序，你可以降低延迟并增加吞吐量。

**把你的整个系统想象成一辆精密工程的跑车**，其中每个组件（CPU、内存、GPU、网络、容器、编排器和编程栈）必须无缝协作才能交付最大性能。像启用持久化模式或优化 CPU 调度这样的小调整，单独看可能微不足道，但当在大型 GPU 集群中结合并扩展时，它们可以带来巨大的时间和成本节省。这些优化确保 GPU 在训练海量 Transformer 模型和运行复杂推理流水线时，始终在接近峰值效率的状态下运行。

随着领域的发展和模型持续增长，系统级调优的重要性只会增加。本章讨论的技术赋予性能工程师和系统架构师利用每一比特硬件潜力的能力。这实现了更快的迭代周期和更具成本效益的 AI 部署。最终，一个深度优化的系统加速了研究，并使尖端 AI 应用能够触达更广泛的受众。

最后，请记住，虽然硬件和软件栈看起来像是一个有着无数互连旋钮和开关的难以管理的庞然大物，但小的调整可以转化为显著的时间和成本节约。通过持续监控性能指标并增量地改进栈的每一层，你可以将潜在的瓶颈转化为效率收益的机会。**让数据为你指路，你将解锁 AI 系统的全部潜能。**







**译境** 再次启动。

第四章将视角转向了 AI 系统的神经网络——**通信**。在多机多卡的超大规模训练和推理中，计算往往不是唯一的瓶颈，数据的流动速度决定了整个系统的上限。本章深入剖析了 NVIDIA Magnum IO 技术栈（NCCL, RDMA, GDS）以及针对推理优化的 NIXL 库。

译文将力求在技术术语的精确性与工程实践的流畅性之间找到平衡，特别是对于“计算通信重叠”、“分离式推理”以及各种复杂的网络拓扑优化策略的描述。

以下是 **第四章：分布式网络通信调优** 的中文译本。

---

# 第四章：分布式网络通信调优

在当今的 AI 版图中，GPU、存储和网络接口之间无缝且低延迟的数据移动已成为刚需。在本章中，我们将介绍用于训练的 **NVIDIA Magnum IO**（例如 NCCL、GPUDirect RDMA、GDS）以及用于分离式推理 (Disaggregated Inference) 的 **NIXL**。我们将把这些技术放在现代 GPU 和像 NVL72 这样的集群背景下进行讨论。你将了解到这些库——及其底层支持的硬件——如何构成了超大规模 AI 系统所需的关键结构。

在大规模系统中，即使是最快的 GPU 也会被低效的通信以及来自内存和磁盘的数据传输所拖累。我们将讨论加速数据传输的策略、正确的数据分片技术、如何直接利用高速存储子系统，以及在 GPU 上实现**通信与计算重叠 (Overlapping Communication and Computation)** 的高级模式。通信与计算重叠是我们将在 AI 系统性能工程之旅中反复提及的一个常见模式。

我们将探索利用 NVIDIA I/O 加速平台 **Magnum IO** 的组件（包括 **NCCL**、**GPUDirect RDMA** 和 **GPUDirect Storage (GDS)**）来实现通信与计算重叠的重要性。我们将演示如何使用这些库来降低通信延迟，减少 CPU 开销，并最大化多节点、多 GPU AI 系统各层级的吞吐量。

像 PyTorch 这样的高级 AI 框架可以使用这些底层库来实现计算与通信的重叠。将这些技术集成到你的 AI 系统中，代表了一种整体性的加速方法，无论是对于训练超大规模模型，还是扩展分布式推理服务器以支持拥有数十亿用户的应用，都至关重要。

所有这些优化都确保每个组件都调整到了峰值性能。性能工程师需要仔细配置和调优网络与存储结构，以维持高水平的 GPU 利用率和“**有效吞吐量 (Goodput)**”。

---

## 通信与计算重叠（流水线化）

通信与计算的重叠，或称**流水线化 (Pipelining)**，在构建大规模高效训练和推理系统中扮演着关键角色。在这些环境中，让 GPU 保持忙碌并减少等待数据的时间至关重要。

核心思想是确保数据传输与正在进行的计算并发发生，这样当一个任务完成时，下一阶段所需的结果已经在传输途中或已送达。现代框架如 PyTorch 支持异步操作，使得集合通信（例如梯度的 All-Reduce）可以与计算任务并行运行。这减少了 GPU 的空闲时间（见图 4-1），并提高了整体系统吞吐量。

*(图 4-1: 在多个 CUDA 流 0–3 上重叠主机到设备 (H2D) 和设备到主机 (D2H) 的通信与计算)*

基于 CUDA 的库利用了多个 **CUDA 流 (Streams)** 的能力。当一个流执行计算密集型的矩阵乘法时，另一个流处理通信任务，如数据拷贝和 All-Reduce 调用。当神经网络的一层完成计算时，上一层的输出已经在去往聚合或进一步处理的路上了。这种重叠确保系统在产出结果时没有不必要的等待期，并维持稳定的数据流。

增加通信事件之间执行的计算量，可以进一步最小化通信的相对开销。当系统处理更大的数据批次 (Batch) 时，它在需要停止并交换信息之前会执行更多的计算。

例如，在分布式训练中，这表现为**梯度累积 (Gradient Accumulation)**，即来自几个小批次 (Minibatches) 的更新被合并为一个单一的同步步骤。通过降低通信事件的频率，系统降低了每次交换的开销并提升了整体吞吐量。

另一种支持计算与通信无缝重叠的技术是**压缩 (Compression)**。压缩减少了需要传输的数据量。例如，如果模型在发送梯度前对其进行压缩，网络传输的数据量就会变小。这减少了传输时间并缓解了拥塞。

更短的传输时间意味着通信阶段对计算阶段的干扰更小。虽然压缩并不直接导致重叠，但它缩短了数据在网络上移动的时间窗口，允许计算工作更有效地并行继续。

现代深度学习框架还将大型张量通信拆分为较小的**桶 (Buckets)** 以促进重叠。像 PyTorch 这样的框架会自动将大的梯度或激活张量划分为几个桶，一旦它们可用就立即传输。与其等待整个层的梯度都准备好，不如让其中的一部分立即开始 All-Reduce。

通过调优桶的大小并适当地调度这些传输，可以实现更高程度的重叠，并防止通信延迟拖慢计算流水线。像 PyTorch Profiler 和 NVIDIA Nsight Systems 这样的工具可以让你深入了解计算和通信是否正在重叠，从而允许工程师调整这些参数以获得最大效率。

通过将更大的批次大小、梯度累积、异步传输、压缩和分桶结合成一个协调的策略，大型分布式 AI 模型可以克服网络限制并减少空闲时间。这种设计在实现高吞吐量和最佳 GPU 利用率的同时，最小化了同步事件。

这些优化的结果是一个能减少整体训练和推理时间并更高效利用现有硬件资源的系统。工程师从重新发明底层网络例程中解放出来，可以专注于创新模型架构和调优高层参数，而不是编写复杂的数据传输机制。

### 使用流 (Streams) 进行异步执行

实现重叠从根本上依赖于**异步执行**。GPU 支持多个流（即操作队列），如果它们针对不同的资源，可以并发执行或重叠执行。一个流可以处理计算内核（如矩阵乘法），而另一个流处理通信（如数据拷贝和 All-Reduce 调用）。

通过将工作分配给不同的流并使用非阻塞操作，通信可以在后台发生。例如，可以在单独的流上启动 All-Reduce 操作而无需等待其完成。

与此同时，默认流继续在独立数据上进行进一步的计算。这要求像 NCCL 这样的通信库使用**非阻塞调用**，立即返回控制权。通过这样做，程序员确保在需要时进行适当的同步。

在实践中，AI 框架隐藏了大部分这种复杂性。PyTorch 的 `DistributedDataParallel` (DDP) 自动在反向传播中安装钩子 (Hooks)，以便每个梯度桶在专用的通信 CUDA 流上触发异步 NCCL All-Reduce，而默认 CUDA 流继续计算后续层的梯度。

> 我们将在后续章节深入探讨 CUDA 流，但现在只需知道它们对于重叠通信和计算——以及避免代码中不必要的同步点——非常有用。

这种利用 CUDA 流交错计算和通信的方式创造了一种稳定的波浪，或称**级联流水线**，它隐藏了通信延迟并使 GPU 始终保持忙碌。为了维持适当的重叠，应避免使用 `torch.cuda.synchronize()` 进行不必要的同步点，或通过 `torch.Tensor.item()` 将张量移动到 CPU 从而无意中触发全设备同步。如果你确实需要测量整体迭代时间，请在迭代的最末端放置一个单一的同步，以等待所有 GPU 工作完成，而不破坏正在进行的流水线。

### 减少通信频率和数据量

如前所述，在每个通信步骤执行更多的工作可以增加重叠和效率。模型训练期间的**梯度累积**就是这样一种技术。你不必为每个小批次都进行 All-Reduce 梯度，而是累积几个小批次的梯度，在本地求和，然后只做一次 All-Reduce。这实际上是用存储未归约梯度的内存来换取更少的同步点。

考虑累积四个小批次；你将 All-Reduce 的频率降低了 4 倍。这允许在同步之间发生更多的计算。缺点是你的有效批次大小增加了。这可能会影响模型收敛和内存使用。你通常可以找到一个“甜蜜点”，在通信频率、内存使用和收敛性之间取得良好的平衡。

另一种减少通信量的方法是压缩或量化交换的数据。像梯度压缩这样的技术可以在不显著影响模型质量的情况下减少每次通信发送的数据量。发送的数据越少意味着传输越快，也就有更多机会将这些传输隐藏在计算之后。这方面的极端是**稀疏化 (Sparsification)**。使用稀疏性时，你只发送梯度的一小部分。但这通常需要算法上的改变来保持准确性。

**分桶 (Bucketing)**，如 PyTorch 的分布式数据并行 (DDP) 通信机制所实现的那样，也通过将许多小张量分组为较大的消息来减少每次调用的开销。然而，桶的大小是一个权衡。非常大的桶最大化了带宽利用率，但延迟了通信的开始，因为你需要等待更多梯度累积才能启动 All-Reduce。非常小的桶更早开始传输，但由于许多小的 NCCL 调用而产生更多开销。

截至撰写本文时，PyTorch DDP 中的默认桶大小为 25 MB。这是一个在大多数情况下重叠效果良好的平衡点。但是，如果你有一个具有非常大层的模型，你可能会增加此值以减少开销。如果你有一个包含许多小层的模型，你实际上可能会受益于更小的桶以便更快地开始通信。最终，实现最大重叠可能需要对不同的桶大小进行性能分析，看看哪个能产生最佳的迭代时间。

### 实战：实现最大化重叠

为了看到重叠通信与计算的好处，让我们通过一个例子来比较两种场景：一种在所有计算完成后同步执行梯度通信且无重叠，另一种通信与计算重叠（如 DDP）。我们将模拟一个使用两个 GPU 的简单分布式训练步骤来说明差异。

假设我们不使用 DDP 的内置重叠，而是手动实现分布式训练，使得每个进程在本地计算所有梯度，然后在反向传播结束时对这些梯度执行 All-Reduce。这将模拟未优化、无重叠的场景，因为通信仅在所有计算完成后发生。我们可以使用 PyTorch 的分布式原语来模拟这一点，禁用 DDP 的钩子，并在 `loss.backward()` 后显式调用 `dist.all_reduce`。

*(此处省略部分代码示例，核心逻辑是手动调用 backward 后再同步调用 all_reduce)*

在无重叠代码中，我们在 `loss.backward()` 之后显式地对每个参数的梯度执行 All-Reduce。这实际上就是 DDP 内部所做的，但这里我们等待并在整个反向传播完成后才做——而不是在反向传播期间并发地做。

如果我们对这个迭代进行计时，All-Reduce 操作将直接增加迭代时间，因为它没有与任何其他步骤重叠。相比之下，完全重叠的实现可能会实现接近两者最大值的总时间。

现在让我们使用 PyTorch 的 DDP 来执行带有重叠的相同操作。DDP 将挂钩到反向传播中，并将梯度归约与反向计算重叠。

*(此处省略 DDP 代码示例)*

使用 DDP 重叠方法，代码更简单，因为我们依赖 `DistributedDataParallel` 来处理梯度同步。当调用 `loss.backward()` 时，DDP 的内部归约器将梯度拆分为桶，并在每个桶准备好后立即在单独的 CUDA 流上启动 NCCL All-Reduce 操作。

如果模型非常小，所有梯度都装进一个桶里，DDP 可能只会在最后做一次 All-Reduce，这不会重叠多少。但对于更大的模型和更大的批次大小，会有多个桶和显著的重叠——展示了这种技术带来的惊人性能提升。

分析 DDP 案例将显示 NCCL All-Reduce 内核与反向计算内核交错。我们在时间线上会看到锯齿状模式：发生一些计算，然后发生一些 NCCL 通信，然后计算，以此类推。

**良好重叠的关键标志是总迭代时间低于计算时间 + 通信时间之和**，并且 GPU 很少因等待通信而空闲，因为 All-Reduce 发生在计算期间，如表 4-1 所示。

*(表 4-1: 重叠的收益)*

| 指标              | 无重叠 (手动同步)               | 有重叠 (DDP)                    | 备注                           |
| :---------------- | :------------------------------ | :------------------------------ | :----------------------------- |
| 总反向+通信时间   | 100% (基准)                     | ~70% 基准                       | 例如，由于重叠，每次迭代快 30% |
| 通信开始时间      | 反向传播完成后                  | 反向传播期间                    | DDP 中，通信在反向传播中途开始 |
| 通信期间 GPU 空闲 | 是 (反向后 GPU 等待 All-Reduce) | 极小 (通信运行时其他层仍在计算) | DDP 隐藏了大部分延迟           |
| SM (GPU) 利用率   | 较低 (通信期间 SM 空闲)         | 较高 (持续活动)                 | 重叠保持 GPU 更持续地忙碌      |

在这个例子中，重叠通信与计算产生了大约 30% 的迭代时间改进。在更大的训练作业中，收益将更加可观，因为更大的模型创造了更多的通信瓶颈潜力。PyTorch DDP 默认使用 25 MiB 的桶。这样，它一旦准备好就为每个桶启动 All-Reduce。调优 `bucket_cap_mb` 可以帮助针对你的特定模型拓扑增加重叠，但更大的桶会增加最后一个桶的延迟。

结论是，一个调优良好的 DDP 应该将大部分梯度通信与计算重叠。通常唯一不能重叠的通信部分是尾端，即最后一个梯度桶。

值得注意的是，某些编码模式可能会无意中消除重叠。例如，如果你执行任何强制在反向传播和下一次迭代之间同步的操作，你将暂停计算直到所有通信完成。

> 避免在你不确定所有异步 GPU 工作已完成之前，无意中将张量从 GPU 移动到 CPU 的操作（例如，对张量调用 `.item()`）。否则，你将强制同步，停顿计算，并减慢训练或推理工作负载。这通常发生在添加 `print()` 或 `log()` 语句进行调试时。这对性能是灾难性的。

简而言之，重叠计算和通信是分布式性能工程中最有效的技术之一。如果利用得当，它可以通过将通信延迟隐藏在有用工作之后来显著减少训练时间。

---

## NVIDIA Magnum IO 优化栈

**Magnum IO** 是 NVIDIA 的总体 I/O 加速平台，汇集了一系列技术以加速 GPU、CPU、存储和网络接口之间的数据移动、访问和管理。Magnum IO 架构有四个关键组件，涵盖存储、网络、网内计算和 I/O 管理，如图 4-2 所示。

*(图 4-2: NVIDIA Magnum IO 加速平台的四个组件)*

以下是图 4-2 中四个组件的描述：

*   **存储 I/O (Storage I/O)**: 由 **NVIDIA GPUDirect Storage (GDS)** 和 **BlueField SNAP** 等技术实现。这些技术允许 GPU 直接访问存储（包括 NVMe SSD），而无需通过主机 CPU 内存进行不必要的拷贝。我们将在第 5 章深入探讨 GDS。
*   **网络 I/O (Network I/O)**: 包括 **GPUDirect RDMA**、**NCCL**、**NVSHMEM**、**UCX** 和 **HPC-X** 等技术，以启用跨节点 GPU 之间的直接、高速数据传输，并在节点间通信中绕过 CPU。
*   **网内计算 (In-network compute)**: **SHARP** 在 Quantum 级 InfiniBand 交换机内部执行网内归约。归约算术发生在交换机芯片中。BlueField DPU 卸载网络功能并可托管控制服务。当启用了 NCCL RDMA SHARP 插件且网络结构具备 SHARP 固件时，符合条件的集合操作可以卸载到 IB 交换机，从而减少主机和 GPU 的开销。
    > 基于以太网的 GPU 集群依赖 RoCEv2 等技术进行 RDMA，但通常缺乏像 SHARP 这样的特性。这也是许多超大规模 AI 系统使用 InfiniBand 或类似高性能互连而非以太网的原因之一。SHARP 提供了显著的性能提升。
*   **I/O 管理 (I/O management)**: **NVIDIA NetQ** 和 **Unified Fabric Manager (UFM)** 等工具属于此类，为数据中心的 I/O 结构提供实时遥测、诊断和生命周期管理。

Magnum IO 现在的目标是最大化大规模 AI 和数据分析工作负载的端到端吞吐量。它继续随着新硬件的发展而演进，例如集成对 **NVLink Switch** 网络域的支持，以及利用 InfiniBand (Quantum-2/X800) 和以太网 (Spectrum-X) 的进步。

在本章中，我们将深入探讨其中的几个组件，如用于网络传输的 RDMA、用于集合操作的 NCCL、用于推理数据移动的 NIXL 以及用于高效数据加载的 GDS。此外，我们还将了解如何对它们进行性能分析和调优。

---

## 使用 RDMA 进行高速、低开销的数据传输

**RDMA (远程直接内存访问)** 是一种针对低延迟、高吞吐量数据传输优化的技术。RDMA 允许设备之间直接进行内存到内存的通信，而无需 CPU 负担不必要的数据拷贝操作。简而言之，RDMA 绕过了大部分传统的内核网络栈，允许 NIC 直接读/写应用程序内存。这避免了每个数据包的 CPU 参与，并减少了上下文切换和缓冲区拷贝。

在 Docker 和 Kubernetes 等容器环境中，确保容器可以直接访问主机的 InfiniBand 设备（例如 `/dev/infiniband`）。否则，NCCL 可能会静默回退到 TCP 套接字而不是 GPUDirect RDMA——并且没有任何明显的错误来突出这种降级。这会导致吞吐量从数十 GB/s 下降到仅几 Gb/s。

> **始终验证是否为真正的 GPUDirect RDMA。** 使用 `lsmod | grep nvidia_peermem` 确认内核模块已加载，并检查 `dmesg` 中的初始化信息。对于端到端检查，运行带 `NCCL_DEBUG=INFO` 的 NCCL 以确认 NET/IB 路径，并使用带 `--use_cuda` 的 RDMA perftests 验证 GPU 到 GPU 的传输。

NVIDIA 针对 GPU 的 RDMA 实现称为 **GPUDirect RDMA**。它允许支持 RDMA 的 NIC（如 InfiniBand 和 **RoCE (RDMA over Converged Ethernet)**）跨两台服务器直接对 GPU 的设备内存执行 DMA——完全绕过主机 CPU 和系统 RAM。

*(图 4-3: 使用 RoCE 的 GPU 到 GPU 直接数据传输)*

通过向 NIC 注册 GPU 缓冲区，GPUDirect RDMA 启用了远程 GPU 之间的单边 RDMA 读写。这最小化了多节点训练中的延迟和 CPU 开销。

RDMA 与标准 TCP/IP 网络之间使用 RDMA 的性能差异可能是巨大的。例如，现代 InfiniBand 链路对于小消息可能提供几微秒量级的延迟，而以太网上的标准 TCP 可能会产生 5–10 倍的更高延迟。

对于受网络带宽限制的大型传输，InfiniBand 上的 RDMA 可以维持数百 Gbps 的极高吞吐量。相比之下，典型的 TCP/IP 网络可能受限于内核开销和 NIC 速度。在分布式深度学习中，大消息吞吐量通常比微小消息延迟更重要，因为梯度很大。

如果你只有以太网可用，你应该使用尽可能高的带宽和最低的延迟配置。例如，200+ Gbps 的带 RDMA (RoCE) 的以太网在处理 All-Reduce 流量时将比基本的 10–25 Gbps TCP 网络表现好得多。至少，确保你正在使用**巨型帧 (Jumbo Frames)**，例如 MTU 9000。这也减少了 CPU 开销。

此外，调优 TCP 栈也很重要。你应该验证 Linux sysctl 参数（如 `net.core.rmem_max`/`wmem_max`）以及自动调优范围是否设置得足够高以充分利用高带宽链路。

在云或混合环境中，要警惕你是否真的拥有受控的高速连接。例如，如果你使用带有 **Elastic Fabric Adapter (EFA)** 的 AWS EC2 实例，你会得到类似于同一“置放群组 (Placement Group)”内实例间 InfiniBand 级别的 RDMA。

---

## 调优多节点连接 (Tuning Multinode Connectivity)

对于使用 GPU 的分布式多节点训练，确保网络不是瓶颈至关重要。这涉及使用正确的通信和网络技术——以及正确配置它们。以下是一些建议和应避免的陷阱：

**了解拓扑**
> 使用 `nvidia-smi topo -m` 获取基本的 GPU 互连视图，但对于基于 NVSwitch 和 NVLink 的系统，建议还使用 `nvidia-smi nvlink` 或 Nsight Systems 来了解多跳交换结构连接。

**利用 NVLink Switch 域（如果可用）**
> NVIDIA 的多节点 GB200 和 GB300 **NVL72** 机架解决方案使用 NVLink Switch 将多达 72 个 GPU 连接在单个 NVLink 域中，提供极低的每跳延迟。如果你的集群包含此类基础设施，请确保你的作业放置在同一个 NVLink 域内，以充分利用这种超快互连。

**尽可能使用 RDMA**
> 如果运行在 InfiniBand 或支持 RoCE 的硬件上，请确保你的通信库（如 NCCL）实际上正在使用 RDMA。如果 RDMA 配置错误或不受支持，NCCL 可能会静默回退到 TCP。

**利用多 NIC 聚合带宽（如果可用）**
> 某些服务器有多个网络接口 (NIC)。NCCL 可以跨多个 NIC 条带化流量（称为 **Multirail**）以增加带宽。但你可能需要设置一些环境变量，如 `NCCL_NSOCKS_PERTHREAD` 和 `NCCL_SOCKET_NTHREADS` 来优化这一点。

**利用优化的“直接 NIC (Direct NIC)”模式**
> 物理上，NIC 通过 PCIe 连接到主机 CPU 或 DPU。在现代 GPU 系统中，NCCL 支持使用 **InfiniBand GPUDirect Async (IBGDA)** 和直接 NIC 路径的 GPU 发起网络，如图 4-4 所示。这让 GPU 在无需 CPU 干预的情况下驱动全带宽 RDMA。

*(图 4-4: 通过 GPU 和 NIC 之间的直接连接绕过 CPU 瓶颈)*

### 多节点通信陷阱

**陷阱 #1: 使用受 CPU 限制的 Gloo 后端而不是 NCCL**
PyTorch 的分布式框架支持多种后端。对于多 GPU 训练，**NCCL** 是 NVIDIA GPU 的首选后端。还有一个名为 **Gloo** 的回退后端，它使用 CPU 和 TCP 套接字。如果错误地使用 Gloo 初始化 ProcessGroup 进行 GPU 训练——或者如果 NCCL 初始化失败并回退到 Gloo，所有跨 GPU 通信都将通过 CPU 和以太网栈进行。这会导致性能极慢。

> **始终为多 GPU 训练指定 NCCL 以利用 RDMA 通信。** 幸运的是，这是 PyTorch 的默认设置。

**陷阱 #2: NCCL 版本不匹配**
如果你运行 PyTorch 捆绑的 NCCL 与系统安装的 `libnccl` 不同版本，可能会挂起系统。或者更糟，静默回退到较慢的实现。确保通过匹配 `nvidia-nccl-cu*` 包或针对系统 NCCL 重新构建 PyTorch 来保持一致。

**陷阱 #3: NCCL 引导期间的 TCP 端口耗尽**
NCCL 使用临时 TCP 端口进行带外设置。如果你的 OS 的 `net.ipv4.ip_local_port_range` 太窄，你可能会耗尽可用端口，导致握手失败或停滞。建议扩大端口范围以避免隐藏的引导故障。

**陷阱 #4: 网络带宽不足或 NIC 配置错误**
另一个多节点陷阱仅仅是网络带宽不足以满足正在同步的数据量。例如，使用 Blackwell GPU 很容易使每个节点的单个 400 Gbps 链路饱和。使用 `nvidia-smi dmon` 或 `ethtool` 监控网络吞吐量。考虑启用 NCCL 的多 NIC 支持（Multirail）。

**陷阱 #5: 掉队节点 (Straggler nodes) 或进程**
在多节点训练中，最慢的节点或 GPU 将决定整体速度。为了避免掉队者，如果可能，请为每个训练作业使用**同构硬件**和专用集群资源。使用像 NVIDIA DCGM 这样的监控工具来发现某个节点是否因热节流或 NIC 链路翻转而性能下降。

**陷阱 #6: UCX/RDMA 下的 GPU 内存碎片化**
PyTorch 的缓存分配器跨迭代持有 GPU 内存。在由于 UCX/RDMA 而长期存在的分配中，这可能会耗尽注册池或导致内存碎片化。监控 `torch.cuda.memory_reserved()` 与 `memory_allocated()` 有助于发现这些边缘情况。

---

## 用于分布式多 GPU 通信的 NCCL (NCCL for Distributed Multi-GPU Communication)

NVIDIA NCCL 是一个多对多通信库，用于 GPU 组之间共享数据的操作，称为**集合 (Collectives)**。NCCL 支撑着 NVIDIA 生态系统中大多数多 GPU 训练工作负载。

NCCL 提供了优化的集合通信操作实现，如 All-Reduce, All-Gather, Broadcast, 和 Reduce-Scatter，可从几个 GPU 扩展到数千甚至数万个 GPU。NCCL 支持通过 PCIe, NVLink, NVSwitch, InfiniBand, 和 TCP 套接字进行通信。它会自动选择任意两个 GPU 之间可用的最快路径。

补充 NCCL 的是较新的 **NVIDIA Inference Xfer Library (NIXL)**，它针对推理和点对点传输（如 KV 缓存移动）进行了优化。

> 许多推理部署现在倾向于使用 NIXL 进行点对点传输，因为它具有更低的延迟。

### NCCL 中的拓扑感知 (Topology Awareness)

拓扑感知在 NCCL 的性能中起着主要作用。NCCL 检测 GPU 是如何物理连接的，并据此优化其通信模式。例如，在一个全连接的 NVLink 和 NVSwitch 系统中，每个 GPU 都会使用这些高速互连与所有其他 GPU 通信。

虽然 NCCL 可以使用像环形 (Ring) All-Reduce 这样的简单模式，但它会自动使用拓扑感知的**分层 (Hierarchical)** 通信模式来最大化性能。例如，对于具有多个 NUMA 节点域的系统，NCCL 可能首先执行节点内归约，然后是跨节点归约，最后是节点内广播。目标是最大化通过最快互连的流量。

NCCL 通常在检测到此类拓扑时会选择性能最高的方法。你可以使用环境变量 `NCCL_ALGO`（例如 `NCCL_ALGO=NVLSTree,Tree,Ring,PAT`）覆盖 NCCL 的算法选择，但通常 NCCL 在自动选择方面做得很好。

通过使用分析工具进行快速实验，可以揭示 NCCL 是否经过了拓扑优化。使用 Nsight Systems 或 NCCL 追踪，当使用分层算法时，你会看到多个 NCCL CUDA 内核。例如，你会看到一些组内 (Intragroup) All-Reduce 内核和一些组间 (Intergroup) 内核。

### NCCL 通信算法

NCCL 可以根据数据大小、GPU 数量和拓扑结构采用不同的通信算法。主要算法包括：

*   **Ring (环形)**: GPU 逻辑上排列成环。带宽最优，适合大消息（带宽主导型工作负载）。缺点是延迟随 GPU 数量线性增加。
*   **Tree (树形) 和 NVLSTree**: 在树结构中完成归约和广播。对于 N 个 GPU，可以在 O(log N) 步内完成。提供更低的延迟，适合小消息（延迟主导型工作负载）。使用 `NVLSTree` 将启用 NVLink SHARP 卸载。
*   **CollTree (分层树集合)**: 构建两级树以在保持高本地带宽的同时减少延迟。在每个快速本地域（例如节点内或 NVSwitch 岛内）形成本地树，然后由领导者参与跨组树。
*   **CollNet (跨节点分层集合)**: 结合两种策略。首先在共享快速本地互连的 GPU 组内聚合数据，然后由指定的领导者 GPU 参与跨组的二级树归约。这对于减少非常大的多节点 GPU 集群中的网络负载特别有效。
*   **PAT (并行聚合树)**: NCCL 的环形和树形算法的流水线混合体。一旦张量的一个段在 GPU 树上被归约，下一个段就同时开始其自己的树归约。这种重叠使得 PAT 能够保持链路饱和，实现接近纯环形 All-Reduce 的带宽，同时具有类似树算法的延迟优势。

### 分布式数据并行策略 (Distributed Data Parallel Strategies)

当在单个节点上扩展到多个 GPU 时，PyTorch 提供了 `nn.DataParallel` (DP) 和 `torch.distributed.DistributedDataParallel` (DDP)。

*   **DataParallel (DP)**: 使用单进程多线程控制多个 GPU。受限于 Python GIL，扩展性差，且梯度收集步骤是同步的，不与计算重叠。
*   **DistributedDataParallel (DDP)**: 每个 GPU 使用一个进程，依靠 NCCL 通信梯度。DDP 通过使用独立进程完全避免了 GIL 问题。并且 NCCL 的高效 C++ 内核处理通信，通常与反向传播计算**重叠**。

**结论：** 对于多 GPU 训练，**始终首选 DistributedDataParallel 而非 DataParallel**。

---

### 网内 SHARP 聚合 (In-Network SHARP Aggregation)

当使用支持网内计算的高级网络硬件时，如 NVIDIA 的 **SHARP (可扩展层次化聚合与归约协议)**，可以通过将部分集合操作卸载到网络本身来实现额外的性能增益。

SHARP 是一种用于 Quantum 级 InfiniBand 交换机的网内归约技术。在 NVLink 域中，类似的能力是 **NVLink SHARP (NVLS)**，它卸载 NVSwitch 结构内的集合操作。

实际上，SHARP 允许像 All-Reduce 这样的集合操作由网络结构部分计算。当来自多个 GPU 的数据流入交换机时，它将归约/聚合（例如求和）数据并共享部分归约的结果。这减少了每个 GPU 必须处理的数据总量，从而降低了大型 MPI 和 NCCL 集合操作的延迟。

对于 All-Gather 操作，NVLS 的硬件多播让每个 GPU 只发送一次数据段，而网络负责复制。这减少了发送方的数据量。

NCCL 可以通过使用 **NCCL RDMA SHARP 插件** 将集合操作卸载到启用了 SHARP 的 InfiniBand 结构。SHARP 的性能影响可能是巨大的。在某些情况下，NVIDIA 报告称在大规模 AI 系统上使用 SHARP 可使 All-Reduce 加速 **2 倍到 5 倍**。

---

## NVIDIA NIXL 与分离式推理 (NVIDIA's NIXL and Disaggregated Inference)

虽然 NCCL 擅长模型训练中常用的多对多组通信模式，但现代大规模 AI 推理引入了新的通信需求。**NVIDIA NIXL** 是一个开源的、高吞吐量、低延迟的点对点通信库，于 2025 年初发布。

NIXL 专门设计用于加速大规模 LLM 分布式和**分离式推理 (Disaggregated Inference)**。NIXL 简化了一对一和一对多的数据传输，例如以最小的延迟和开销移动 **KV 缓存**（由分离的阶段共享）。它补充了主要用于多对多集合操作的 NCCL。

NIXL 具有跨 GPU、CPU、SSD 和共享网络存储移动数据的一致异步 API。它总是为正在移动的每个缓存块选择最快的路径。这在 NVIDIA Dynamo 的 KV Cache Manager 中得到了体现，如图 4-5 所示。

*(图 4-5: NVIDIA Dynamo 分布式 KV Cache Manager 将较少访问的 KV 缓存卸载到更经济的存储层级)*

在扩展 LLM 推理时，在 GPU、CPU、计算节点和机架集群的对等点之间高效传输大型数据缓存（例如 Transformer 的注意力 KV 缓存）非常重要。例如，使用 NIXL，推理引擎可以使用 NVLink/InfiniBand 将大型（例如 100 GB）KV 缓存从 GPU 卸载到对等点，且开销极小。这释放了 GPU 以处理新请求。这对于服务具有大上下文窗口的 LLM 至关重要。

NIXL 利用 GPUDirect RDMA 直接在跨节点的 GPU 之间移动数据，完全绕过主机内存。实际上，支持 RDMA 的 NIC（或 DPU）直接在 GPU 内存之间执行传输。这就是延迟如此之低的原因。

### 分离式预填充和解码推理阶段

Transformer 模型的推理路径实际上分为两个不同的阶段：**预填充 (Prefill)** 和 **解码 (Decode)**。

第一阶段，预填充，通常是**计算密集型 (Compute Bound)** 的，因为它使用大量矩阵乘法从传入的请求数据（Prompt）构建 KV 缓存。第二阶段，解码，通常是**内存吞吐量密集型 (Memory-throughput Bound)** 的，因为它需要从 GPU HBM 内存中收集模型权重来计算下一组 Token。

这种预填充/解码分离在常见的推理引擎如 vLLM, SGLang, 以及 NVIDIA 的 Dynamo 和 TensorRT-LLM 中都有实现。**分离式服务 (Disaggregated Serving)** 将预填充工作者放在 GPU 集群中，将解码工作者放在另一个 GPU 集群中。预填充集群中的 GPU 生成输入序列的 KV 缓存，并使用 NIXL 将其传输到解码集群中的 GPU。图 4-6 比较了传统的“单体”服务模型与“分离式”服务模型。

*(图 4-6: 分离式服务将预填充和解码阶段分离到不同的 GPU 集群)*

在这种情况下，可能长达数十 GB 的 KV 缓存必须在近乎实时的情况下无缝地从一个处理单元移动到另一个。传统通过 CPU 内存甚至存储传递数据的方法无法满足要求。NVIDIA 创建 NIXL 正是为了解决这个场景。NIXL 允许对等点之间的高带宽 GPU 到 GPU 直接传输，并尽可能与计算重叠。

NIXL 智能地选择最快的通道，无论是同一块板上的 NVLink、跨机架域的 NVSwitch、跨机架的 InfiniBand/RoCE，甚至是直接 NVMe 存储访问。

---

## 关键要点 (Key Takeaways)

通过精心的工程设计——并使用本章描述的技术——你通常可以达到或接近物理硬件的“光速”极限。以下是调优网络层时需要记住的关键教训：

**拓扑至关重要**
> 节点间（InfiniBand）和节点内（NVLink/NVSwitch）的互连影响最佳通信策略。始终确保你使用了可用的最快互连——而不是因为配置错误而意外地通过慢速路径发送数据！利用 SHARP 等特性进行大规模网内聚合。

**调优环境和系统**
> 有时单个环境变量或 OS 设置就能提升吞吐量。例如，增加 NIC 缓冲区，启用/禁用 NCCL 特性和日志记录，以及正确绑定 CPU。

**利用最新的硬件创新**
> 新硬件如 Grace Hopper/Blackwell 超级芯片提供海量 CPU 内存和快速 CPU-GPU 互连。利用它们来托管大型数据集、卸载大型 KV 缓存。像 SHARP 这样的网内计算可以加速集合操作 2倍–5倍。

你需要让你的 GPU 饱和到 100% 计算的时间，同时在后台进行通信。你也想让你的网络链路充满有用的数据。并且你想让你的磁盘全速流式传输数据。所有这一切应该完美和谐地通过**协同设计**发生。

---

## 结论 (Conclusion)

高性能、分布式和多 GPU 通信及存储系统的演进，是调优大型复杂 AI 系统的基础。通过利用专门的库（如用于集合操作的 NCCL，用于高效推理数据传输的 NIXL，以及用于超低延迟通信的 RDMA），AI 系统可以显著减少瓶颈并提高性能。

对于性能工程师来说，教训是：**快速的数据移动与原始计算能力一样关键。** 如果世界上最快的 GPU 总是要在等待来自 CPU 或另一个 GPU 的数据，那么它提供的好处微乎其微。

在下一章中，我们将探索基于 GPU 的**存储策略**和优化。补充网络协议和库，GDS 以及高效的输入流水线是保持 GPU 持续获得工作供给的整体方法的一部分。









**译境** 已就位。

第五章将目光投向了 AI 系统的“粮草补给线”——存储 I/O。在千卡、万卡训练的规模下，如果数据喂送速度跟不上，再强大的 GPU 也会因饥饿而空转。本章深入探讨了如何打通从磁盘到 GPU 显存的高速通道，涉及 GPUDirect Storage (GDS)、DeepSeek 的 3FS 文件系统、以及数据流水线的精细化调优。

译文将力求在技术细节的精准度与工程实践的指导性之间保持平衡，特别是在描述复杂的 I/O 路径和异步流水线时，力求清晰透彻。

以下是 **第五章：基于 GPU 的存储 I/O 优化** 的中文译本。

---

# 第五章：基于 GPU 的存储 I/O 优化

对于 AI 工作负载而言，向 GPU 输送数据与计算本身一样重要。设想一个在数千个 GPU 上训练的 100 万亿参数模型的场景。这样的模型可能需要处理数十亿个训练样本，包括 Token、图像、音频和视频等。

这意味着必须从存储中读取海量数据，并尽可能快地输送给 GPU。如果存储流水线缓慢，GPU 就会“挨饿”并闲置。这将导致即便我们采用了复杂的通信优化，利用率依然低下。

本章致力于解决存储和输入流水线的优化问题。具体来说，我们将演示如何高效地从磁盘或远程存储读取数据，如何对其进行预处理，以及如何将其 I/O 与 GPU 计算重叠。

---

## 快速存储与数据局部性 (Fast Storage and Data Locality)

大型模型训练作业通常需要读取巨大的数据集。大语言模型的训练样本达到数十亿甚至数万亿级别是很常见的。这相当于 TB 级的文本数据或 PB 级的图像数据。

在超大规模下，你的存储系统必须持续提供巨大的吞吐量，以跟上成千上万个可能连续运行数月的 GPU。将 NVMe SSD 与机架共置——或使用具有机架本地交换拓扑的 **NVMe over Fabrics (NVMe-oF)**——可以最小化网络跳数并提高性能一致性。

如果你的数据位于网络附加存储（如 NFS 服务器或云对象存储，例如 Amazon S3）中，你需要确保所有计算节点的聚合读取带宽是充足的。考虑这样一个场景：基于模型和批次大小，每个 GPU 需要 200 MB/s 的训练数据才能保持忙碌。如果你总共有 8 个 GPU，那就需要大约 1.6 GB/s 的聚合带宽。像 Blackwell 和 Rubin 这样的现代高端 GPU 需要更高的带宽才能保持饱和。

一个 NVIDIA Grace Blackwell GB200/GB300 NVL72 机架拥有 72 个在一个 NVLink 域中连接的 Blackwell GPU。如果每个 GPU 需要 200 MB/s 的训练数据，这可能需要 **14–20 GB/s** 的聚合存储吞吐量才能让所有 72 个 GPU 保持忙碌。对于这类超大规模工作负载，你的存储解决方案需要相应地进行扩展。

> 如果你的工作负载流式传输更重的媒体或多模态样本，请使用你测得的“每样本字节数”和“每秒样本数”进行校准。在这种情况下，聚合需求可能会高得多。

一种解决方案是使用更快的本地存储，例如同一机架内的 NVMe SSD——或 NVMe-oF 网络拓扑。另一种解决方案是使用并行文件系统（如 Lustre 或 GPFS 等）将数据缓存在本地 SSD 上。假设存储系统能跟上，配置多个数据加载线程以保持管道饱和非常重要。注意 Python 的 **GIL (全局解释器锁)** 问题！

只要可能，将数据放置在物理上尽可能靠近计算节点的地方。“近”可能意味着在同一个物理节点上（如本地 NVMe SSD 驱动器），或者至少在同一个机架内并通过高速互连（如 NVMe over Fabric 或高级存储加速器）连接。

对于分布式多节点模型训练，一种常见的方法是将数据集**分片 (Shard)** 到各个节点，使得每个节点主要从其本地磁盘读取数据子集。例如，如果你有 100 TB 数据和 10 个节点，你可以预先将 10 TB 分割到每个节点的本地存储中。然后每个节点的数据加载器只从其本地的 10 TB 读取。这避免了用冗余读取通过网络——特别是当数据集大小无法轻易放入 RAM 时。

像 PyTorch 的 `DistributedSampler` 这样的框架工具会协调 worker，使每个进程在每个 epoch 获得唯一的数据切片。这与将数据分片到多个集群节点的目标非常一致。

---

## 顺序读取与随机读取模式 (Sequential Versus Random Read Patterns)

GPU 处理数据的速度极快，但为了效率，它们更喜欢以大的连续块读取数据。同样，存储设备在大规模顺序读取时的吞吐量远高于小规模随机读取。因此，在准备数据集或存储布局时，应尽可能安排顺序访问。

例如，在训练图像模型时，避免存储数百万个单独的图像文件，因为这将导致磁盘上大量的随机寻道。相反，考虑将它们存储在少数几个大的二进制文件（如 **Arrow**, **TFRecord**, 或 **Parquet**）、数据库文件、WebDataset tar 文件或等效格式中。在这些情况下，每个文件包含许多连接在一起的样本，这是理想的。

> 结合小文件为大分片在当今更快的 GPU 时代更为重要，因为过多的小规模随机读取会更快成为瓶颈。虽然大多数现代并行文件系统和对象存储可以处理一定程度的小随机读取，但最好还是显式验证性能。

从较大的文件中读取一块数据将自然地一次性获取许多样本。如果使用像 Amazon S3 这样的对象存储，出于同样的原因，通常会提前将较小的对象合并为较大的对象。

此外，调整读取大小也很重要，因为读取 **1 MB** 的块比 4 KB 的块能产生更好的吞吐量，因为每次读取的开销更低。许多数据加载库允许调整缓冲区大小和预取块大小。例如，Python 的 `open()` 使用操作系统的预读 (Read-ahead) 缓冲区来加速顺序扫描，但随机读取不会从更大的缓冲区或缓冲 I/O 库中受益太多。

相反，你应该将读取**批处理 (Batch)** 为更大的连续块，或使用高级数据集 API（例如 `TFRecordDataset` 或具有可配置预取大小的 PyTorch `IterableDataset` 和 `DataLoader`）。虽然许多框架和库内部已针对大顺序读取进行了优化，但调整它们的缓冲和预取参数仍然很重要。

如果你的访问模式必须是随机的，请使用调用 `pread()` 的线程或 Linux 的异步 I/O 接口（如 `io_uring`）并行发出多个读取。通过预注册缓冲区和轮询等功能，`io_uring` 允许以最小的内核开销提交批量 I/O 请求。它可以通过减少每次系统调用的开销来进一步提高随机读取吞吐量。这有助于隐藏延迟并实现高 IOPS。

应该使用针对大型并发 I/O 优化的文件系统。**XFS** 在 Linux NVMe 服务器上很常见。你应该使用 `noatime` 挂载它，以消除每次读取时昂贵的访问时间更新。对于像 Amazon EFS 这样的网络存储服务，确保你的 EFS 文件系统处于 **Max I/O** 性能模式以获得最高聚合吞吐量。如果你需要一致的带宽，可以从默认的突发 (Bursting) 吞吐量模式切换到预配置 (Provisioned) 吞吐量模式。这些设置确保你的 I/O 层能跟上大规模并行 AI 工作负载。

---

## 调优 NVMe 和文件系统以提高吞吐量

现代 Linux 使用多队列块 I/O 调度器 **blk-mq**，它将 I/O 分散到 CPU 核心上。对于快速 NVMe SSD，你可能需要调优队列深度和提交队列的数量。通常默认值是可以的，但如果你知道你的工作负载主要是顺序的，你可能会使用 `none` I/O 调度器。

传统的完全公平排队 (CFQ) 调度器已过时。现代内核默认对 NVMe 使用 `none` 或 `mq-deadline` 多队列调度器。可以使用 `/sys/block/<device>/queue/scheduler` 检查此设置。“none”调度器是低延迟工作负载的标准。在某些存储设备上，你可能会遇到预算公平排队 (BFQ) 调度器。

> 对于高性能 NVMe，建议仍使用 `none` 或 `mq-deadline` 多队列调度器以最大化吞吐量。这几乎总是开箱即用的配置，但值得快速检查验证。

另一个调优方面是**预读 (Read Ahead)**。当内核检测到顺序读取时，它会自动预读额外数据。你可以在 `/sys/block/<device>/queue/read_ahead_kb` 中查看预读设置。例如，默认情况下它可能设置为 128 KB。如果你正在流式传输大文件，请将其增加到几 MB。这将通过减少系统调用开销和流水线化读取来提高吞吐量。这可以使用 `blockdev --setra` 在设备上完成。

如果使用 NVMe SSD 磁盘，请确保它们设置在系统可用的最快接口上。并确保你有足够的通道（例如 PCIe），以免成为瓶颈。有时，可以使用 RAID 0 条带化多个 SSD，以充分利用这些设备并最大化吞吐量——特别是如果单个磁盘无法满足 GPU 需求时。

Linux **页缓存 (Page Cache)** 会自动将最近从磁盘读取的数据缓存到 RAM 中。对于大型数据集，你可能会超出可用 RAM 并导致缓存抖动 (Thrashing)。但对于中等大小的数据集，**热缓存 (Warm Caches)** 可以极大地加速训练。

如果你的数据——或其中很大一部分——可以放入 RAM（包括 Grace Blackwell 超级芯片上的 CPU + GPU 统一内存），你应该考虑在启动时将其完全**预加载 (Preloading)** 到内存中。这实际上为 GPU 创建了一个超快的内存内缓存。这可以极大地减少训练期间的磁盘 I/O。然而，对于海量的 PB 级数据集，这通常是不可行的。在这些情况下，使用优化 I/O 流式传输数据是必由之路。

确保在数据加载中使用多个 worker（例如，PyTorch 的 `DataLoader(num_workers=N)`）。这些独立的 CPU 线程/进程将并行获取和预处理数据，以喂饱训练作业中的多个 GPU。找到合适的 worker 数量是经验性的。

我们将于第 13 和 14 章深入探讨 PyTorch 性能调优，但这里值得注意的是，你应该启用 `pin_memory=True` 并使用 `non_blocking=True` 来启用重叠的主机到设备拷贝。并且通过设置 `persistent_workers=True`，你可以避免跨 epoch 的 worker 重生开销。针对每个工作负载调优 `prefetch_factor` 也很有用。对于 `num_workers > 0`，默认的 `prefetch_factor` 为 2。

Worker 太少，GPU 就会闲置。Worker 太多，线程将开始争夺可用的 CPU 核心和 I/O 带宽。监控 CPU 使用率和磁盘吞吐量。理想情况下，你希望磁盘吞吐量接近 100% 利用率，而 CPU 还有一些余量。

> 对于拥有极高核心数的 CPU，例如 GB200/GB300 超级芯片中使用的 72 核 NVIDIA Grace CPU，你通常可以使用更多的数据加载 worker。只需留意由过度 I/O 竞争引起的收益递减。

---

## 使用 NVIDIA GDS (GPUDirect Storage)

**GDS** 是一项允许 GPU 直接从存储设备或通过网络存储栈读取数据的特性，而无需在 CPU 内存中创建额外副本。通常，当 GPU 想要从 NVMe SSD 读取数据时，数据首先从 SSD 到 CPU 内存。然后一个 CUDA 调用将数据从 CPU 内存复制到 GPU 内存。

> GDS 与 GPUDirect RDMA 相辅相成，因为 GDS 加速**存储到 GPU** 的 DMA，而 GPUDirect RDMA 加速**网络到 GPU** 的 DMA。两者都不消除 CPU 的编排作用，但都移除了主机内存的“反弹缓冲区 (Bounce Buffer)”。

有了 GDS，GPU 可以针对 SSD 或 NIC 发起**直接内存访问 (DMA)**，将数据移动到其自己的 HBM 内存中。这绕过了通过 CPU 路径的额外拷贝。GDS 支持本地 NVMe 设备和使用 NVMe-oF 的远程存储。

实际上，GDS 创建了一条直接 DMA 路径，绕过了存储和 GPU 内存之间的主机内存反弹缓冲区。这拓宽了 GDS 对集群文件系统甚至某些对象存储系统的适用性。（注意：CPU 仍然负责配置和编排 I/O。）

启用 GDS 需要现代 NVIDIA GPU 和支持直接内存访问的存储栈——以及正确的 NVIDIA 驱动程序和 CUDA 工具包。通常使用本地 NVMe SSD 或 RAID 卷。GDS 支持取决于文件系统和支持 RDMA 的栈。截至撰写本文时，支持的栈包括 XFS/EXT4 上的本地 NVMe 和 NVMe-oF（需 `O_DIRECT`），NFS over RDMA，以及特定的并行文件系统，如 **BeeGFS**, **WekaFS**, **VAST**, **IBM Storage Scale**，以及其他与 `nvidia-fs` 集成的系统。

应用程序需要使用正确的 API。你可以使用 CUDA 的 `cuFile` 库通过 GDS 读取文件。`cuFile` 支持诸如自动缓冲区对齐和与常见文件系统集成等特性。

在实践中，如果你设置了 GDS 并且读取路径使用 `cuFileRead`，数据可以直接从磁盘流向 GPU 内存。这降低了 CPU 利用率（允许 CPU 做其他预处理），并可以提高吞吐量，特别是当 CPU 成为瓶颈时。`cuFileRead` 直接与 Linux 文件系统集成。你也可以使用 `cuFile` 的异步 API，如 `cuFileReadAsync` 和 `cuFileWriteAsync`，将存储 I/O 集成到 CUDA 流（将在第 11 章讨论）上以实现重叠和流水线化。

> 尽可能使用 `O_DIRECT` 以启用直接 DMA 并绕过 OS 页缓存。在现代 GDS 版本中，`cuFile` 也可以在非 `O_DIRECT` 文件描述符上操作，但不对齐可能会导致额外的拷贝或性能降低。

许多存储供应商如 **WekaIO**, **DDN**, **VAST**, **Cloudian** 等都已经发布了 GDS 感知的解决方案或插件，以便他们的系统可以直接使用 RDMA 将数据传送到 GPU 内存。这种生态系统支持意味着 GDS 可以开箱即用于企业级网络附加存储 (NAS) 和并行文件系统。

来自 **VAST Data** 的报告显示，在某些 AI 工作负载上使用 GDS 读取吞吐量提升了 20%。在他们的案例中，在单个 A100 GPU 上使用 GDS 实现了 20% 更高的顺序读取吞吐量，这在适用时显著接近了每 NIC 100 Gb/s 的链路容量。图 5-1 展示了有无 GDS 的架构对比。

*(图 5-1: VAST Data 的网络架构对比：有 GDS vs 无 GDS)*

左侧我们看到传统的通过主机内存拷贝的分阶段 DMA。右侧是使用 GDS 的直接 GPU 拉取，绕过了主机内存拷贝并降低了 CPU 利用率。VAST 的一份报告测量到在 NVIDIA Ampere A100 GPU 上有 20% 的读取吞吐量提升，而在 Hopper H100 GPU 上提升超过 30%，这是由于其更高的 NIC 带宽和更大的 CPU 负担。

然而，GDS 可能需要调优，并非所有工作负载都能看到巨大提升。如果你的 CPU 处理数据传输很轻松，GDS 可能不会改变太多吞吐量。但是，它会降低 CPU 使用率，从而释放 CPU 执行数据处理和其他任务。另一方面，如果 CPU 因许多 `memcpy` 操作而饱和，那么 GDS 将大有裨益。

使用 GDS 时必须确保正确应用 `O_DIRECT` 语义和对齐。主机锁页内存不用于存储到 GPU 的数据路径。`cuFile` 注册 GPU 设备缓冲区，`nvidia-fs` 内核驱动程序直接在存储设备或 RDMA NIC 与 GPU 内存之间编排 DMA。它直接与 POSIX 文件描述符集成，因此你可以对常规文件使用 `cuFile`——如果网络文件系统支持 RDMA，也包括它们。

考虑一个 1 MB 的微小训练批次大小，并且想要每秒向 GPU 输送 1,000 个批次。这大约是 1,000 MB/s。用 CPU 做这个拷贝很容易消耗几个核心。有了 GDS，GPU 将直接从磁盘拉取这 1,000 MB/s，从而释放 CPU。在更高速率下——或有数千个 GPU 时——这变得更加明显。

由于训练工作负载绝大多数是读取密集型的，大多数 GDS 性能收益是在从存储读取数据时评估的。然而，拥有快速的检查点写入也很重要。为了实现 RDMA 加速的写入，文件系统必须支持 GDS 的 RDMA 写入。

**WekaFS** 是超大规模 AI 训练工作负载的知名存储提供商。他们提供了一个并行文件系统，附带 GDS 感知的插件，用于 RDMA 上的读写工作负载。

### 使用 cuda-checkpoint 对 GPU 状态进行检查点 (Checkpointing GPU State with cuda-checkpoint)

你可以使用 NVIDIA 的 `cuda-checkpoint` 工具结合 CPU 进程检查点工具（如 **CRIU**）在 Linux 上对 GPU 状态进行检查点操作。`cuda-checkpoint` 在运行的进程内挂起 CUDA，等待提交的工作完成，将设备内存复制到由驱动程序管理的主机分配中，并释放 GPU 资源。这样，CPU 端检查点工具就可以对进程进行快照。

挂起路径会锁定 CUDA 驱动程序入口点，排空未完成的工作，将设备内存复制到主机，并释放 GPU 资源。在估算挂起时间时，要考虑使用的设备内存量——以及挂起期间可用的主机链路带宽。

由于驱动程序在挂起阶段将设备内存复制到主机分配中，有效的挂起时间受限于内存镜像大小和你的平台互连。你应该在锁定和检查点调用周围使用 Nsight Systems 标记进行分析，以验证挂起阶段实际花费的时间。

当你希望进程恢复时，驱动程序重新获取 GPU，将设备内存映射回其原始地址，恢复 CUDA 对象（如流和上下文），然后解锁驱动程序和进程以允许 CUDA 调用继续。

具体来说，CUDA Driver API 暴露了 `cuCheckpointProcessLock`, `cuCheckpointProcessCheckpoint`, `cuCheckpointProcessRestore`, 和 `cuCheckpointProcessUnlock`。恢复需要启用持久化模式（或调用 `cuInit`），并且它可以重映射到相同芯片类型的不同物理 GPU。

值得注意的是，此路径与框架级模型检查点（例如 PyTorch checkpoints）是**正交**的。CUDA 检查点对于容错、抢占以及长期运行的训练和推理作业的迁移非常有用。

与使用 GDS 的数据摄取不同，检查点路径**不**直接从 GPU 内存 DMA 到存储。相反，设备内存镜像首先在挂起期间由驱动程序带入主机内存。然后 CRIU 将该进程内存持久化到检查点镜像。使用此功能来补充，而非替代你的框架的 `state-dict` 或分片检查点文件。

### 使用 gdsio 测量 GDS

NVIDIA 提供了一个名为 `gdsio` 的工具，默认安装在 `/usr/local/cuda/gds/tools` 下，用于基准测试磁盘和 GPU 之间的 GDS 吞吐量。这非常有用。

使用 GDS 时，看到吞吐量提高 10%–20% 或更多并不罕见——尤其是在受 CPU 限制的场景中。让我们看一个例子，使用 NVIDIA 的 `gdsio` 工具比较纯 CPU 中介读取（“之前”）与直接 GDS 读取（“之后”）。这里是 CLI 命令和吞吐量/延迟结果：

```bash
# Before (Storage -> CPU Memory only)
# CPU path, host memory, async copies (-x 2)
$ /usr/local/cuda/gds/tools/gdsio \
  -f /mnt/data/large_file \
  -d 0 -w 4 -s 10G -i 1M -I 0 -x 2

Total Throughput: 8.0 GB/s
Average Latency: 1.25 ms
```

这里显示的第一个调用使用带有锁页主机内存和异步拷贝的 CPU 路径 (`-x 2`) 在读取模式 (`-I 0`) 下收集基准。下面的第二个调用在相同配置下启用了 GDS 路径 (`-x 0`)。在比较路径时，请务必一致地使用传输选择器。对于 `gdsio`，`-x 2` 测量 CPU 中介传输，`-x 0` 测量 GDS 路径：

```bash
# After (Storage -> GPU Memory using GPUDirect Storage)
# - same config, GDS path (-x 0)
$ /usr/local/cuda/gds/tools/gdsio \
  -f /mnt/data/large_file \
  -d 0 -w 4 -s 10G -i 1M -I 0 -x 0

Total Throughput: 9.6 GB/s
Average Latency: 1.00 ms
```

我们看到，使用 GDS 创建从磁盘到 GPU 内存的直接数据路径，吞吐量增加了 20%，平均 I/O 延迟相应降低，如表 5-1 所示。它在做到这一点的同时释放了以前用于通过主机缓冲区移动数据的 CPU 周期。这个简单的基准测试展示了如何在你的系统中验证 GDS 的好处。

*(表 5-1: GDS 前后的吞吐量和延迟对比)*

| 路径                | 吞吐量          | 延迟           |
| :------------------ | :-------------- | :------------- |
| 存储 → CPU (无 GDS) | 8.0 GB/s        | 1.25 ms        |
| 存储 → GPU (有 GDS) | 9.6 GB/s (+20%) | 1.00 ms (-20%) |

在这个例子中，使用 GDS (存储 → GPU) 将读取吞吐量从 8.0 GB/s 增加到 9.6 GB/s，并将延迟从 1.25 ms 减少到 1.00 ms。这转化为吞吐量（更高）和延迟（更低）方面约 **20%** 的改进。

---

## DeepSeek 的 Fire-Flyer 文件系统 (3FS)

**DeepSeek** 从零开始创建了一个名为 **Fire-Flyer File System (3FS)** 的自定义开源文件系统。它的诞生源于他们观察到 AI 工作负载执行大量的随机读取。

这些随机读取使得传统的读取数据缓存对于 LLM 训练和推理工作负载变得无效——甚至适得其反。通过消除缓存并采用直接文件 I/O，3FS 确保每个请求都直接到达 NVMe SSD 设备，避免了浪费的缓存管理。这种方法类似于优先考虑直接存储访问的现代 HPC 文件系统。因此，3FS 最小化了内核页缓存的参与和读取期间的主机内存拷贝。

> 3FS 映照了专为 AI 协同设计存储的趋势。这类似于 NVIDIA 的 GDS，后者旨在与高性能并行文件系统配合使用，以实现类似的直接 GPU 吞吐量。

3FS 由四个关键组件组成：集群管理器、元数据服务、存储服务和客户端。这些组件通过像 InfiniBand 或 RoCE 这样的支持 RDMA 的结构互连，以最小化 CPU 参与和主机端拷贝。这些组件和连接如图 5-2 所示。

*(图 5-2: DeepSeek Fire-Flyer File System (3FS) 的组件)*

3FS 是基于 Linux 的文件系统，这允许与现有应用程序兼容，同时利用 RDMA 读取进行直接 GPU 可访问的数据传输。元数据在多个节点上分片和复制以实现横向扩展性能。数据路径完全绕过 OS 页缓存以保持最佳吞吐量。

> 如果文件系统是使用用户空间的 **FUSE** 实现的，它将无法提供 GDS 路径，因为 GDS 需要具有 `O_DIRECT` 语义的内核级文件系统集成。只有启用了 GDS 的内核客户端或专门集成的并行文件系统才能提供直接传输到 GPU 内存的能力。

为了将数据直接输送到 GPU 流水线，DeepSeek 在 3FS 中集成了基于 RDMA 的传输。如果你需要真正的 GDS 路径，请使用启用 GDS 的内核文件系统客户端，如 NVMe, NVMe-oF, BeeGFS, WekaFS, IBM Storage Scale, 或 VAST。这允许以最小的开销将异步、零拷贝数据直接移动到 GPU 设备内存中。

3FS 通过启用数据预取和传输与 GPU 内核并发运行，补充了本章关于重叠 I/O 与计算的技术。3FS 有效地将级联流水线/波浪概念（在第 4 章讨论）扩展到了存储层。

DeepSeek 已公开报告了 3FS 在大型集群上的多 TB/s 聚合读取吞吐量，在其环境中的结果高达 **7.3 TB/s**。在另一个基准测试中，一个大型 3FS 集群使用 68 节点 AI-HPC 集群（配备 10 × 16 TB NVMe SSD 和双 100 Gb/s）实现了 **6.6 TB/s** 的聚合读取吞吐量。这还是在同时服务额外 1.4 TB/s 的后台工作负载的情况下完成的。这一报告的 3FS 吞吐量 6.6 TB/s 远超类似硬件上 Ceph 的 ~1.1 TB/s。

3FS 通过协调跨节点的 I/O 实现此性能。这种水平的持续带宽有助于防止数据暂存阶段成为瓶颈——并有助于在训练和推理工作负载中保持高 GPU 利用率。

通过创建针对随机读取优化的自有文件系统并将其与 RDMA 优先的数据路径集成，DeepSeek 证明了端到端、全栈性能工程——包括存储设计——对于利用大规模 AI 系统的全部性能潜力至关重要。

3FS 展示了重新思考存储层如何消除最后的 I/O 瓶颈。构建自己的文件系统是一项高级技术，需要大量的前期投资和持续维护。相反，你更可能会从现有的分布式文件系统或对象存储开始。我们接下来讨论这些。

---

## 分布式、并行文件系统和对象存储 (Distributed, Parallel Filesystems and Object Stores)

在多节点上训练时，常见的设置是使用共享文件系统，如 NFS 服务器，或并行文件系统，如 **Lustre**, **GPFS**, **Ceph** 等。有了这些系统，所有节点都可以访问同一数据集。虽然方便，但如果配置不当，这些文件系统可能会成为瓶颈。

虽然设置简单，但如果有许多节点同时读取，单个 NFS 服务器很容易成为吞吐量瓶颈。如果你必须使用 NFS，确保服务器有多个快速 NIC。你还应考虑使用多个 NFS 服务器拆分数据集，以便每个服务器处理数据的一个分区。

对于多 GPU 集群，你应该只在规模适中（如少数几个节点）时考虑 NFS。对于更大的训练集群，单个 NFS 服务器——即使是高端实现——也很可能成为瓶颈。这就是为什么并行文件系统和像 **Amazon FSx for Lustre** 这样的云存储缓存是现代 AI 训练集群的首选。

NFS 也有调优参数，如 `rsize`/`wsize`（读/写请求大小）。建议使用最大值（例如 1 MB）以提高吞吐量。确保底层 NFS 存储足够快（使用 NVMe SSD——可能在 RAID 0 配置中）。别忘了检查 NFS 客户端挂载选项。它们也应该被调优。

例如，你可以使用 `rsize=1048576,wsize=1048576,noatime,async` 挂载 NFS 客户端，以使用 1 MiB 块并消除访问时间更新 (`noatime`)。你还可以添加 `actimeo=60,lookupcache=pos` 将文件属性和目录条目缓存 60 秒。这些简单的调整可以极大地减少每请求开销并提高大型共享数据集上的并行读取吞吐量。

像 **Amazon S3** 这样的对象存储不是典型的文件系统，但在 AI 工作负载中非常常见。如果在训练期间简单地访问对象存储可能会很慢。解决方案通常涉及将数据暂存在本地 NVMe SSD 存储上——或在对象存储之上使用缓存层（例如 S3 之上的 Amazon FSx for Lustre）。像 `s5cmd` 和 `aws s3 cp` 这样的工具可以让你在训练开始前下载数据。

> 确保使用高度并行、优化的数据传输工具，如 AWS S3 C++ SDK 和多线程实用程序（如 `s5cmd`）以获得最佳性能。

你也可以使用支持范围请求 (Range Requests) 并执行缓存的流式库从 Amazon S3 读取对象。如果直接从 Amazon S3 读取，使用尽可能大的请求——并使用多线程范围 Get 操作。

像 Lustre 和 GPFS 这样的并行文件系统专为高并发和吞吐量设计。例如，Lustre 设置有多个**对象存储目标 (OSTs)** 来服务数据。通过跨 OST 条带化 (Striping) 文件，你可以成倍增加吞吐量。如果你有这样的并行文件系统，确保你的大数据文件跨许多 OST 条带化。

条带化意味着文件的块存在于不同的服务器上。这允许并行读取。例如，你可能将 Arrow, TFRecord, 或 Parquet 文件跨 4 个 OST 条带化。如果每个 OST 提供 500 MB/s，你可以实现 2 GB/s 的理论峰值读取吞吐量。

### 调优、复制和压缩数据

要调优这些文件系统，请务必查阅文档。例如，`lfs setstripe` 用于 Lustre 上设置大数据集跨 4 或 8 个 OST 的条带化，以聚合 OST 带宽。

在训练期间监控文件系统的 I/O，使用像 `lmt` (Lustre) 这样的工具——或供应商特定的监控工具。你要观察存储集群中的个别节点是否过热。如果是，你需要确定原因。原因很可能是分片问题，即更多的读/写落在了较少数量的节点上。

为了完全消除网络读取，在某些情况下，你可以选择将数据集**复制 (Replicate)** 到计算集群中的每个节点上。这假设每个节点上有足够的存储空间。这是一个公认的暴力但相对常见且非常有效的解决方案，可以完全消除网络读取。你会看到立竿见影的性能提升——代价是额外的存储空间。

另一个提高性能的选项是将数据**压缩**存储在文件系统或对象存储上——并即时解压。例子包括图像 (JPEG) 和压缩文本 (Arrow 和 Parquet)。这可以节省 I/O 带宽，代价是一些额外的 CPU 或 GPU 周期。然而，如果 I/O 是瓶颈而 CPU 和 GPU 空闲，这是一个合理的权衡。

许多数据流水线已经在这样做了，所以你只需要验证压缩比并确保你在流水线的每一步都使用了它。关键是找到平衡点，使解压步骤不会成为瓶颈。

像 **nvJPEG** 这样的库可以在 GPU 上解码图像。现代 GPU 增加了片上**解压引擎 (Decompression Engine)**，支持 LZ4, Snappy, 和 Deflate 等格式，以加速移动和解包数据到 GPU 内存。如果你将压缩批次存储在磁盘上，Blackwell GPU 可以使用解压引擎在流水线中解压它们。这释放了 SM 以运行更高价值的任务，如计算内核。你应该在 I/O 受限的工作负载中优先考虑这些压缩格式。

这是另一种将算术操作从 CPU 卸载到 GPU 的方法——并且可能在训练期间将数据解码与梯度计算重叠。由于高带宽的 CPU-GPU NVLink-C2C 互连（高达 900 GB/s 双向带宽），你可以防止 CPU 辅助阶段成为瓶颈。

利用这些巧妙的软件和硬件 GPU 卸载特性可以进一步将工作负载从 CPU 转移出去，保持输入数据流水线平衡。关键还是要确保解压时间不会取代 I/O 成为瓶颈，否则额外的压缩计算可能就不值得了。

---

## 监控存储 I/O (Monitoring Storage I/O)

与任何性能工程任务一样，测量是关键。类似于监控网络通信，使用所有可用工具来监控存储流水线通信非常重要。

这些工具包括 Linux `iostat`, `iotop`, `nvme-cli`, `perf`, 和 `eBPF`。此外，你可以使用供应商特定的实用程序和仪表板来监控队列、延迟、预读效果和缓存命中率。这将有助于显示本地 NVMe 设备使用情况，并确定在从 NAS 或对象存储读取数据时是否使网络链路饱和。

还要考虑使用 **Nsight Systems** 来追踪 I/O 等待时间并可视化与 GPU 内核的重叠。使用 Nsight Systems 选项 `--trace=gds`。这将捕获 `cuFile` API 活动并在时间轴上追踪。你还可以使用 `/etc/cufile.json` 启用 GDS `cuFile` 静态跟踪点，以便在 Nsight Systems 中查看 `cuFile` 事件。

> NVMe 点对点 DMA 路径的内核模式计数器未在 Nsight Systems 中暴露，并且可能并非对所有 GDS 栈可用。

另一个工具是 **NVIDIA Data Center GPU Manager (DCGM)**，它报告有用的 GPU I/O 统计信息。总之，这些 GPU 特定工具补充了主机 OS 工具，并提供了关于因 I/O 导致的 GPU 饥饿的更完整图景。

在 PyTorch 中，调用 `next(data_iterator)` 测量 GPU 等待下一批次空闲的总时间。这个时间包括任何后台预取和主机 → 设备拷贝——而不仅仅是 Python 数据加载逻辑。

如果你想隔离纯数据加载成本，可以临时设置 `num_workers=0` 以取消预取。然后你可以只对迭代器拉取进行计时。你可以单独将 `.to("cuda")` 或锁页内存暂存包装在它自己的计时器中（或使用 CUDA 事件）以捕获主机 → 设备拷贝开销。

因为你的瓶颈可能在 Python 流水线中，或者在进入 GPU 内存的 `memcpy` 中，你可以通过执行以下操作并将时间与整体“GPU 空闲”时间进行比较来区分它们：

**DataLoader vs Python 成本**
> 使用 `num_workers=0` 进行分析，看看 Python 循环和转换本身花费多长时间。这消除了任何后台线程调度。

**主机 → 设备拷贝成本**
> 通过检查 Nsight Systems 中的“Copy”通道来仅测量设备传输时间，以量化将数据暂存到 GPU 缓冲区实际上让 GPU 停顿了多久。你也可以在 `.to("cuda")` 调用周围包装 `torch.cuda.Event`。

通过将这两个计时与你的整体“GPU 空闲”时间进行比较，你将知道是加速你的 Python 流水线（例如，增加 worker，简化转换），还是优化 H2D 传输路径（例如，使用锁页内存，增加互连带宽，或切换到 GDS）。

通过监控存储流水线，你可能会发现，例如，你的 GPU 花费 30% 的时间等待数据。在这种情况下，GPU 的整体吞吐量受限于 I/O，因此你将希望实施这里提到的一些策略来减少 I/O 停顿并增加计算吞吐量。调优后，也许你的 GPU 只等待 5% 的时间。你也会看到整体每秒训练步数成比例增加——在本例中是 6 倍（注：此处原文计算可能有误，如果等待时间从30%降至5%，吞吐量提升应为 $1/0.7 \to 1/0.95$，约35%提升，原文可能有特定上下文或笔误）。

存储和 I/O 优化通常关于消除累积起来的小低效；例如，这里 5 毫秒延迟，那里 10 MB 过小的缓冲区。但在规模上，修复这些低效会带来巨大差异。底线与前面的章节类似：**保持流水线满载**。在这种情况下，不仅仅是计算流水线，还有数据流水线。从磁盘到 GPU 内存的每个组件都应受到监控、分析和改进，以确保数据尽可能连续地流入这些 GPU。

---

## 调优数据流水线 (Tuning the Data Pipeline)

除了原始存储 I/O，CPU（或 GPU）上的预处理和数据加载流水线是整体 AI 工作负载性能的关键部分。调优良好的数据流水线确保 GPU 永远不会因等待新数据而闲置。此外，重要的是适量的 CPU 工作正在并行完成以喂饱 GPU 猛兽。

现代深度学习框架提供了高级 API 来加载和预处理数据。这些可以——也应该——针对性能进行调优。我们将讨论通用策略以及 NVIDIA 的工具如 **DALI** 和 **NeMo** 用于高级数据流水线管理。

### 高效的数据加载与预处理

训练中的典型数据加载过程涉及从存储读取数据、解码或反序列化数据（如解析 JSON 和解码 JPEG）、应用一些转换（如 Token 化文本和裁剪图像），以及将数据整理成批次。这些步骤可能是 CPU 密集型的，但如果它们是计算密集型的，也可以卸载到 GPU。为了维持高吞吐量，你可以采用以下几种技术：

**使用多个 Worker 进程/线程**
> 如前所述，像 PyTorch DataLoader 这样的框架允许你指定 `num_workers`。每个 worker 并行运行以获取和预处理数据。通常这些是独立的进程以避免 Python GIL 问题。主进程使用队列异步地从 worker 进程获取批次。

**避免 Python 瓶颈**
> 如果你的数据加载逻辑在 Python 中，要警惕繁重的 Python 级处理。如果你看到纯 Python 代码被用于在循环中 Token 化单行文本，这是一个危险信号。在这些情况下，如果可能，改为向量化操作。或者使用 C++/C 绑定以提高性能。许多库都存在用于此类常见任务，包括 Hugging Face Tokenizers 库和 TorchText。虽然它们有 Python 绑定，但在底层是用快速的 Rust/C++ 编写的。此时，Python 只是 C/C++ 代码之上的易用接口。

**重叠 CPU-GPU**
> 想法是将数据准备与 GPU 处理重叠。在完美场景中，当 GPU 正在处理批次 N 时，CPU 已经加载并预处理了批次 N+1 并使其在锁页内存中可用。当 GPU 完成批次 N 时，它只需 DMA 拷贝批次 N+1 并立即开始计算。与此同时，CPU 继续处理批次 N+2。这种流水线化对于性能至关重要。大多数框架在使用多个 worker 时默认这样做，但你应该监控以确保它正在发生。如果没有，你可能会看到 GPU 在每次迭代开始时闲置等待更多数据。

**通过整理张量 (Collating Tensors) 批量执行操作**
> 如果可能，你希望加载器按批次执行操作而不是按样本。例如，你想使用向量化操作一次性对整批张量应用转换。你可以通过自定义 `collate_fn()` 整理批次来做到这一点——或者可能在 GPU 上的训练循环本身中进行。这比单独对每行输入数据执行这些操作要好得多。然而，某些转换需要按样本执行，因此你需要理解工作负载才能有效地批处理和整理。

**在数据加载中使用内存锁定 (Memory Pinning)**
> 在 PyTorch DataLoader 中启用 `pin_memory=True` 使主机 → 设备 (H2D) 传输更快，并允许真正的异步 `.to(..., non_blocking=True)` 拷贝，当源被锁定时。从锁页内存 DMA 避免了额外的拷贝和缺页中断，因为数据被锁定在 RAM 中并准备好直接传输。这在向 GPU 传输数据时几乎总是有益的。确保设置高 `ulimit -l`（或容器 `--ulimit memlock`）以避免大型锁页缓冲区分配失败。

**预取批次 (Prefetch batches)**
> 一些框架允许你指定预取队列长度。这是它应该提前加载多少批次。默认情况下，PyTorch 的 DataLoader 使用保守值，如 `prefetch_factor=2`。在这种情况下，PyTorch 每个 worker 预取两个批次。在底层，它保持多达 `num_workers * prefetch_factor` 个批次排队。因此，在阻塞之前，每个 worker 加载两批数据。如果你的工作负载有突发 I/O——或者你看到 worker 偶尔让 GPU 挨饿——你可以将 `prefetch_factor` 增加到 4 或 8。

*(此处省略 PyTorch 代码示例，演示了 pin_memory=True 和 prefetch_factor=4 的使用)*

在这个例子中，8 个 worker 进程中的每一个都预加载大小为 4 的批次到锁页内存中。因为主机内存被锁定了，异步 `.to(device, non_blocking=True)` 传输可以使用 DMA 进行高速数据拷贝。

因此，当 GPU 处理当前批次（批次 N）时，DataLoader 已经在并行准备和传输下一个批次（批次 N+1）。这种重叠至关重要。如果没有锁页内存，系统需要为每次传输即时锁定内存，这会引入不必要的延迟。本质上，锁页内存确保从 CPU 到 GPU 的数据传输更迅速地发生，并与 GPU 计算并发，最大化整体吞吐量。

另一个选项是启用 `persistent_workers=True`，这样 worker 保持存活并在 epoch 之间持续填充队列。当你多次循环同一个数据集时——特别是如果这些迭代（即 epochs）非常短时，这最有效。持久化 worker 还有助于当 worker 启动因导入模块、打开文件等产生显著开销时。有了持久化 worker，你避免了在每个 epoch 边界生成和拆除进程的成本。你的 worker 保持存活，因此它们可以以最小的开销立即开始为下一个 epoch 预取。

一个常见的陷阱是在你的流水线中引入隐蔽的瓶颈。这很容易通过添加调试日志或昂贵的 CPU 转换做到。延迟可能只在负载下显示出来。为了捕捉这些，首先**隔离分析 DataLoader**，通过计时它在禁用所有下游 GPU 工作的情况下生成 100 个批次需要多长时间。一旦你测量了那个基线，将其与你的目标迭代时间和正常训练期间测量的总 GPU 空闲时间进行比较。

如果 DataLoader 单独就很慢，优化你的 Python 流水线，移除逐元素日志记录，简化转换，或添加更多 worker。如果隔离加载器速度与实际运行加载器速度之间的差距很大，你可能受限于主机 → 设备传输或内核启动开销。

> 如果你禁用 GPU 内核以隔离 DataLoader 进行分析，你也减少了 CPU 侧的内核启动开销。因此，你的“纯”数据加载吞吐量通常会显得比你在真实训练运行中看到的要低。这仍然是一个有用的技术；只要记住这一点。

### 随着 GPU 数量扩展而扩展 Worker

当你添加更多 GPU 时，你也应该扩展你的数据流水线，否则你会让设备挨饿。在实践中，这意味着增加 DataLoader 的 worker 数量或 I/O 带宽，以便你可以喂饱每个 GPU。这是提高总批次大小所必需的，以便每次迭代在更多设备上移动更多样本。

不扩展摄取流水线资源而仅扩展计算，会将瓶颈进一步推向数据加载流水线。在多节点、数据并行配置中，每个 rank 读取不同的分片。合计起来，总数据加载工作量随集群规模扩展。

> **始终测量 CPU 使用率**，因为随着 GPU 训练加速，数据输入流水线将成为瓶颈。

为了维持必要的吞吐量，你将需要并行的、高带宽的和分布式的存储后端（如前所述），以支持跨集群中许多节点的超大规模数据分片。回想我们之前关于每节点分片数据集的讨论。具体来说，当你添加更多节点时，确保每个节点的本地存储可以处理其数据份额。

### 使用 NVIDIA DALI 进行多模态数据处理

对于复杂或繁重的数据预处理，NVIDIA 提供了 **Data Loading Library (DALI)**。DALI 通过将数据处理移动到 GPU 或使用优化的 C++ 编写的 CPU 代码来加速数据处理。它对于图像和视频数据特别有用，其中解码和增强可以从 GPU 加速中受益。

例如，DALI 可以在 GPU 上解码 JPEG 图像，并应用随机裁剪、调整大小和归一化等增强，全部在 GPU 上进行。这通常比在 CPU 上快——假设 GPU 有可用周期。这从 CPU 卸载了处理并减少了所需的 CPU worker 数量。

DALI 流水线以声明方式定义为操作符的静态图。你子类化 `nvidia.dali.pipeline.Pipeline` 并在 `define_graph()` 中声明你的数据源和 CPU/GPU 操作。DALI 随后使用其自己的线程池和队列内部处理执行、预取和线程。

如果你的工作负载是输入受限的（例如，模型训练），集成 DALI 可能会显著提高吞吐量。然而，必须将其集成到训练循环本身中，这增加了一些复杂性并且有一点学习曲线。

对于许多常见工作负载如分类、目标检测和分割，NVIDIA DALI 提供了预构建的流水线，可在 GPU 上解码图像和视频。这充分利用了 GPU 的媒体加速硬件。

考虑一个读取图像和视频、执行增强并训练目标检测模型的数据流水线。你可能会观察到 CPU 使用率达到 800%，即 8 个核心以 100% 利用率运行。但 GPU 仍然偶尔停顿。

通过使用 DALI，你可能会将 CPU 使用率降至 200%，即仅用两个核心执行文件读取，而 GPU 进行实际的图像和视频解码。并且 GPU 可以与计算并发执行读取。

在实践中，真正的加速完全取决于你在流程的何处放置 DALI。如果你仅使用 DALI 解压 JPEG，然后立即将原始像素交回给 CPU 进行增强和整理，你将招致额外的主机-设备-主机拷贝，这可能会抵消使用 DALI 的性能增益。

> 对 DALI 更好的方法可能是识别 GPU 友好的预处理操作，并将它们直接融合到基于 GPU 的预处理计算图中。大多数预处理可以使用现有的基于 CUDA 的库（如 TorchVision 和 TensorRT）——或使用自定义 CUDA 内核完成。这样，你避免了在 CPU 和 GPU 之间过度移动数据。这可能产生比在流水线中使用 DALI 更高的端到端性能，因此值得探索。

一如既往，在现实条件下对端到端系统进行基准测试。比较纯 CPU 流水线、启用 DALI 的流水线和完全融合的 GPU 图实现，以确定哪种方案为你的模型和数据集提供了 CPU 节省和 GPU 利用率的最佳平衡。

### 使用 NVIDIA NeMo Curator 创建高质量 LLM 数据集

**NVIDIA NeMo** 是一个用于开发和训练语言模型的工具包。NeMo 工具包中包含了 NeMo 库和框架套件，其中包括开源的 **NeMo Curator** 框架。

**Curator** 帮助为 LLM 训练准备大型多模态数据集。当处理来自不同来源的 TB 级数据时，它很有帮助。Curator 支持数据处理步骤，如清洗、Token 化和洗牌 (Shuffling)。

NeMo Curator 可以跨多个 GPU 或节点分发数据集预处理。这就利用了多个加速器来更快地准备数据——这是组装多 TB 训练数据集时的重要考虑因素。

此外，Curator 可以压缩并将数据打包成少量的大文件——或将数据转换为二进制格式以便机器更容易消费。它还可以创建新的、合成的训练数据集来增强人类数据集，后者相对有限且变得越来越稀缺。

有了 Curator 离线并在训练过程之前完成繁重的预处理，在线训练数据流水线变得简单得多，因为它只是读取准备好的数据，或许再进行一些轻量级的、“最后一英里”的洗牌。

NeMo Curator 还可以通过去重数据和移除有问题的内容来强制执行数据质量过滤。这对于 LLM 训练质量和性能都很重要。预先拥有结构良好、预处理和清洗过的数据集意味着训练流水线有一致的结构良好且大小均匀的数据流（例如，填充到固定长度），不必即时 Token 化文本，并且可以避免棘手的字符串处理。

如果你可以使用像 NeMo Curator 这样的工具，明智的做法是利用它们，以便你的训练作业主要是 GPU 前向和后向传播——而不是处理文本和读取数百万个随机大小的小文件。对于基于 NeMo 的训练，预处理的数据集通常存储为内存可映射的 `.bin` 数据文件和 `.idx` 索引文件。NeMo Curator 的 `DocumentDataset` 随后读取和写入分片的 JSONL 或 Parquet。当你构建索引数据集时，会处理下游转换为 `.bin/.idx` 的工作。

> 你可能想考虑存储数据的 N 个副本，以 N 种不同方式洗牌，以避免在 N 个训练 epoch 中的运行时洗牌成本。明显的权衡是磁盘空间和内存，但这值得考虑。

一般来说，**在训练前准备好你的数据**。你几乎不应该用原始文本进行训练。离线预处理数据可能需要一些时间，但在长远来看，更快的训练运行、更快的迭代和更可预测的扩展会带来回报。

这里描述的所有技术旨在**永远不要让数据加载流水线导致昂贵的 GPU 集群闲置**。如果数据流水线无法足够快地提供输入，性能最高的 GPU 也是无用的。因此，需要在所有层级（包括存储、网络、CPU 和 GPU）采取整体的、全栈的优化方法。

---

## 持续分析与调优工作流 (Continuous Profiling and Tuning Workflow)

性能工程是一个迭代过程。为了确保你的分布式训练或推理应用在扩展或修改时保持高效，你应该采用**持续分析和调优工作流**。这意味着定期收集性能数据，识别瓶颈，应用优化，然后再次测量。

随着时间的推移，硬件和软件更新会改变最佳设置，因此你需要持续分析和调优。为了保持领先，注重性能的工程团队通常维护性能仪表板，以跟踪随时间变化的指标，如 samples/sec。

考虑设置自动化的夜间运行，分析你的训练和推理工作负载。这样，你可以捕捉回归或改进，并将其追溯到代码更改。

让我们看一个典型的最佳实践工作流，它可以广泛应用于所有分析和调试情况——不仅仅是本章描述的主题：

**建立基线**
> 从单个 GPU 或最小设置开始，测量性能，如以 samples/sec 衡量的训练吞吐量或以毫秒（希望如此！）衡量的推理延迟。然后扩展到单个节点上的多个 GPU，然后是多个节点——每次都使用像整体吞吐量这样的高级指标分析性能如何扩展。理想情况下，对于简单的数据并行工作负载，N 个 GPU 给出 N 倍的吞吐量。如果你看到的远小于此，这是一个信号，表明你的系统产生了太多开销。下一步是量化瓶颈。例如，8 个 GPU 只带来 5 倍的吞吐量增加，揭示了系统只有 62.5% 的效率。这并不理想。

**分析多 GPU 运行以寻找瓶颈**
> 为了诊断瓶颈的确切原因，我们深入挖掘并在多 GPU 作业整体上使用像 **Nsight Systems (nsys)** 这样的系统分析工具，以获得时间花费的概览。第一步是查看 GPU 利用率时间线。GPU 是否频繁停顿？如果是，它们在等什么？还要检查 CPU 时间线。主进程是否落后于其他 worker 进程？是否有每个线程都在等待的同步点？例如，如果在模型训练的梯度 All-Reduce 期间 GPU 空闲，你知道通信是瓶颈。如果它们在每次迭代开始时空闲，也许数据加载或特定内核是瓶颈。

**如果需要，深入特定内核**
> 如果你发现某个 GPU 操作（网络或计算）运行得比预期慢，你可以使用 **Nsight Compute (ncu)** 深入该内核检查其效率。例如，在我们之前的讨论中，我们查看了一个 NCCL 内核，当通信通过 PCIe 而不是 NVLink 传输时，看到了仅 60% 的 SM 利用率和高内存停顿计数。优化通信以使用 NVLink 后，SM 利用率升至 90% 忙碌——并测得更少的内存停顿。这种深度挖掘可以确认内核是受限于网络带宽、内存带宽还是计算。它会是其中之一。

**识别原因**
> 一旦你发现瓶颈，将其映射到一组假设原因并逐一验证（或证伪）。例如，如果瓶颈是网络受限，也许你没有使用 RDMA，或者你的消息大小太小，或者你需要更好的重叠等。如果 GPU 空闲等待其他 GPU，你可能有**掉队者 (Straggler)** 情况，其中一个 GPU 由于数据不平衡而比其他 GPU 做更多工作。如果你的工作负载受限于 CPU，也许数据加载器配置不正确，或者有一个更适合 GPU 的 CPU 聚合操作等。如果瓶颈受限于 GPU 内存，也许某些内核在寄存器和 HBM 内存之间传输太多数据，所以你可以尝试减少批次大小或引入内核融合等。

**应用修复或优化**
> 在过一遍你的假设并找到实际原因后，是时候采取行动了。对于网络和通信瓶颈修复，确保启用了 GPUDirect RDMA，如果你有多个 NIC 且仍受网络带宽限制，增加 `NCCL_NSOCKS_PERTHREAD`，并考虑使用梯度压缩等技术压缩数据（我们将在后面的章节中介绍）。如果你正在跨越 NUMA 节点，尝试分层方法或配置 NCCL 拓扑以使用更少的每 NUMA 节点 GPU 等。
> 对于节点内拓扑问题，如果你的 GPU 分布在 PCIe 交换机上，尝试尽可能将你的作业绑定到单个 NUMA 节点的 GPU 以避免慢速互连——或使用更多拓扑感知的算法。对于 CPU 和数据问题，检查数据加载器是否太慢。在这种情况下，添加更多 worker 进程/线程或使用 DALI 将一些预处理移动到 GPU。或提前做更多离线预处理。如果一个 GPU 较慢，也许在做额外的验证或日志记录，尝试减少工作或使用异步操作将其移出关键路径。
> 如果同步是问题，移除代码中可能无意中序列化执行的不必要的 `torch.cuda.synchronize()` 调用或屏障（第 13 章将详细介绍）。如果环境需要调优，也许需要设置 `NCCL_IGNORE_CPU_AFFINITY=1`。或将 CPU 线程绑定到不同的拓扑配置等。只需相对较少的努力，有时只需几个小改动就能将非常差的资源利用率转变为最大利用率（当然知易行难，但保持乐观是好的！）。

**每次更改后重新测量**
> 与任何调试工作一样，重要的是一次只更改一两件事——然后测量。否则，你不知道哪个更改起了作用。当你在当前配置上实现良好的扩展时，记录好的指标值，并致力于在扩展时保持这些好的值。

**保持软件更新，但始终验证**
> 新版本的 NCCL 或 CUDA 通常带来可以提升性能的改进。例如，较新的 NCCL 可能会自动执行分层操作或使用通信/计算重叠机制。或者 PyTorch 更新可以减少 DDP 开销或引入更高效的分布式优化器。然而，每次更新也可能通过改变最佳设置带来不稳定性。再次运行你的分析工作流，并确保你维持了上一个已知良好系统配置中的那些好的指标值。

**利用现代硬件特性**
> 假设你突然获得了拥有更多统一内存、更多内存带宽和更快互连的最新硬件。第一步是理解新的改进并利用它们。你现在可以使用更大的输入数据批次大小并将更大的模型放入内存。只需确保缓慢提升并监控资源利用率。如果你扩展得太激进，你会使新的改进资源饱和——并不得不重新开始分析和调优工作流！

**自动化生产监控**
> 如果你定期运行大型训练和推理工作负载，最好在生产中拥有一致的监控设置，以随时间持续分析 GPU 利用率、网络吞吐量和内存吞吐量。这样，如果作业或推理请求由于环境问题、内核更新或数据流水线退化而运行得比预期慢，你可以迅速捕捉到。Kubernetes 和其他作业调度器与监控工具集成良好。设置警报，例如如果利用率低于某个阈值。

**记录和教育**
> 性能调优通常涉及团队之间的隐性知识，比如哪些环境变量被覆盖，哪些库版本有 bug 等。将这些发现直接记录在代码或配置文件中，以便其他人或未来的你每次打开这些文件时都能被提醒。例如，注释“在这个集群配置中，我们发现设置 `NCCL_SOCKET_NTHREADS=2` 将多节点吞吐量提高了 10%。” 希望这是一个标准做法。

通过持续遵循此工作流，你本质上创建了一个反馈循环：**运行 → 测量 → 调优 → 运行 → 测量 → 调优 → …** 这种反馈循环确保当你扩展到更多 GPU 并转向更好的模型时，你继续维持系统的性能和效率。维持良好的性能比在性能随时间退化后重新获得它要容易得多。保持这么多移动部件在最高水平运行是一场持续的战斗。

简而言之，将性能视为一个需要持续测试和验证的特性。就像你为代码正确性编写测试一样，你应该为性能编写测试。例如，GPU 加倍是否使吞吐量大致加倍？如果不是，请使用分析器深入研究并启动分析和调优工作流。建议结合使用 Nsight Systems 获取高级系统视图，Nsight Compute 获取低级 GPU 内核分析，以及 NCCL 和 PyTorch 的日志记录。总之，这些将为你提供一个综合工具包，以便在问题出现时查明原因。

在这个过程结束时，你将拥有一个微调过的 AI 系统。当你更新代码或硬件时，再次迭代。在像 AI 这样动态、快速移动的环境中，性能调优永远不会完成。但当你只要知道要找什么时，它会变得更容易。这正是你阅读本书的原因！

### 诊断通信密集型与计算密集型工作负载

要理解计算或通信是否是模型训练工作负载中的限制因素，你可以改变计算与通信的比率，并查看这如何影响 NIC 上测得的已实现网络吞吐量（GB/s）。

考虑分析一个训练作业的反向传播。它当前显示你的梯度 All-Reduce 在 100 GB/s NIC 上仅利用了 60 GB/s。

为了弄清楚在这种情况下是网络堵塞还是 GPU 太慢，你可以固定通信量并增加/减少计算量，方法是**增加/减少批次大小**。这是完美的，因为增加批次大小不会影响反向传播梯度 All-Reduce 期间传输的数据量。这是因为梯度的数量随模型参数的数量扩展——而不是批次大小。

在通信量固定的情况下，将批次大小减半，看看这对已实现的网络吞吐量有何影响。如果它保持在 60 GB/s，那么 GPU 本可以做更多工作，但网络不让它们做更多工作。因此，**网络是限制因素**。

然而，如果已实现的网络利用率从 60 GB/s 下降到（比如）40 GB/s，说明 GPU 通过没有足够快地完成计算来让网络挨饿，导致无法保持 NIC 忙碌。在这种情况下，网络是空闲的，等待来自 GPU 的更多数据。因此，**计算是限制因素**，而不是网络。

你可以通过反转实验并加倍批次大小来进一步验证此假设。同样，All-Reduce 梯度通信量保持不变。所以如果通信是真正的限制器，你会看到随着批次大小和计算工作负载的增加，NIC 保持在 60 GB/s。但如果是计算限制，花在 All-Reduce 通信上的总迭代时间百分比将相对于不断增长的计算时间而**缩小**。

观察这两个实验中 NIC 上的绝对 GB/s 以及通信相对于计算所花费的相对时间，将精确定位哪个子系统需要调优。更具体地说，你可以绘制 GB/s 对比通信百分比随批次大小增加/减少的变化图。这将向你确切展示上限在哪里，并确定工作负载是**通信受限 (网络)** 还是 **计算受限 (GPU SMs)**。

> 使用 Nsight Systems 获取端到端时间线。如果你看到 GPU 空闲，以计算内核之间的长间隙形式等待数据（对应于 NCCL 等待），那么你很可能是通信受限。如果 GPU 很忙但没有达到预期的 FLOPS，你可能是内存受限或计算受限。Nsight Compute 和 PyTorch Profiler 可以帮助确定内核的内存和计算效率。

---

## 关键要点 (Key Takeaways)

分布式 AI 的峰值性能来自于跨全栈的协同优化：从 GPU 内核和网络传输到 CPU 线程和存储。任何一个环节的弱点都可能成为整个系统的瓶颈。以下是调优存储层时需要记住的关键教训：

**随计算扩展同步扩展输入数据流水线**
> 在扩展 GPU 时不要忽视存储和数据加载。确保你的存储系统提供足够的带宽——并且你正在充分利用这带宽。随着 GPU 数量的增加，相应地增加数据加载器的并行度。否则，你会遇到一个点：添加 GPU 不会带来加速，因为你的输入流水线跟不上。

**工欲善其事，必先利其器**
> NCCL 专为模型训练中常用的可扩展集合 (All-Reduce 等) 通信而设计。NIXL 针对模型推理中常见的高吞吐量点对点和流式传输。在 Token 流式传输主导工作负载的地方使用 NIXL。相比之下，对于批量集合和对称内存模式，首选 NCCL/NVSHMEM。GPUDirect RDMA 和 GDS 分别移除了网络和存储 I/O 的主机内存反弹缓冲区，而 CPU 仍负责调度和控制传输。对于多 GPU 训练，**始终使用 DistributedDataParallel 而非 DataParallel**。这些专用库和框架的存在——并且经过了大量调优——是为了从硬件中榨取性能。利用它们，而不是重新发明轮子。

**端到端分析**
> 瓶颈并不总是显而易见的。使用 Nsight Systems, Nsight Compute, 和 PyTorch Profiler 等分析器查看时间花在哪里。GPU 要么是计算受限、通信受限，要么是 I/O 受限。遵循我们之前讨论的分析示例可以帮助你找到问题的根源。例如，你可以在 Nsight Systems 中验证 NCCL 内核是否与计算正确交织，并检查 GPU 空闲时间。

---

## 结论 (Conclusion)

高性能、分布式存储系统是调优大型复杂 AI 系统的基础组件。通过集成像 NVMe SSD 和 GDS 这样的先进存储技术，你可以提高数据加载流水线性能，减少训练时间，并提高实验和迭代的速率。

通过离线预处理数据流水线、高效数据缓存和异步通信等技术解决存储和 I/O 挑战，使得现代 AI 部署即使在模型复杂度和数据集规模扩大的情况下也能维持高吞吐量。

对于从业者来说，要点是你不需要从头发明自定义 I/O 解决方案。NVIDIA 和开源社区提供了高度调优的、目的构建的库和工具，以便你可以专注于你的模型、数据和应用逻辑，而不是底层管道。

对于性能工程师来说，教训是**快速的数据移动与原始计算能力一样关键**。如果世界上最快的 GPU 总是不断地等待来自存储的数据，它提供的好处微乎其微。

像 GDS 和高级输入流水线这样的技术是全栈方法的一部分，旨在保持数据顺畅流动并为 GPU 提供工作。通过利用这些技术并持续分析和调优，你可以在规模上将分布式 AI 系统推向其理论峰值极限。

在接下来的章节中，我们将在此基础上深入探讨 CUDA 和 PyTorch 优化策略以及一些高级系统调优主题。这里学到的原则将继续应用于堆栈的每一层，因为我们将继续重叠通信/计算，利用尽可能快的链路，并接近理论最大硬件性能。最终，这一切都将导致更快的洞察时间、更好的资源利用率和成本节约。





**译境** 已就位。

第六章深入探讨了 GPU 的心脏——架构与编程模型。本章不仅复习了 CUDA 的基础知识，更重要的是结合了最新的 **Blackwell** 架构特性（如 TMEM 和 TMA），阐述了如何通过**占用率 (Occupancy)** 和**屋顶线模型 (Roofline Model)** 来压榨硬件的极限性能。

译文将保持技术概念的严谨性，同时致力于传达作者对“延迟隐藏”这一核心理念的深刻理解。

以下是 **第六章：GPU 架构、CUDA 编程与最大化占用率** 的中文译本。

---

# 第六章：GPU 架构、CUDA 编程与最大化占用率

在本章中，我们将首先回顾**单指令多线程 (SIMT)** 执行模型，以及 Warp (线程束)、线程块 (Thread Blocks) 和网格 (Grids) 如何将你的 GPU 算法映射到**流式多处理器 (SM)** 上。

我们将回顾现代 NVIDIA GPU 上的 SIMT 执行模型，包括 Warp、线程块和网格如何映射到 SM。随后，我们将深入探讨 CUDA 编程模式，讨论片上内存层级结构（寄存器文件、共享内存/L1、L2、HBM3e），并演示 GPU 的异步数据传输能力，包括**张量内存加速器 (TMA)** 和作为 Tensor Core 累加器的**张量内存 (TMEM)**。

我们还将引入**屋顶线分析 (Roofline Analysis)** 来识别内核是**计算受限 (Compute-bound)** 还是**内存受限 (Memory-bound)**。这将为推动现代 GPU 系统接近其理论峰值吞吐量上限提供基础。

---

## 理解 GPU 架构 (Understanding GPU Architecture)

与针对低延迟单线程性能进行优化的 CPU 不同，GPU 是针对吞吐量优化的处理器，专为并行运行数千个线程而构建。图 6-1 展示了 CPU 和 GPU 之间简单的 CUDA 编程流程。

*(图 6-1: 简单的 CUDA 编程流程)*

最初，主机将数据加载到 CPU 内存中。然后，它将数据从 CPU 内存拷贝到 GPU 内存。在调用 GPU 内核处理 GPU 内存中的数据后，CPU 将结果从 GPU 内存拷贝回 CPU 内存。现在，结果回到了 CPU 上以供进一步处理。

GPU 依靠大规模并行性来隐藏数据传输延迟，例如图 6-1 中描述的 CPU-GPU 数据传输。每个 GPU 包含许多 **SM (流式多处理器)**，它们大致类似于 CPU 核心，但针对并行性进行了精简。在 Blackwell 上，每个 SM 可以并发跟踪多达 64 个 Warp（32 线程组）。

现代 GPU 上的每个 SM 可以并发跟踪多达 64 个 Warp（即 2,048 个线程）。Blackwell GPU 的每个 SM 配备了 64K 个 32 位寄存器（总计 256 KB）以及每个 SM 256 KB 的组合 L1 缓存/共享内存。其中高达 228 KB（227 KB 可用）的 SRAM 可以配置为每 SM 的用户管理**共享内存 (Shared Memory)**。任何单个线程块都可以请求高达 227 KB 的动态共享内存（228 KB 中的 1 KB 由 CUDA 保留）。这些特性帮助 SM 支持 GPU 的高水平线程级并行。

在 Blackwell SM 内部，多个 Warp 调度器向可用流水线发射指令；四个独立的 Warp 调度器允许每个周期向可用流水线发射来自多达四个 Warp 的指令。此外，每个调度器支持**双发射 (Dual-issue)**，能够为每个 Warp 发射两条独立指令（例如，一条算术指令和一条内存操作）。注意，双发射必须来自同一个 Warp，而不是跨 Warp。

在最佳情况下，每个调度器的一个 Warp 可以在每个周期并发发射一条指令，允许每个周期并行执行四个 Warp。当利用指令混合时，这将进一步提升吞吐量，如图 6-2 所示。

*(图 6-2: Blackwell SM 包含四个独立的 Warp 调度器，每个调度器能够在每个周期发射一条 Warp 指令，并支持每个调度器进行一次数学运算和一次内存操作的双发射)*

在这里，每个 SM 被细分为四个独立的调度分区——每个分区都有自己的 Warp 调度器和分发逻辑。你可以将 SM 想象为四个共享片上资源的“迷你 SM”。这让硬件能够挑选就绪的 Warp，并在每个时钟周期从多达四个不同的 Warp 发射指令。

在每个“迷你 SM”分区内，调度器可以从**同一个** Warp 每个周期发射两条指令：一条算术指令（例如，INT32、FP32 或 Tensor Core）和一条内存指令（加载或存储）。这就是调度器被称为**双发射**的原因。表 6-1 总结了这些数字。

*(表 6-1: 关键 SM 调度器和指令发射限制 [每时钟周期])*

| 指标             | 值                                 |
| :--------------- | :--------------------------------- |
| 调度器数量       | 4                                  |
| 最大发射 Warp 数 | 4 (每个调度器 1 个)                |
| 最大数学操作数   | 4 (每个调度器的算术发射 1 个)      |
| 最大内存操作数   | 4 (每个调度器的加载/存储发射 1 个) |

因此，在最佳情况下，你可以每个周期在四个 Warp 间双发射四条数学指令和四条内存指令。这将同时最大化计算和内存吞吐量。这些数字是 SM 四路分区的结果——以及它在每个分区挑选一个 Warp 并在每个周期发射两条正交指令的能力。

**特殊函数单元 (SFU)** 位于 INT32、FP32 和 Tensor Core 流水线旁边。它们处理超越函数运算（例如正弦、余弦、倒数、平方根）。然而，它们**不**是双发射数学和内存对的一部分。SFU 使用专用的 SFU 流水线，独立于主 INT32/FP32 和加载/存储 (LD/ST) 流水线运行。

因为 SFU 占用单独的流水线并且可以在需要时并行执行，SM 可以继续发射数学和内存指令，而无需等待较慢的函数完成。这种分离进一步增加了混合操作内核的指令级并行度和整体吞吐量。它们防止复杂的数学运算阻塞核心计算和内存流水线。

因为有四个调度器——且每个通常每周期可以发射一条 Warp 指令——当有足够的独立工作和发射配对时，多达四个 Warp 可以在每个周期取得进展。例如，内存操作可以流经 SM 的总共 16 条加载/存储 (LD/ST) 流水线（每个调度器 4 条 LD/ST 流水线）。这些将读写数据到 L1/共享内存、L2 缓存或全局内存（将在后续章节介绍）。

> 确切的 LD/ST 流水线计数和配对并不保证。请依靠分析计数器 (Profiling counters) 来确定你的内核是受限于内存发射还是计算发射。并查阅 NVIDIA 文档以获取你架构的具体细节。Blackwell 调优指南是一个很好的起点。

简而言之，GPU 擅长数据并行工作负载，包括大型矩阵乘法、卷积和其他对许多元素应用相同指令的操作。开发者可以直接使用 CUDA C++ 编写内核，或通过 PyTorch 等高级框架以及 OpenAI Triton 等特定领域的基于 Python 的 GPU 语言间接编写。

在深入内核开发和内存访问优化之前，让我们回顾一下支撑所有这些实践的 CUDA 线程层级结构和关键术语。

---

## 线程、Warp、线程块和网格 (Threads, Warps, Blocks, and Grids)

CUDA 将并行工作构建为一个三级层级结构——**线程 (Threads)**、**线程块 (Thread Blocks)**（又称协同线程数组 [CTAs]）和**网格 (Grids)**——以在可编程性与巨大吞吐量之间取得平衡。在最低层级，每个线程执行你的内核代码。你将线程分组为线程块，现代 GPU 上每个块最多 1,024 个线程。当你启动内核时，线程块形成一个网格，如图 6-3 所示。

*(图 6-3: 线程、线程块 [即 CTAs] 和网格)*

通过适当地调整网格大小，你可以扩展到数百万个线程而无需更改内核逻辑。CUDA 运行时（以及像 PyTorch 这样的框架）处理跨所有 SM 的调度和分发。图 6-4 展示了线程层级结构的另一个视图，包括调用 GPU 设备上运行的 CUDA 内核的基于 CPU 的主机。

*(图 6-4: 线程层级结构视图，包括启动 GPU 设备内核的基于 CPU 的主机)*

传统上，来自不同线程块的线程无法直接相互协作。然而，现代 GPU 架构和 CUDA 版本支持**线程块集群 (Thread Block Clusters)**。线程块集群是可以跨 SM 相互通信的线程块组。

具体来说，在一个线程块集群内，不同线程块中的线程可以访问彼此的共享内存，并使用硬件支持的、集群范围的屏障 (Barriers)。这允许更大的计算操作，包括矩阵乘法，这在当今大规模 LLM 工作负载中非常常见。线程块集群在参与该集群的 SM 之间共享一个**分布式共享内存 (DSMEM)** 地址空间，如图 6-5 所示。

*(图 6-5: 在包含多个线程块的线程块集群中使用的硬件支持 DSMEM)*

DSMEM 是一项硬件特性，它通过快速片上互连将线程块集群中所有 SM 的共享内存库链接起来。有了 DSMEM，SM 共享一个组合的多 SM 分布式共享内存池。这种统一允许不同块中的线程以片上速度读取、写入和原子更新彼此的共享缓冲区——且无需使用全局内存带宽。

> 我们将在第 10 章涵盖线程块集群和 DSMEM 等高级主题。这是现代 GPU 处理中极其重要的补充——对于 AI 系统性能工程师来说非常重要。对于本章，我们的重点仍然是块内 (Intrablock) 共享内存优化。

在每个线程块内，线程使用低延迟的片上**共享内存**共享数据，并使用 `__syncthreads()` 进行同步。因为每个屏障都会产生开销，你应该最小化同步点，如图 6-6 所示。

*(图 6-6: 在代码的两部分之间同步线程块内的所有线程)*

目标是最小化同步点。然而，GPU 硬件将尝试通过在 Warp 之间快速切换来隐藏长延迟事件，如全局内存加载、缓存填充和流水线停顿。

线程块被细分为 32 个线程的 **Warp (线程束)**，它们在 SIMT 模型下使用 Warp 调度器步调一致地执行。如图 6-7 所示。

*(图 6-7: Warp [32 线程] 作为一个整体推进，由 Warp 调度器管理指令)*

保持更多 Warp 在运行中被称为 SM 上的**高占用率 (High Occupancy)**。当你的 CUDA 代码允许高占用率时，意味着当一个 Warp 停顿时，另一个 Warp 已准备好运行。这使 GPU 的计算单元保持忙碌。

然而，高占用率必须与每线程资源限制（如寄存器和共享内存）相平衡。将寄存器溢出到较慢的内存会产生新的停顿。结合寄存器和共享内存使用情况分析占用率，有助于你选择一个既能最大化吞吐量又不会触发资源竞争的块大小。

> 我们将在第 8 章涵盖占用率调优，但这是在 SM、Warp、线程等背景下需要理解的一个关键概念。

线程块独立执行，且**没有保证的顺序**。这允许 GPU 调度器将它们分发到所有 SM 上，并充分利用硬件并行性。这种网格-块-Warp 层级结构保证了你的 CUDA 内核将在拥有更多 SM 和线程的未来 GPU 架构上无需修改即可运行。

吞吐量还取决于 Warp 执行效率。Warp 中的线程必须遵循相同的控制流路径并执行**合并 (Coalesced)** 内存访问。如果某些线程发生分歧 (Diverge)，例如一个分支走 `if` 路径而其他走 `else` 路径，Warp 将串行化执行，按顺序处理每个分支路径。这被称为 **Warp 分歧 (Warp Divergence)**，如图 6-8 所示。

*(图 6-8: SIMT Warp 分歧 [左] 与 一致性 [右])*

通过屏蔽不活跃的通道 (Lanes) 并运行额外的轮次来覆盖每个分支，Warp 分歧将整体执行时间乘以分支的数量。我们将在第 8 章深入探讨 Warp 分歧——以及检测、分析和缓解它的方法。

> 分歧仅是单个 Warp 内线程的问题。不同的 Warp 可以遵循不同的分支而没有性能惩罚。

---

## 选择每块线程数和每网格块数的大小 (Choosing Threads-per-Block and Blocks-per-Grid Sizes)

GPU 性能的一个关键方面是选择与硬件 32 线程 Warp 大小对齐的线程块大小。因此，你通常选择 32 的精确倍数的线程块大小。例如，一个 256 线程的块（8 个 Warp = 256 ÷ 32）完全占用了每个 Warp，而一个 33 线程的块将需要两个 Warp 插槽，并且只使用第二个 Warp 通道的 1/32。这浪费了并行机会，因为无论 Warp 是活跃运行 32 个线程还是仅 1 个线程，它都占用一个调度器插槽。

此外，不同的 GPU 代际有不同的硬件限制，包括每 SM 最大线程数和每 SM 寄存器数量。如果我们想保持良好的性能，这自然限制了我们块的大小。例如，太大的块可能需要太多的寄存器，这将导致**寄存器溢出 (Register Spilling)** 并降低内核性能。

一个大块也可能需要太多的共享内存，这在 GPU 硬件中是有限的。具体来说，Blackwell 每 SM 仅提供 **228 KB (227 KB 可用)** 的共享内存，供 SM 上所有驻留的线程块寻址。

这些硬件限制影响了一个 SM 上可以同时活跃多少个块/Warp。这就是我们之前介绍的占用率的度量。较小的块可能允许更高的占用率，如果它们允许更多的并发 Warp 在 SM 上并发运行。

理解你的 GPU 代际的相对规模和硬件线程限制非常重要，包括线程、线程块、Warp 和 SM 的数量。图 6-9 显示了这些资源的相对规模，包括它们的限制。

*(图 6-9: Blackwell GPU 上线程的相对规模和硬件限制)*

表 6-2 总结了 Blackwell B200 GPU 的这些 GPU 限制。其余限制可在 NVIDIA 网站上找到。（其他 GPU 代际将有不同的限制，所以请务必检查你系统的确切规格。）

*(表 6-2: 线程级和块级限制 [Blackwell B200])*

| 资源                 | 硬件限制   | 备注                                                         |
| :------------------- | :--------- | :----------------------------------------------------------- |
| Warp 大小            | 32 线程    | 基本 SIMT 执行单元是 32 线程 (一个 Warp)。始终使用 32 的倍数以避免浪费。 |
| 每线程块最大线程数   | 1,024 线程 | blockDim.x * blockDim.y * blockDim.z ≤ 1024。                |
| 每线程块最大 Warp 数 | 32 Warp    | (1,024 线程 ÷ 32 线程/Warp) = 每块最大 32 Warp。             |

我们已经讨论了 32 线程的 Warp 大小限制，这鼓励我们选择 32 线程倍数的块尺寸以创建“满 Warp”并避免利用率不足的 Warp。注意每个块最多可以有 1,024 个线程，相应地，一个块只能包含 32 个 Warp。这些限制影响你的占用率，因为一旦块被调度，每个 SM 只能同时托管有限数量的 Warp 和块。

此外，还有每 SM 限制，或通常称为 **SM 驻留限制 (SM-resident limits)**。Blackwell 的这些 SM 驻留限制总结在表 6-3 中。

*(表 6-3: SM 驻留资源限制 [Blackwell B200])*

| 资源 (每 SM)        | 硬件限制   | 备注                                                         |
| :------------------ | :--------- | :----------------------------------------------------------- |
| 每 SM 最大驻留 Warp | 64 Warp    | 硬件可以保持多达 64 个 Warp 在飞行中 (64 × 32 线程 = 2,048 线程)。<br>注意：此限制已在许多代产品中保持不变，Blackwell 依然如此。 |
| 每 SM 最大驻留线程  | 2,048 线程 | 等于 64 Warp × 32 线程/Warp。如果每个块使用 1,024 线程，那么一次最多只能有 2 个这样的块 (64 Warp) 驻留在一个 SM 上。<br>使用较小的块 (例如 256 线程) 允许更多块驻留在 SM 上 (多达 8 块 × 256 = 2,048 线程)，这可以增加占用率并帮助隐藏延迟——虽然太多的小块会增加调度开销。 |
| 每 SM 最大活动块    | 32 块      | 一个 SM 上最多可以同时驻留 32 个线程块 (如果块较小，更多块可以填满此限制)。 |

在这里，我们看到 Blackwell 上每 SM 的最大并发 Warp 数是 64。这在最近的 GPU 代际中没有改变，因此占用率考量依然适用。每 SM 最大活动块数是 32，相应地，每 SM 最大驻留线程数是 2,048。CUDA 网格也有最大维度，如表 6-4 所示。

*(表 6-4: CUDA 网格限制)*

| 网格维度            | 限制                                       | 备注                                                         |
| :------------------ | :----------------------------------------- | :----------------------------------------------------------- |
| X, Y, 或 Z 最大块数 | X: 2,147,483,647<br>Y: 65,535<br>Z: 65,535 | 3D 网格可以大至 $2^{31}-1 \times 65535 \times 65535$ 个块。  |
| 最大并发网格 (内核) | 128 网格                                   | 一个设备上最多可以并发执行 128 个内核 (即，一次驻留 128 个网格)。 |

虽然了解理论网格限制很好，但你通常会受到前面显示的线程/块/每 SM 限制的约束。如果你曾需要在一个维度上超过 65,535 个块，你可以启动 2D 或 3D 网格来将工作拆分到多个内核启动中 (Multilaunch)。我们在后面的部分展示这方面的例子。在实践中，在达到其他资源限制之前很少会触及网格大小限制。

---

## CUDA GPU 向前和向后兼容性模型

CUDA 的核心优势之一是其向前和向后兼容性模型。今天编译的内核通常可以在未来的 GPU 代际上无需修改地运行——只要你在二进制文件中包含 **PTX** 以实现向前兼容性。如果你只发布针对单个架构（例如 Hopper 的 `sm_90` 或 Blackwell 的 `sm_100`）的 **SASS** 而没有 PTX，该二进制文件将**不会**在更新的架构上向前运行。特定于家族的目标（如 `sm_100f` 或 `compute_100f`）将可移植性限制在具有相同特性的家族设备上。最好发布一个包含通用 `cubin`/PTX 和所需家族特定 `cubin`（例如，用于优化等）的 **Fatbin**。

你可以通过设置 `CUDA_FORCE_PTX_JIT=1` 在加载时强制 PTX JIT 编译并缓存结果来验证兼容性。如果你的二进制文件缺少 PTX，内核启动将失败。这将迫使你重新构建以支持 PTX。

这种兼容性模型是庞大 CUDA 生态系统的基础。它让你能够从单一代码库同时面向传统硬件和尖端硬件。

> 为了真正维持跨当前和未来 GPU 代际的向后和向前兼容性，你应该使用通用目标进行编译——或显式包含 PTX。当你需要利用较新硬件特性的特定优化时，你可以使用特定于代际的目标。这样做时，请务必为其他架构提供回退路径。

---

## CUDA 编程复习 (CUDA Programming Refresher)

在 CUDA C++ 中，你通过编写**内核 (Kernels)** 来定义并行工作。这些是用 `__global__` 注解的特殊函数，在 GPU 设备上执行。当你从 CPU（主机）代码调用内核时，你使用 `<<< >>>` “Chevron (尖括号)” 语法来指定应该运行多少线程——以及它们是如何组织的——使用两个配置参数：`blocksPerGrid` 用于线程块的数量，`threadsPerBlock` 用于每个块内的线程数量。

下面是一个简单的例子，演示了 CUDA 内核和内核启动的关键组件。这个内核简单地将输入数组中的每个元素原地翻倍，因此不创建额外的内存——只使用输入数组。在幕后，CUDA 将 `__global__` 函数编译为 GPU 设备代码，可以由成千上万个轻量级线程并行执行：

```cpp
//-------------------------------------------------------
// Kernel: myKernel 运行在设备 (GPU) 上
// - input : 指向长度为 N 的浮点数组的设备指针
// - N     : 输入中的元素总数
//-------------------------------------------------------
__global__ void myKernel(float* input, int N) {
    // 计算唯一的全局线程索引
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    // 只处理有效元素
    if (idx < N) {
        input[idx] *= 2.0f;
    }
}

// 此代码运行在主机 (CPU) 上
int main() {
    // 1) 问题规模: 一百万个浮点数
    const int N = 1'000'000;
    float *h_input = nullptr;
    float *d_input = nullptr;

    // 1) 在主机上分配 N 大小的输入浮点数组
    cudaMallocHost(&h_input, N * sizeof(float));

    // 2) 初始化主机数据 (例如，全为 1)
    for (int i = 0; i < N; ++i) {
        h_input[i] = 1.0f;
    }

    // 3) 在设备上为输入分配设备内存
    cudaMalloc(&d_input, N * sizeof(float));

    // 4) 将数据从主机拷贝到设备
    cudaMemcpy(d_input, h_input, N * sizeof(float),
               cudaMemcpyHostToDevice);

    // 5) 选择内核启动参数
    // 每块线程数 (32 的倍数)
    const int threadsPerBlock = 256;
    // 每网格块数 (对于 N=1000000 为 3,907)
    const int blocksPerGrid = (N + threadsPerBlock - 1) /
                              threadsPerBlock;

    // 6) 跨 blocksPerGrid 个块启动 myKernel
    // 每个块有 threadsPerBlock 个线程
    // 传递 d_input 设备数组的引用
    myKernel<<<blocksPerGrid, threadsPerBlock>>>(d_input, N);

    // 7) 等待内核在设备上运行完成
    cudaDeviceSynchronize();

    // 8) 完成后，将结果 (存储在 d_input) 从设备拷贝回
    //    主机 (存储在 h_input)
    cudaMemcpy(h_input, d_input, N * sizeof(float),
               cudaMemcpyDeviceToHost);

    // 清理: 释放设备和主机上的内存
    cudaFree(d_input);
    cudaFreeHost(h_input);

    // 返回 0 表示成功!
    return 0;
}
```

这段代码并未完全优化。随着本书的深入，我们将优化性能。但这给了你一个简单、完整的模板来开始构建你自己的 CUDA 内核。

在这里，我们传递内核输入参数 `d_input` 和 `N`，它们在内核函数内部可访问以进行处理。处理是按设计在许多线程之间并行共享的。

完整的数据流如下：
1.  在主机上分配内存 (`h_input`)。
2.  使用 `cudaMemcpy` 和 `cudaMemcpyHostToDevice` 将数据从主机 (`h_input`) 拷贝到设备 (`d_input`)。
3.  在设备上使用 `d_input` 运行内核。
4.  同步以确保内核已在设备上完成执行。
5.  使用 `cudaMemcpy` 和 `cudaMemcpyDeviceToHost` 将结果 (`d_input`) 从设备传输到主机 (`h_input`)。
6.  使用 `cudaFree` 和 `cudaFreeHost` 清理设备和主机上的内存。

你可以在启动时通过 `<<< >>>` 传递额外的、高级的、CUDA 特定的参数给你的内核，包括共享内存大小（以及许多其他参数），但两个核心启动参数 `blocksPerGrid` 和 `threadsPerBlock` 是任何 CUDA 内核调用的基础。在下一节中，我们将讨论如何最好地选择这些启动参数值。

你可能想知道为什么我们要传递 `N`，即输入数组的大小。这看起来是多余的，因为内核应该能够检查数组的大小。然而，这是 GPU CUDA 内核函数和典型 CPU 函数的核心区别：CUDA 内核函数设计为在单个线程内工作，与成千上万的其他线程一起，处理输入数据的一个**分区 (Partition)**。因此，`N` 定义了这个特定内核将处理的分区的大小。

结合内置的内核变量 `blockDim`（在本例中为 1，因为我们传递的是一维输入数组）、`blockIdx` 和 `threadIdx`，内核计算输入数组中的特定 `idx`。这个唯一的 `idx` 让内核能够清晰、唯一地处理输入数组的每个元素，并在跨许多不同 SM 同时运行的许多线程中并行进行。

注意边界检查 `if (idx < N)`。这是避免越界访问所必需的，因为 `N` 可能不是块大小的精确倍数。例如，考虑一个场景，输入数组大小为 63，即 N = 63。Warp 调度器可能会分配两个 Warp（每个 32 线程）来处理输入数组中的 63 个元素。

第一个 Warp 将同时运行 32 个内核实例来处理元素 0–31，且永远不会超过 N = 63。这很简单。与第一个 Warp 并行运行的第二个 Warp 将预期处理元素 32–64。然而，当它达到 N = 63 时，它将停止。

如果没有 `if (idx < N)` 边界检查，第二个 Warp 将尝试处理 `idx = 64`，并将抛出**非法内存访问错误**（例如 `cudaErrorIllegalAddress`）。边界检查确保每个线程要么处理有效的输入元素，要么在其 `idx` 超出范围时立即退出。

CUDA 内核在设备上异步执行，没有每线程异常；相反，任何非法操作（越界访问、未对齐访问等）都会为整个启动设置一个全局故障标志。主机驱动程序仅在下次调用同步或其他 CUDA API 函数时检查该标志，因此错误是**延迟 (Lazily)** 浮现的（例如，作为 `cudaErrorIllegalAddress` 或通用的启动失败）。

这种设计使 GPU 的流水线和互连保持完全占用，但要求你在主机上显式同步和轮询错误——通常在内核启动后立即使用 `cudaGetLastError()` 和 `cudaDeviceSynchronize()`。这样，你可以在故障发生时立即捕获它们。

你会看到许多 CUDA 内核中都有边界检查。如果你没看到，你应该理解为什么没有。它可能以某种方式存在——或者 CUDA 内核开发者可以某种方式保证非法内存访问错误永远不会发生。

最后，我们来到实际的内核逻辑。计算出输入数组的唯一索引 `idx` 后，该内核（在跨许多 SM 并行的数千个线程上分别运行）将输入数组中索引 `idx` 处的值乘以 2。然后它（原地）更新输入数组中的值。在这个特定内核中，除了 `int` 类型的临时变量 `idx` 外，不需要额外的内存。

---

## 配置启动参数：每网格块数和每块线程数

如前所述，选择一个作为 Warp 大小 (32) 倍数的线程块大小至关重要。`threadsPerBlock` 大小为 **256**（8 个 Warp）是一个常见的起点，以平衡占用率和资源使用。这将帮助我们避免在内核执行期间出现部分填充的 Warp，隐藏延迟，并平衡 SM 和其他硬件资源：

**32 线程的倍数**
> 选择 32 线程倍数的块大小有助于避免空 Warp 插槽。否则，那些未填满的 Warp 会占用稀缺的调度器资源——而不贡献有用的工作。

**延迟隐藏**
> 每个 SM 需要数百个线程来隐藏 DRAM 和指令延迟停顿。如果你在一个拥有 2,048 线程容量的 SM 上启动（比如）8 个 256 线程的块，你可以保持流水线忙碌而不过度订阅。

**占用率**
> 以 256 `threadsPerBlock` 为例，你每块只需要 8 个 Warp。这倾向于提供良好的占用率，而不会耗尽每块的寄存器或共享内存。
>
> *对于像 Blackwell 这样的现代 GPU，考虑每块 256–512 个线程以最大化占用率，同时遵守寄存器和共享内存限制。*

**资源平衡**
> 256 足够小，你很少会超过每块 1,024 线程的限制。而且它足够大，当其他 Warp 中的线程停顿时，你不会留下太多 Warp 空闲。
>
> 从 `threadsPerBlock=256` 开始，你可以根据内核的寄存器和共享内存需求——以及占用率特性——向上或向下调优（128, 512 等）。

对于 `blocksPerGrid`，你可以基于输入元素数量 `N` 和 `threadsPerBlock` 的值。例如，`blocksPerGrid` 通常设置为 `(N + threadsPerBlock - 1) / threadsPerBlock` 以向上取整，以便在 N 不是 `threadsPerBlock` 的精确倍数时覆盖所有元素。这是一个常见的选择，保证每个输入元素都被一个线程覆盖。

---

## 2D 和 3D 内核输入

当你的输入数据天然存在于二维（例如图像）中时，你可以启动 2D 块的 2D 网格。例如，这是一个处理二维 1,024 × 1,024 矩阵的内核，使用 16 × 16 维度的线程块，总共 256 个线程：

*(代码示例省略，展示了使用 dim3 结构定义 2D 网格和块，并在内核中计算 x, y 坐标)*

对于大多数情况，本书使用 1D 或 2D（平铺）值的 `blocksPerGrid` 和 `threadsPerBlock`。在 1D 情况下，你可以将 `blocksPerGrid` 和 `threadsPerBlock` 定义为简单的常量而不是 `dim3`。

---

## 异步内存分配和内存池

标准 `cudaMalloc`/`cudaFree` 调用是同步的且相对昂贵。它们需要全设备同步（相对较慢），并涉及 OS 级调用如 `mmap`/`ioctl` 来管理 GPU 内存。这种 OS 级交互会导致内核空间上下文切换和驱动程序开销，使得它们相对于纯设备侧操作来说相对较慢。因此，建议使用异步版本 `cudaMallocAsync` 和 `cudaFreeAsync`，以在 GPU 上进行更高效的内存分配。

默认情况下，CUDA 运行时维护一个 GPU 内存的**全局池 (Global Pool)**。当你异步释放内存时，它会回到池中以供后续分配重用。`cudaMallocAsync` 和 `cudaFreeAsync` 在底层使用 CUDA 内存池。

内存池回收已释放的内存缓冲区，避免重复调用 OS 分配新内存。这有助于通过重用以前释放的块来减少随时间的内存碎片，例如在长时间运行的训练循环的每次迭代中。内存池在 PyTorch 等高性能库和运行时中默认启用。

事实上，PyTorch 使用自定义的**缓存分配器 (Caching Allocator)**，通过 `PYTORCH_ALLOC_CONF` 配置。PyTorch 内存缓存分配器在精神上类似于 CUDA 的内存池：它重用 GPU 内存，避免了例如在长时间运行的训练循环的每次迭代期间为每个新创建的 PyTorch 张量调用同步 `cudaMalloc` 操作的成本。

在执行频繁、细粒度分配的 CUDA 应用程序中，使用异步的基于池的例程——`cudaMallocAsync` 和 `cudaFreeAsync`——比传统的同步 `cudaMalloc`/`cudaFree` 要高效得多，后者会导致全设备同步甚至 OS 级调用。要使用流式排序分配，创建一个非阻塞流：

```cpp
cudaStream_t stream1;
cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking);
```

> 使用显式 CUDA 流是重叠传输、内核和内存操作的最佳实践。将每个流视为一个强制其自身操作顺序的隔离通道。此外，建议使用 `cudaStreamCreateWithFlags(..., cudaStreamNonBlocking)` 创建非阻塞流，以避免传统的默认流屏障。我们将在第 11 章更详细地探讨多流重叠技术和最佳实践。

然后，每当你需要一个 N 个浮点数的缓冲区时，你在该流上分配并释放它，使用 `cudaMallocAsync` 和 `cudaFreeAsync`。

这些 API 从每设备内存池分配，但尊重你传递的流的顺序，因此释放操作会推迟直到该流的工作完成。因为 `cudaFreeAsync` 仅等待 `stream1` 完成，所以没有昂贵的全局 `cudaDeviceSynchronize`，也没有与其他流的隐式同步。当你的代码发出成千上万——或数百万——次分配/释放循环时，结果是分配开销大大降低，减少了碎片化并平滑了延迟峰值。总的来说，相对于传统的 `cudaMalloc` 和 `cudaFree`，这种模式减少了全局同步和碎片化。

你可以通过 `cudaMemPoolSetAttribute` 进一步调优来自设备内存池的流式排序分配的行为（例如，调整 `cudaMemPoolAttrReleaseThreshold`）以调优释放阈值，在最小内存占用和低碎片化之间取得适当的权衡。

---

## 理解 GPU 内存层级结构 (Understanding GPU Memory Hierarchy)

到目前为止，我们在高层次上广泛讨论了内存分配，通常是从全局内存中分配。但在现实中，GPU 提供了多级内存层级结构，有助于平衡容量和速度。层级结构包括寄存器、共享内存、缓存、全局内存，以及在 Blackwell GPU 及更高版本上的专用 **TMEM**。

TMEM 是 Blackwell 第 5 代 Tensor Core 指令 (`tcgen05.*`) 使用的专用每 SM 片上内存（约 256 KB）。它不能从 CUDA C++ 直接指针寻址。相反，数据移动由 **TMA (张量内存加速器)** 硬件（全局内存 ↔ SMEM）和 `tcgen05` Tensor Core 数据移动指令（SMEM ↔ TMEM，使用张量描述符隐式进行）编排。

全局内存（HBM 或 DRAM）很大，位于片外，相对较慢。寄存器很小，位于片上，极快。L1 缓存、L2 缓存和共享内存介于两者之间。缓存和共享内存的好处是它们隐藏了访问大型片外内存存储的相对较长的延迟。图 6-10 展示了 GPU 内存层级结构（包括 CPU）的高级视图。

*(图 6-10: GPU 内存层级结构，包括 CPU)*

**TMEM** 是一个专用的每 SM 256 KB 缓冲区，以数十 TB/s 的带宽透明地与 Tensor Cores 通信。这减少了 Tensor Core 对全局内存的依赖。图 6-11 展示了 TMEM——连同 SMEM——服务于 Tensor Cores 以计算 C = A × B 矩阵乘法。

*(图 6-11: TMEM 和 SMEM 服务于 Tensor Cores 进行 C = A × B 矩阵乘法)*

在这里，操作数 B 源自 SMEM。操作数 A 在 TMEM 中（尽管它也可能在 SMEM 中）。累加器也在 TMEM 中。图块 (Tiles) 使用 TMA（例如 `cuda::memcpy_async`）通过 L2 缓存从全局内存流向 SMEM。操作数通过 Tensor Core 指令（如统一矩阵乘累加 [UMMA] 和 `tcgen05.mma`）在 SMEM 和 TMEM 之间隐式移动。

表 6-5 显示了 Blackwell GPU 的不同内存级别及其特性。

*(表 6-5: Blackwell 内存层级结构和特性)*

| 级别                        | 范围                 | 容量                                                 | 延迟                                      | 带宽 (约)                        |
| :-------------------------- | :------------------- | :--------------------------------------------------- | :---------------------------------------- | :------------------------------- |
| **寄存器 (Registers)**      | 每线程 (在 SM 上)    | 每 SM 64 K 32位寄存器 (每线程最大 255)               | 接近寄存器延迟 (读写是单周期的，基本免费) | 每 SM 数十 TB/s (寄存器文件端口) |
| **共享内存和 L1 数据缓存**  | 每 SM                | 228 KB (227 KB 可用) 共享 + 剩余部分作为 L1/数据缓存 | ~20–30 周期 (L1/shared 基准)              | 每 SM TB/s 级 (无 Bank 冲突)     |
| **TMEM**                    | 每 SM                | 256 KB SRAM 每 SM，专用于 Tensor Cores               | ~10 周期 (SM 上的专用 SRAM)               | 与 Tensor Cores 的 TB/s 级通信   |
| **常量内存缓存**            | 每 SM                | ~8 KB 缓存用于 64 KB `__constant__` 空间             | ~1 周期 (Warp 广播)                       | TB/s 级 (广播吞吐量)             |
| **L2 缓存**                 | GPU 全局 (所有 SM)   | 总共 126 MB                                          | ~200 周期                                 | 聚合多 TB/s                      |
| **本地内存 (Local Memory)** | 每线程 (溢出到 DRAM) | 接近无限 (由全局内存支持)                            | 100s → 1,000 周期 (类 DRAM)               | ~8 TB/s (HBM3e)                  |
| **全局内存 (HBM or DRAM)**  | 设备全局 (片外 DRAM) | Blackwell B200 高达 180 GB                           | 100s → 1,000 周期                         | 总共 ~8 TB/s                     |

在这里，你可以看到为什么最大化寄存器、共享内存和 L1/L2 缓存中的数据重用——并最小化对全局内存和本地内存（由全局内存支持）的依赖——对于高吞吐量 GPU 内核至关重要。

### 统一内存 (Unified Memory)

统一内存（也称为 CUDA 托管内存）为你提供了一个单一的、一致的地址空间，跨越 CPU 和 GPU，因此你不再需要处理独立的主机和设备缓冲区或发出显式的 `cudaMemcpy` 调用。在底层，CUDA 运行时用页面支持每个 `cudaMallocManaged()` 分配，这些页面可以在连接 CPU 和 GPU 的任何互连上**按需迁移**，如图 6-16 所示。

*(图 6-16: CPU-GPU 统一内存的自动页面迁移)*

虽然访问统一内存对开发者非常友好，但它可能会导致 CPU 和 GPU 之间不必要的按需页面迁移。这将引入隐藏的延迟和执行停顿。例如，如果 GPU 线程访问当前驻留在 CPU 内存中的数据，GPU 将发生缺页中断，并等待数据通过 NVLink-C2C 互连传输。统一内存性能在很大程度上取决于底层硬件。

在传统的 PCIe 或早期 NVLink 系统上，这些迁移以相对较低的带宽传输——通常使得故障时传输比手动 `cudaMemcpy` 慢。但在 Grace Hopper 和 Grace Blackwell 超级芯片上，NVLink-C2C 结构在 CPU 的 HBM 和 GPU 的 HBM3e 之间提供高达 ~900 GB/s 的带宽。因此，缺页驱动的迁移速度更接近设备原生速度——尽管它们仍然带有非零延迟。

即便如此，内核启动期间的任何意外缺页都会使 GPU 停顿，直到运行时将所需的页面移动到位。为了避免那些“惊喜”停顿，你可以使用 `cudaMemPrefetchAsync()` 提前预取内存，如图 6-17 所示。

*(图 6-17: 使用 cudaMemPrefetchAsync() 通过 NVLink-C2C 从 CPU 流式传输数据到 GPU)*

这向驱动程序提示在你启动内核之前将指定范围移动到目标 GPU（或 CPU）上，将昂贵的首次接触迁移转变为可重叠的异步传输。你还可以给出内存建议 (`cudaMemAdvise`)，例如使用 `PreferredLocation` 告诉驱动程序你主要在哪里使用数据，以及当数据主要是只读时使用 `ReadMostly`，如图 6-18 所示。

*(图 6-18: 指定“首选位置”以告诉 CUDA 驱动程序数据主要如何使用)*

默认情况下，任何 CUDA 流或设备内核都可以触发托管分配上的缺页。这可能会导致意外的迁移和隐式同步。如果你知道某个缓冲区一次只会被一个流/GPU 使用，将其**附加 (Attach)** 到该流允许迁移与其他流中的操作重叠。调用 `cudaStreamAttachMemAsync` 将该内存范围绑定到指定流。

简而言之，显式预取托管内存并提供内存建议可以消除统一内存的大部分“惊喜”停顿。数据在内核运行时已经就位，而不是 GPU 暂停以按需获取数据。

---

## 维持高占用率和 GPU 利用率 (Maintaining High Occupancy and GPU Utilization)

GPU 通过并发运行许多 Warp 来维持性能，以便当一个 Warp 等待数据而停顿时，另一个 Warp 可以运行。这种在 Warp 之间快速切换的能力允许 GPU **隐藏内存延迟**。如前所述，SM 容量中实际被活跃 Warp 占用的比例称为**占用率 (Occupancy)**。

如果占用率低（只有几个活跃 Warp），当一个 Warp 等待内存时，SM 可能会闲置。这导致 SM 利用率低下。在 Blackwell 上，鉴于其巨大的寄存器文件（每 SM 64K 寄存器），实现高占用率稍微容易一些，可以支持许多 Warp 而不溢出。

相反，高占用率（每 SM 许多活跃 Warp）将使 GPU 计算单元保持忙碌，因为当一个 Warp 等待内存访问时，其他 Warp 将交换进入 SM 并执行。这掩盖了长内存访问延迟。这通常被称为**隐藏延迟 (Hiding Latency)**。

这是 CUDA 性能优化的最基本规则之一：**启动足够的并行工作以完全占用 GPU。**

如果你的已实现占用率（使用的硬件线程槽位比例）远低于 GPU 的限制且性能不佳，第一个补救措施是增加并行度——使用更多块或线程，使占用率在现代 GPU 上接近 80%–100% 的范围。

相反，如果占用率已经中等到高，但内核受限于内存吞吐量，将其推到 100% 可能无济于事。你通常只需要足够的 Warp 来隐藏延迟，除此之外，瓶颈可能在别处（例如，内存带宽）。

### 使用启动边界 (Launch Bounds) 调优占用率

在某些情况下，仅仅使用更多线程是不够的——特别是如果每个线程使用大量资源（如寄存器和共享内存）。我们可以通过使用 CUDA 的 `__launch_bounds__` 内核注解来指导编译器优化占用率。

此注解允许我们在编译时为内核指定两个参数：我们将启动的**每块最大线程数**和我们要保持在每个 SM 上驻留的**最小线程块数**。这些提示影响编译器的寄存器分配和内联决策。

```cpp
__global__ __launch_bounds__(256, 16)
void myKernel(...) { /* ... */ }
```

这里，`__launch_bounds__(256, 16)` 承诺 CUDA 内核永远不会以每块超过 256 个线程启动。它还请求编译器分配足够的寄存器和内联函数，以便至少 16 个 256 线程的块，即 4,096 个线程（16 块 × 256 线程/块），可以同时驻留在 SM 上。

> 记住，在现代 GPU（例如 Blackwell）上，我们每块只能有 1,024 个线程，每 SM 最多 2,048 个驻留线程。

实际上，由于当前 NVIDIA GPU 限制每 SM 2,048 个总线程和每块 1,024 个线程，编译器会将你的请求减少到硬件最大值——在这种情况下，每 SM 2,048 个线程。如果请求超过 SM 容量，它会发出警告。

在实践中，使用 `__launch_bounds__` 通常会导致编译器限制每线程寄存器使用（有时限制展开或内联）以避免溢出并允许更高的占用率。我们本质上是在牺牲一点每线程性能（不使用每一个寄存器或最大化展开），以换取通过保持更多 Warp 在飞行中来获得更一致的 Warp 吞吐量。

你还可以使用 CUDA Occupancy API 在运行时确定最佳启动配置。例如，`cudaOccupancyMaxPotentialBlockSize` 将计算产生最高占用率的块大小，考虑其寄存器和共享内存使用情况。

---

## 使用 NVIDIA Compute Sanitizer 调试功能正确性

由于 CUDA 应用程序可以为每个内核生成数千个线程，传统调试可能无法捕捉微妙的内存错误和竞争条件。**NVIDIA Compute Sanitizer**，CUDA Toolkit 中包含的一个功能正确性套件，通过在运行时检测代码来解决这些挑战，以便在开发早期发现错误。

Sanitizer 使用 `compute-sanitizer` CLI 调用，并支持 **NVIDIA Tools Extension (NVTX)** 注解以进行更细粒度的分析。

Compute Sanitizer 包含四个主要工具：
*   **Memcheck**: 精确检测全局、本地和共享内存中的越界或未对齐访问；报告 GPU 硬件异常；并可以识别设备端内存泄漏。
*   **Racecheck**: 报告共享内存数据冒险，包括写后写、读后写和写后读。这有助于开发者验证 Warp 和线程块内正确的线程间通信。
*   **Initcheck**: 标记对未初始化设备全局内存的任何访问。这有助于避免因陈旧或垃圾数据引起的微妙 Bug。
*   **Synccheck**: 检测同步原语（如不匹配的屏障）的无效使用。它识别可能导致死锁和线程间状态不一致的线程排序危险。

---

## 屋顶线模型：计算受限或内存受限工作负载

**屋顶线模型 (Roofline Model)** 是一个有用的可视化工具，它绘制了两个硬件强加的性能上限：一条水平线表示处理器的峰值浮点速率，一条对角线由峰值内存带宽设定。它们共同形成一个“屋顶线”包络，揭示给定内核是受限于计算 (**Compute Bound**) 还是数据移动 (**Memory Bound**)。

这些线相交的地方称为**脊点 (Ridge Point)**。这对应于内核从内存受限（脊点左侧）过渡到计算受限（脊点右侧）的“**算术强度 (Arithmetic Intensity)**”阈值。算术强度被测量为每字节传输（在片外全局内存和 GPU 之间）所执行的 FLOPS 数。

图 6-21 展示了 Blackwell 级 GPU 的代表性屋顶线模型，包括峰值计算性能（水平线，约 80 TFLOPs/sec FP32）和峰值内存带宽（对角线，对应 8 TB/s）。

*(图 6-21: Blackwell 级 GPU 的屋顶线模型)*

对于内存受限的工作负载，目标是将内核的操作点推向屋顶线的右侧，增加其算术强度，并更接近成为计算受限。通过向计算受限区域移动，你的内核可以更好地利用 GPU 的全部浮点马力。

一种使内核减少内存受限的简单方法是使用**低精度数据**。例如，如果你使用 16 位浮点数 (FP16) 代替 32 位 (FP32)，你将每操作传输的字节数减半，并立即将 FLOPS/byte 强度翻倍。

Blackwell 还引入了对 **4-bit 浮点数 (FP4)** Tensor Cores 的原生支持。这进一步减少了每操作的字节数，并更多地增加了 FLOPS/byte 强度。

例如，Blackwell 支持 FP8 Tensor Cores（每值 1 字节），相对于 FP16 使吞吐量翻倍并减半内存使用。它还支持 FP4（每值半字节）用于某些工作负载如模型推理。

单个 128 字节内存事务可以携带 32 个 FP32、64 个 FP16、128 个 FP8 或 256 个 FP4 值。Blackwell 引入了硬件解压缩以加速压缩模型权重。例如，模型可以以压缩形式存储在 HBM 中，甚至超过 FP4 压缩，硬件可以即时解压权重。这实际上在读取这些权重时进一步增加了可用内存带宽。

因此，Blackwell 对于像基于 Transformer 的 Token 生成这样的内存受限工作负载具有架构优势。通过使用低精度和硬件解压，你可以减少传输的数据量，增加内核的算术强度，并提高工作负载的整体内存吞吐量。

当内核受限于内存时，Nsight Compute 将报告非常高的 DRAM 带宽利用率，同时伴随较低的已实现计算指标（如低 ALU 利用率）。这表明 Warp 将大部分时间花在等待内存访问上。

为了深入了解发生了什么，最好使用 Nsight Compute 获取每内核计数器，包括延迟、缓存命中率和 Warp 发射停顿。然后你可以使用 Nsight Systems 获取整体时间线视图，显示 GPU 空闲间隙、与 CPU 工作的重叠以及 PCIe/NVLink 传输。

关键是迭代地分析并识别内存热点。你应该在可疑代码周围添加 NVTX 范围，放大时间线行为，并利用反馈进行优化。

---

## 结论 (Conclusion)

本章通过揭开 GPU SIMT 模型、线程层级结构和多级内存系统的神秘面纱，为高性能 CUDA 开发奠定了基础。请记住，占用率——活跃 Warp 与理论 GPU 最大值的比率——对于**隐藏延迟**很重要。

然而，最大化占用率并不保证在每种情况下都是最佳性能。如果线程具有足够的指令级并行性 (ILP)——或者如果瓶颈在其他资源——GPU 通常可以在中等甚至低占用率下实现非常高的吞吐量。

虽然较高的占用率有助于隐藏延迟，但在某些场景中，减少活跃线程数可以为其他线程释放寄存器。这允许每线程更多计算——并最终提升吞吐量。始终对不同的占用率水平进行基准测试，以找到适合你工作负载和硬件的最佳设置。

掌握了这些基础知识和分析技术，你现在已经准备好深入进行针对性优化，如避免 Warp 分歧、利用 GPU 内存层级结构和异步预取内存。我们还将深入探讨 **TMA**，它处理批量内存传输，释放 GPU 以专注于有用的工作并增加计算有效吞吐量。









**译境** 已就位。

第七章是 CUDA 优化的核心战场：**内存访问模式**。从简单的合并访问 (Coalescing) 到复杂的 TMA 异步预取，本章涵盖了从初级到高级的全部内容。

译文将力求在技术术语的准确性与工程指导的实用性之间保持平衡，特别是在描述复杂的内存层级结构（从寄存器到 HBM）以及如何利用 Nsight 工具进行分析时，力求清晰、透彻。

以下是 **第七章：剖析与调优 GPU 内存访问模式** 的中文译本。

---

# 第七章：剖析与调优 GPU 内存访问模式

随着 AI 模型在规模和复杂性上的增长，GPU 的内存系统往往成为阻碍理论计算能力转化为现实世界性能的瓶颈。正如你在第六章所见，现代 NVIDIA GPU 结合了成千上万个简单的、吞吐量优化的核心与专用的 Tensor Cores。它们还包括高带宽内存 (HBM)、一致的 CPU-GPU 统一内存地址空间（例如 Grace Blackwell 超级芯片）、片上共享内存、缓存，以及像**张量内存加速器 (TMA)** 这样的专用直接内存访问 (DMA) 引擎。

在本章中，你将看到各种 CUDA C++ 和 PyTorch 优化技术，用于对齐数据结构以实现高效内存访问，消除冗余数据加载，并通过硬件将数据传输与计算重叠。

通过矩阵乘法、张量操作等具体的“优化前”与“优化后”示例，你将看到内存访问模式、分块 (Tiling) 策略和异步数据传输中的微小变化，是如何减少带宽浪费、提升算术效率，并将内核从内存受限转变为计算受限的。

读完本章，你将知道如何编写能更好地利用 GPU 内存层级结构和硬件优化数据传输引擎的 CUDA 内核。

---

## 合并与未合并的全局内存访问 (Coalesced Versus Uncoalesced Global Memory Access)

代码的内存访问模式会极大地影响性能。当一个 Warp 中的线程访问**连续**的内存地址，使得硬件可以将这些访问合并为更少、更大的事务时，全局内存访问速度最快。如果线程访问分散或未对齐的地址，设备就无法将请求合并为最小数量的缓存行事务（现代 GPU 上是 128 字节的行，由四个 32 字节的扇区组成）。这将导致更多的内存事务检索未使用的数据，从而迅速耗尽内存带宽。

在 Blackwell GPU 上，单设备 HBM3e 带宽高达 8 TB/s。在 Grace Blackwell GB200 和 GB300（双 GPU 超级芯片）内，这一数字在两个 GPU 间增加到 16 TB/s。使用未合并的内存访问将导致大部分带宽因过多的内存事务和停顿而被闲置。

在未合并的情况下，Warp 中的每个线程从分散的地址加载数据。这会导致许多独立的内存事务。即使 Warp 中的线程访问连续地址，如果首地址未与 128 字节对齐，Warp 的请求也将跨越两个 128 字节缓存行。

例如，如果一个 Warp 的第一个线程从一个未 128 字节对齐的地址开始，Warp 的内存请求将跨越缓存行边界，通常导致两个 128 字节事务而不是一个。在这种情况下，Warp 可能会获取超出最佳四个扇区的额外扇区，导致跨两行的总共五个扇区。这是带宽的浪费。一个未对齐的、连续的 128 字节 Warp 加载是触及 5× 32B 扇区还是 8× 32B 扇区，取决于起始偏移量。对齐的访问将其保持在 4× 32B 扇区。

而在**合并 (Coalesced)** 的情况下，线程从连续地址加载，这些地址被合并为一个单一的宽事务。图 7-1 比较了合并与未合并的内存访问。

*(图 7-1: 比较合并与未合并的内存访问模式)*

在内核代码中，这个问题通常表现为**跨步 (Strided)** 或不规则的索引，使得每个线程触及不同的缓存行。当内核线程使用跨步或不规则索引获取数据时，GPU 会发出许多小的、未合并的全局内存事务，而不是少数几个全宽加载。

在 Nsight Compute 中，当存在未合并模式时，**Memory Workload Analysis** 部分将显示较低的**全局内存加载效率 (Global Memory Load Efficiency)**，较高的 DRAM 扇区读取计数，以及高于 4.0 的**平均每请求扇区数 (Average Sectors per Request)**。这是因为获取的扇区比正确合并的内存访问模式更多，这表明你在通过获取主要未使用的字节来浪费带宽。DRAM 吞吐量百分比也将远低于峰值。这证实了你的 Warp 正在花费周期在内存上停顿，而不是驱动 ALU。

为了摆脱这种内存受限的状态，你可以重组数据，以便每个 Warp 的 32 个线程加载连续元素。要么使用 `input[idx]` 索引数组，其中 `idx = blockIdx.x * blockDim.x + threadIdx.x`，要么切换到**结构数组 (Structure-of-Arrays, SoA)** 布局，以便线程 `i` 总是触及元素 `i`。**数组结构 (Array-of-Structures, AoS)** 与 SoA 的区别如图 7-2 所示。

*(图 7-2: 数组结构 (AoS) 与 结构数组 (SoA))*

一旦你做出了改变，硬件会自动将 Warp 的全局内存加载合并为更少、更宽的事务，并返回更多可用（更少浪费）的数据。Nsight Compute 计数器将立即显示改进。

让我们通过一个例子来演示这一点。接下来的“优化前”和“优化后”代码演示了将全局内存访问从跨步模式重构为连续模式如何产生显著的性能提升。

**优化前示例 (C++)：未合并的跨步访问**。在这个例子中，每个线程以 2 的步长从输入数组复制数据，导致未对齐的内存访问：

```cpp
#include <cuda_runtime.h>
#include <iostream>

__global__ void uncoalescedCopy(const float* __restrict__ in,
                                float* __restrict__ out,
                                int N, int stride) {
    // n = 1048576, stride = 2
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        // 以步长从 in[] 加载，导致获取多个内存段
        out[idx] = in[idx * stride];
    }
}
// (main 函数省略，设置 stride=2 并启动内核)
```

这个 CUDA C++ 内核发出的全局内存加载地址被一个大于 1 的步长分隔，根据定义这是不连续的。这导致每个 Warp 生成多个小事务，而不是单个宽事务。

**优化前示例 (PyTorch)**。在 PyTorch 中，可以使用跨步索引为 `gather` 操作创建类似的情况：

```python
import torch

def uncoalesced_copy(input_tensor, stride):
    flat_tensor = input_tensor.contiguous().view(-1)
    # 生成带有固定步长的索引进行 gather
    idx = torch.arange(0, flat_tensor.numel(), stride, ...)
    # index_select 使用 gather 内核，执行未合并加载
    return torch.index_select(flat_tensor, 0, idx)
```

这段 PyTorch 代码片段使用带有跨步索引模式的 `torch.index_select`，这导致底层 GPU gather 内核执行未合并加载。具体来说，一个 32 线程的 Warp 将访问相距 `stride * 4` 字节的地址。这不允许单个宽事务，而是生成 32 个独立的加载。这阻止了合并。图 7-3 显示了合并与跨步访问模式——以及随机访问模式。

*(图 7-3: 合并、跨步和随机内存访问模式)*

在 GPU 上运行 C++ 和 PyTorch 代码后，我们测量性能指标。在未合并版本中，每个 Warp 的内存请求平均被分解为 8 个独立的 32 字节扇区。由于每个 128 字节缓存行获取被拆分为 4 个独立的 32 字节扇区（稍后会详细介绍数字 4），访问模式跨越每 Warp 两行来检索 8 个独立的 32 字节扇区。

在未优化版本中，每个 Warp 的未合并加载将单个逻辑请求分解为多达 8 个独立的 32 字节扇区，导致事务计数激增并使内存管道挨饿。结果，Nsight Compute 报告仅达到约 25% 的持续峰值 DRAM 吞吐量。

现在，让我们通过**合并**内存访问并让线程读取连续元素来优化这一点，以便每个 Warp 发出更少、更大的事务。

**优化后示例 (C++)：合并访问**。只需移除步长（或将其设为 1），每个线程复制一个连续元素。这种线程访问的对齐允许硬件将内存请求合并为完整的 128 字节事务：

```cpp
__global__ void coalescedCopy(const float* __restrict__ in,
                              float* __restrict__ out,
                              int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // 连续加载
        // 线程复制相邻元素
        out[idx] = in[idx];
    }
}
// (main 函数省略，启动配置与之前类似)
```

我们为了简洁省略了合并的 PyTorch 实现，因为它已经内置在 PyTorch 中了。PyTorch 中的合并版本只需执行类似 `out = inp.clone()` 的操作。这有效地复制连续元素。实际上，对连续张量的 `clone()` 在底层使用向量化内存拷贝，类似于我们的合并内核。

> 你也可以使用 `torch.compile` 和默认的 TorchInductor 后端来减少冗余拷贝并在安全时融合相邻操作。`clone()` 已经是设备到设备拷贝，因此通常不会融合到单独的自定义内核中。启用自动调优（例如 `mode="max-autotune"`）以帮助 TorchInductor 在形状稳定时选择合并和向量化的调度。我们将在第 13 和 14 章深入介绍 PyTorch 编译器。

通过合并访问，即无步长，每个 Warp 的线程访问相邻地址。在这种情况下，硬件将每个 Warp 的加载合并为最小数量的 128 字节事务——通常当首地址 128 字节对齐时为每 Warp 一个 128 字节事务，如果访问跨越边界则为两个。表 7-1 显示了由此优化带来的显著改进。

*(表 7-1: 合并与未合并内存访问性能)*

| 指标                 | 优化前 (未合并) | 优化后 (合并)  |
| :------------------- | :-------------- | :------------- |
| DRAM 吞吐量 (% 峰值) | 25%             | 90% (3.6倍)    |
| 全局内存加载效率     | 23%             | 99%            |
| 平均每请求扇区数     | 8.0             | 4.0 (最佳)     |
| SM 活跃度 %          | 62%             | 99%            |
| 内核执行时间 (ms)    | 4.8 ms          | 1.3 ms (3.7倍) |

修复数据布局并合并全局内存访问后，DRAM 吞吐量从峰值的 25% 上升到 90%，大约高出 3.6 倍。内核执行时间提高了约 3.7 倍，从 4.8 ms 降至 1.3 ms，因为更少的停顿释放了 SM 以取得进展。

全局内存加载效率从 23% 上升到 99%，意味着几乎每个获取的字节都是有用的。同时，平均每请求扇区数降至约 4.0。

> 值接近 4.0 意味着 Warp 的加载完全合并。首地址 128 字节对齐时，所有 32 个线程映射到一个 128 字节行。此行有四个 32 字节扇区，因此指标报告平均每请求 4.0 个扇区。

在跨步或分散访问的最坏情况下，该值可能接近 32，因为 L2 以 32 字节扇区报告活动。高于 4.0 的值表示未合并或未对齐的访问。在未优化版本中，未合并加载可以将单个逻辑访问分解为许多每请求扇区，接近最坏情况。

这里我们大约是每请求 4.0 个扇区，这表明请求干净地映射到 128 字节行，没有未使用的扇区。随着内存停顿减少，SM 活跃度百分比从 62% 提高到 99%。

总之，通过将每个 Warp 的线程对齐在连续地址上，GPU 的内存控制器可以用几次大事务而不是几十次小事务来服务每个 Warp。这将在将全局内存加载效率提升至接近 100% 的同时，通过保持 Warp 忙碌而非空闲来提高 SM 活跃度。

虽然你已经看到合并如何重塑线程到地址的映射，使整个 Warp 的加载排列在 GPU 的 128 字节段上。然而，即使是完全合并的 Warp，在底层仍然发出 32 个独立的 4 字节读取（每个线程一个），迫使硬件将它们重新缝合到每个 128 字节事务中。

为了消除这最后一层低效，我们转向**向量化内存访问 (Vectorized Memory Access)**：让每个线程在单条指令中获取更宽、对齐的块（例如，16 字节的 `float4`），这样一个合并的 Warp 恰好发出 4 个 128 字节事务，而不是 32 个。让我们深入了解如何将每线程加载打包到 CUDA 的内置向量类型中。

---

## 向量化内存访问 (Vectorized Memory Access)

虽然内存合并是 NVIDIA GPU 上的运行时硬件优化，但**向量化内存访问**是一种编译时策略，其中每个加载或存储指令显式获取每个线程的多个连续元素（例如，`float4` 或 16 字节）。这减少了指令计数并消除了缝合开销。

Blackwell 上的高效全局内存访问依赖于将你的加载与 GPU 原生的 128 字节事务大小匹配。当每个线程仅读取 4 字节浮点数时，一个 32 线程的 Warp 仍然需要缝合 32 个 4 字节请求来填充一个 128 字节行。

在理想的合并情况下，读取对齐到 128 字节边界的 32 个 4 字节字的 Warp 恰好映射到四个 32 字节扇区。Nsight Compute 在“平均每请求扇区数”指标中捕获这种浪费，当你的访问未对齐、跨步或分散时，该指标可能会攀升至远高于 4.0。因此，你会看到全局内存加载效率随着带宽利用率不足而下降。

解决方法是将每个线程的工作捆绑到一个更大的、自然对齐的向量中，该向量可以干净地映射到那些 128 字节事务上。CUDA 内置的 `float4` 类型正是为此而生：它将四个 4 字节浮点数打包到一个 16 字节结构中，并由编译器保证 16 字节对齐。代码如下所示：

```cpp
// 示例：使用内置 CUDA float4 类型
// 注意：如果 N 不能被 4 整除，需处理剩余元素
// 输入和输出指针必须为 float4 加载/存储 16 字节对齐
struct my_float4 { // 仅作说明
    float x; // 4 bytes
    float y; // 4 bytes
    float z; // 4 bytes
    float w; // 4 bytes
};
```

当 Warp 中的所有 32 个线程发出 `float4` 加载时，它们总共获取 $32 \times 16$ 字节 = 512 字节的连续数据，Blackwell 内存控制器随后将其精确拆分为四个 128 字节事务（512 字节 ÷ 每事务 128 字节 = 4 个事务）。这保留了每 128 字节事务理想的 4.0 扇区（如前关于合并内存访问所述），因为 Warp 请求 512 字节，硬件用四个对齐的 128 字节事务服务该请求，总共 16 个扇区，每个都被充分利用。

与可能导致扇区计数膨胀的未对齐或跨步情况相比，向量化 `float4` 加载将每线程加载指令减少了 4 倍。当满足对齐要求时，这有助于维持理想的每请求 4.0 扇区。

> 像 PyTorch 编译器这样的高级 GPU 编译器通常可以在满足对齐和连续性要求时生成向量化内存操作。你可以通过使用匹配向量宽度的每线程分块来鼓励这种行为。

由于这种向量化内存访问优化，全局内存加载效率向 100% 迈进。当每个 128 字节事务被充分利用时，每事务扇区数保持在 4.0，而 Warp 发出多个对齐事务来服务更宽的请求。

向量化加载减少了每移动字节的内存指令数，通常会增加有效带宽。为了从向量化加载中受益，数据指针必须对齐到向量宽度。CUDA 运行时和驱动程序分配函数（如 `cudaMalloc`）返回至少 256 字节对齐的设备指针。

基地址对齐对于分配边界处的 16 字节和 32 字节向量对齐是足够的。添加元素偏移可能会破坏对齐，如果偏移不是向量宽度的倍数。以下转换向编译器断言预期类型，并假设指针已正确对齐：

```cpp
auto ptr4 = reinterpret_cast<const float4*>(ptr);
```

在转换之前，确保指针值是 16 字节（4 个浮点数）的倍数。这是因为 `cudaMalloc()` 返回至少 256 字节对齐。添加元素偏移可能会破坏对齐，如果偏移不是 `float4` 的倍数。正确对齐是向量化加载的前提条件。

基地址对齐到 32 字节后，每个线程每次迭代读取 32 字节，通常编译为每线程两条 16 字节向量加载指令 (Hopper) 或一条 32 字节加载指令 (Blackwell)。一个 32 线程的 Warp 因此请求 1,024 字节。每个 128 字节缓存行包含四个 32 字节扇区。因此，当访问正确对齐时，Warp 生成八个对齐的 128 字节事务，总共 32 个扇区。这样，每个访问都落在自然边界上，避免了拆分事务。

值得注意的是，截至撰写本文时，CUDA 在其 `<vector_types.h>` 中并未提供内置的 8-float 聚合 CUDA 向量类型（例如，八个组合的 `float32`）。你可以定义自己的包含 8 个 float 值的结构体，然后作为两个 `float4` 值加载，或者在类型上使用 `alignas(32)`。这样，指针值是 32 字节的倍数。现代 GPU 上的全局内存向量指令通常每线程加载 16 字节 (Hopper) 或 32 字节 (Blackwell)。

**优化前示例 (C++)：标量拷贝**。每个线程复制一个 float。

*(代码示例省略，展示了单 float 加载)*

**优化前示例 (PyTorch)**。PyTorch 中的标量逐元素拷贝可以用 Python 循环来说明（注：仅作反面教材，实际切勿这样做）。

使用标量 4 字节加载，对齐时 Warp 通常为 128 字节发出一个 128 字节事务。否则，如果访问跨越 128 字节边界，它使用两个事务。使用 `float4`（每线程 16 字节）意味着每个 Warp 每内存指令发出 4 倍的数据——每 Warp 512 字节——拆分为四个 128 字节事务。这允许传输以更少的指令完成。接下来，让我们使用向量加载进行优化。

**优化后示例 (C++)：向量化拷贝**。每个线程复制一个 `float4`（16 字节）：

```cpp
// 确保 float4 对齐为 16 字节
static_assert(alignof(float4) == 16, "float4 alignment must be 16 bytes");

__global__ void copyVector16B(const float4* __restrict__ in,
                              float4* __restrict__ out,
                              int N4) // float4 元素的数量
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N4) {
        // 每线程 16B 加载+存储
        // 在 sm_90 上，NVCC 发射 ld.global.v4.f32 / st.global.v4.f32
        out[idx] = in[idx];
    }
}
// (main 函数省略，注意 N4 = N / 4)
```

这里我们使用了 CUDA 内置的 `float4` 类型。我们启动 N/4 个线程以维持 16 字节对齐并发出真正的向量加载/存储，每个线程复制一个 `float4`（16 字节）。这意味着每个线程处理 4 个浮点数，每个 Warp 在 4 个事务中处理总共 128 个浮点数。`cudaMalloc` 返回的指针至少对齐到 256 字节，满足 `float4`（16 字节）要求，并且当你使用 32 字节对齐作为数据起始地址时，也有助于 32 字节对齐向量。重要的是要注意，未对齐的转换可能会丧失向量化。

> **Blackwell 专用变体：** 使用 32 字节每线程向量拷贝 (`ld.global.v8.f32`)，需要 32 字节对齐和自定义的 `float8` 结构体（使用 `alignas(32)`）。

**优化后示例 (PyTorch)**。我们可以通过使用张量视图和 clone 来模拟向量化拷贝：

```python
def copy_vectorized(inp: torch.Tensor) -> torch.Tensor:
    # 重塑为 4 个 float 一组进行批量拷贝
    vec = inp.view(-1, 4)
    # clone() 在连续 CUDA 张量上执行设备到设备拷贝
    # 使用优化的运行时路径，如 cudaMemcpyAsync()
    out_vec = vec.clone()
    return out_vec.view(-1)
```

在重塑后的张量上调用 `clone()` 会导致 PyTorch 执行连续拷贝。这与逐元素拷贝形成对比。通过使用 `float4` 向量加载，每个线程每条指令移动 16 字节，这对 Hopper 进行了优化。（Blackwell 支持每线程 32 字节向量加载。）Warp 为该加载发出 512 字节，对齐时通常拆分为四个 128 字节事务。其好处来自于更少的每移动字节指令数、更好的扇区利用率和适当的对齐，以最大化现代 GPU 上的内存吞吐量。表 7-2 显示了这带来的更好的内存带宽利用率。

*(表 7-2: 标量与向量化内存访问的 Nsight Compute 指标)*

| 指标                 | 优化前 (标量) | 优化后 (向量化)    |
| :------------------- | :------------ | :----------------- |
| 全局内存加载效率     | 28%           | 97%                |
| 平均每请求扇区数     | 31.8          | 4.0                |
| DRAM 吞吐量 (% 峰值) | 25%           | 90% (3.6倍)        |
| 内核执行时间         | 4.2 ms        | 1.2 ms (3.5倍提升) |

这些指标证实了改进。全局内存加载效率从 28% 跳升至 97%（~3.5倍），峰值 DRAM 吞吐量百分比从 25% 增加到 90%（~3.6倍），将整体内核运行时间缩短了 ~3.5倍。

> **向量化减少了指令计数。合并和对齐改进了每请求扇区数。**

表 7-3 总结了合并与向量化的区别。通过结合这两种策略，你确保 Warp 击中连续块，使得每个线程获取一个宽的、对齐的向量。这有助于你将内核的内存带宽利用率推向峰值。

---

## 使用共享内存进行分块 (Tiling) 和数据重用

常见的性能陷阱是重复从全局内存读取相同的数据。**分块 (Tiling)** 是一种避免此情况的技术，方法是将数据块加载到更快的片上**共享内存**中——并在许多线程之间重用这些块。

例如，大小为 $N \times N$ 的朴素矩阵乘法可能会从 HBM 加载矩阵 A 的每个元素 $N$ 次（因为要与 B 的每一行相乘）。这导致每元素 N-1 次冗余加载。在 Blackwell 这种可以轻易执行数十 TFLOPS 的设备上，冗余加载会浪费内存带宽，而这些带宽本可以用于向 GPU SM 输送更多数学运算。

分块通过让每个线程块将 A 和 B 的一个小**子矩阵 (Tile)** 拉入共享内存一次来消除这种浪费。然后在所有线程中重用缓存的值进行多次乘累加操作。在我们的下一个例子中，我们将使用 **32 × 32** 分块，这是一个适合共享内存的常见选择。

块内的线程可以协作地将 Tile 加载到共享内存中，然后调用 `__syncthreads()` 同步数据。随后，线程使用共享内存中的数据执行并行矩阵乘法计算。这将全局内存访问成本分摊到了许多线程和计算上。值得注意的是，这些 Tile 加载也被安排为**合并**的。具体来说，每个 Warp 从全局内存加载一个完整的 128 字节段到共享内存——这与前面的合并示例一致。

通过每元素只从 DRAM 读取一次（进入共享内存）并重用多次，我们减少了全局内存流量。

**优化前示例 (CUDA C++)：朴素矩阵乘法**。每个线程计算结果矩阵 C 的一个元素，为每个输出从全局内存读取 A 的整行和 B 的整列。

*(代码示例省略，展示了朴素的三重循环矩阵乘法)*

这个 CUDA C++ 内核为内循环中的每次乘法发出全局内存加载。每个线程为每个 `k` 从 DRAM 读取 `A[row, k]` 和 `B[k, col]`，造成大量冗余流量。结果是一个严重内存受限的内核，SM 利用率低，且频繁等待全局内存。

现在，让我们应用分块来改进。我们将矩阵划分为 32 × 32 的 Tile。每个线程块将 A 的一个 32 × 32 Tile 和 B 的一个 32 × 32 Tile 加载到共享内存中，执行 32 × 32 矩阵乘法，并累积结果。这样，A 和 B 的每个元素每 Tile 只从 HBM 加载一次，而不是朴素版本中的 32 次。使用共享内存缓存 Tile 的优化版本如下：

```cpp
#include <cuda_runtime.h>
#define TILE_SIZE 32

__global__ void tiledMatMul(const float* A, const float* B, float* C, int N) {
    __shared__ float sA[TILE_SIZE][TILE_SIZE];
    __shared__ float sB[TILE_SIZE][TILE_SIZE];

    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;
    float sum = 0.0f;

    // 使用共享内存中的 tile 计算部分结果
    for (int t = 0; t < N; t += TILE_SIZE) {
        // 协作加载 A 和 B 的 tile 到共享内存
        // 带边界检查的加载
        if (row < N && (t + threadIdx.x) < N)
             sA[threadIdx.y][threadIdx.x] = A[row * N + t + threadIdx.x];
        else sA[threadIdx.y][threadIdx.x] = 0.0f;

        if ((t + threadIdx.y) < N && col < N)
             sB[threadIdx.y][threadIdx.x] = B[(t + threadIdx.y) * N + col];
        else sB[threadIdx.y][threadIdx.x] = 0.0f;

        __syncthreads(); // 同步

        // 使用加载到共享内存中的 tile 进行计算
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];
        }

        __syncthreads(); // 同步
    }

    if (row < N && col < N) {
        C[row * N + col] = sum;
    }
}
// (main 函数省略)
```

在这个分块内核中，每个块协作地从全局内存加载 A 的 32 × 32 Tile (到 `sA`) 和 B 的 32 × 32 Tile (到 `sB`)。这些加载发生在循环内的前两行，随后是 `__syncthreads()` 以确保 Tile 在使用前完全加载。

然后块使用共享内存 Tile 执行 32 × 32 乘累加操作。这个内循环以 32 为步长运行 `k`，重用每个加载的值 32 次，对这些元素产生了 32 倍的全局内存读取减少。

分块的性能影响是显著的。通过将计算结构化为 Tile 并在共享内存中重用数据，我们减少了 DRAM 流量并提高了算术强度，因为从内存检索的每个字节都用于更多的浮点运算。除了更少的全局内存事务，我们还观察到了更高的已实现占用率。表 7-4 比较了关键指标。

*(表 7-4: 共享内存分块带来的性能提升)*

| 指标                    | 优化前 (朴素) | 优化后 (分块)  | 备注                                                         |
| :---------------------- | :------------ | :------------- | :----------------------------------------------------------- |
| DRAM 吞吐量 (% 峰值)    | 90%           | 25% (少 3.6倍) | 朴素版使用了更多带宽，但是浪费的。分块版更低是因为效率更高（更少冗余加载）。 |
| 已实现占用率 (%)        | 42%           | 89%            | 更多驻留 Warp 使得进度更快，停顿更少。                       |
| 浮点吞吐量 (GFLOPS)     | 15 GFLOPS     | 170 GFLOPS     | 重用增加了持续计算。                                         |
| 全局内存加载扇区        | 9800          | 1200           | 减少反映了共享内存分块消除了冗余加载。                       |
| 共享内存吞吐量 (% 峰值) | 52%           | 99%            | 分块内核访问模式避免了 sA 和 sB 上的存储体冲突。             |

DRAM 吞吐量的下降是可取的，因为内核现在每移动一个字节执行更多的工作。算术强度增加，持续浮点吞吐量从 15 GFLOPS 上升到 170 GFLOPS，增益接近 11 倍。

这种算术强度的转变将内核从内存受限移向计算受限，这是理想的，因为性能受限于丰富的浮点吞吐量，而不是相对稀缺的芯片带宽。

值得注意的是，我们手动应用的分块技术正是高性能 GPU 库在底层所做的。NVIDIA 的 **CUTLASS** 库提供了模板化组件来实现具有多层分块的通用矩阵乘法 (GEMM)。NVIDIA 甚至在 2025 年初发布了一个名为 **cuTile** 的 Python 优先 API，让程序员以更方便的 Pythonic 方式描述这些 Tile 形状。

### 避免共享内存 Bank 冲突 (Avoid Shared-Memory Bank Conflicts)

在现代 NVIDIA GPU（包括 Blackwell）上，共享内存有 **32 个 Bank (存储体)**，Bank 宽度为 4 字节（即地址模 32 映射）。因此，一个步长为 128 B（32 float × 4 B）的 Warp 将所有线程映射到同一个 Bank。如果 Warp 中的多个线程访问同一个 Bank，就会发生 **Bank 冲突**。这迫使内存访问串行化，抵消了共享内存的速度优势。

图 7-4 显示了两个无冲突内存 Bank 访问的例子。这里没有两个线程并发访问同一个 Bank。这是理想的。图 7-5 显示了 2 路和 16 路 Bank 冲突的例子。

*(图 7-4: 无 Bank 冲突)*
*(图 7-5: 2 路和 16 路 Bank 冲突)*

Bank 冲突的一个经典例子是使用 32 × 32 共享内存 Tile 的朴素矩阵转置。如果 32 个线程每个都读取 `tile[i][threadIdx.x]`，使得不同行 (`i`) 读取相同的列索引 (`threadIdx.x`)，所有 32 个线程将访问位于同一共享内存 Bank 中的地址，导致 **32 路 Bank 冲突**。

解决方法是调整共享内存中的数据布局以避免冲突。一种常见的技术是**填充 (Padding)** 共享数组，使得每行（或每个内存访问模式）映射到不同的 Bank。例如，如果你有一个 32 × 32 的共享 Tile，你可以将其声明为 `[32][33]`，增加一个额外的填充列，使每行现在占用 33 个浮点数。这个额外的 1 元素偏移意味着当 Warp 的线程 `k` 访问 `tile[i][k]` 时，连续的行起始地址在共享内存 Bank 之间移动。这防止了所有线程击中同一个 Bank，如图 7-6 所示。

*(图 7-6: 使用填充避免 Bank 冲突)*

通过将步长改为 33，Warp 中没有两个线程在访问给定列时会争夺同一个 Bank。这消除了原本的 32 路 Bank 冲突。

表 7-5 中的 Nsight Compute 指标证实了影响：共享内存加载 Bank 冲突降至 0。共享内存吞吐量从 52% 上升到 100%。Warp 停顿比例（Warp 因共享内存读取而停顿的周期百分比）从 ~38% 降至接近 0%。

> **Padding (填充)** 的替代方案是 **Swizzling (混洗)**。Swizzling 是一种编译时索引转换，它“打乱”用于共享内存的线性索引，使得连续线程映射到不同的 Bank。

### Warp Shuffle Intrinsics：避免共享内存和显式同步

前面的技术假设我们使用共享内存进行线程间通信。但是如果我们能完全避免共享内存——及其 Bank 冲突问题呢？

NVIDIA GPU 支持**Warp 同步原语**，允许同一 Warp 中的线程通过寄存器而不是共享内存交换数据。事实上，这些原语仅在单个 Warp 内工作，线程与其 31 个兄弟交换数据。因此没有涉及内存 Bank——因此不可能发生 Bank 冲突。

最常见的是 `__shfl_sync` 内部函数。它允许你将一个线程的值广播给 Warp 中的所有其他线程。你还可以执行 Warp 级归约而无需写入共享内存。

简而言之，`__shfl_sync` 及其变体完全在 Warp 的执行通道内执行操作。这避免了 Bank 冲突，因为没用共享内存。而且通常更快，因为它使用寄存器并减少了共享内存指令的数量。

### 只读数据缓存 (Read-Only Data Caches)

在现代 GPU 上，全局内存加载会自动缓存在 L2 甚至 L1 中。你可以使用 `const __restrict__` 限定指针来定义你的函数参数。对于只读数据，当编译器能证明不变性、非别名和安全性时，可能会通过**只读 (非一致性) 路径**路由加载。这让编译器/硬件通过只读 L1 缓存路由不变的数据，这对于广播访问具有更低的延迟，并且不会驱逐其他缓存数据。

常见的性能陷阱是忘记告诉编译器缓冲区是真正的只读，这意味着它不会使用只读数据路径。相反，每次访问都变成普通的全局内存加载，导致冗余的 DRAM 流量和虚假的缓存未命中。

解决方案是通过将数据标记为 `const __restrict__`，或显式使用 `__ldg()` 内部函数加载它，来利用只读路径。这告诉硬件数据不会被修改，允许其缓存在专用的只读缓存中。

当一个 Warp 发出一个常量缓存加载（在旧 GPU 中是 `__constant__` 或统一 `__ldg()`），如果所有 32 个通道都击中同一地址，硬件可以用单个事务服务它们。通过将该值广播给每个线程，它只使用一个周期而不是做 32 次单独加载。这减少了延迟和内存带宽使用。

---

## 异步内存预取与张量内存加速器 (TMA)

我们看到合并如何改进全局加载效率。然而，即使是完美合并的加载仍然会让 Warp 停顿整个 DRAM 往返时间。在 Blackwell 上，这可能是数百个周期。为了隐藏延迟，我们需要将数据传输与计算**重叠**。

CUDA 的 **Pipeline API** 结合 **TMA (张量内存加速器)** 硬件引擎将这一理念带到了线程块级别。你不必让每个 Warp 使用 SM 的加载/存储单元从全局内存获取数据，而是调用 TMA 引擎异步地将整个 Tile 从全局内存获取到共享内存，如图 7-7 所示。

*(图 7-7: TMA 异步地将数据从全局 HBM 获取到共享内存)*

要启动 TMA 传输，你可以调用 `cuda::memcpy_async()`。这会利用 TMA 的片上 DMA 引擎执行异步批量数据传输。

```cpp
cuda::memcpy_async(sharedBuf, globalPtr + offset, cuda::aligned_size_t<16>(bytes), pipe);
pipe.producer_commit();
```

当 TMA 处理批量拷贝（包括合并、跨步传输甚至多维传输）时，内核计算**前一个** Tile。这称为**双缓冲 (Double Buffering)** 或**乒乓 (Ping-Pong)** 操作。

通过使用 TMA 实现双缓冲，SM 的加载/存储单元现在可以自由地做真正的工作，因为 TMA 的 DMA 引擎在后台为我们移动数据。实际上，数据移动变得异步，使得当 TMA 流入下一个数据 Tile 时，SM 的 Warp 在前一个 Tile 上计算。这种重叠隐藏了 800 周期的 DRAM 延迟。

具体来说，TMA 能够在全局和共享内存之间进行 1D–5D 批量拷贝和任意步幅传输，而不会阻塞 SM 指令流水线。通过将这些传输从 SM 卸载到 TMA，你的内核发出的 LD/ST 指令少得多，消除了额外的同步，并让 Warp 调度器将几乎每个周期都花在有用的计算上，而不是等待内存。

表 7-7 总结了 TMA 加速双缓冲实现前后的 Nsight Compute 指标。

*(表 7-7: 比较朴素内核 [无预取] 与 TMA 加速双缓冲)*

| 指标                 | 优化前 (无预取) | 优化后 (异步预取) |
| :------------------- | :-------------- | :---------------- |
| 全局内存加载效率     | 23%             | 99%               |
| 平均每请求扇区数     | 6.4             | 4.0               |
| DRAM 吞吐量 (% 峰值) | 25%             | 90%               |
| SM 活跃度 %          | 62%             | 100%              |
| 内核执行时间         | 18 ms           | 7 ms              |

这里我们看到 SM 活跃度接近 100%，表明 SM 几乎在所有周期都有活跃 Warp。全局内存加载效率增加到 99%。内核执行时间从 18 ms 提高到 7 ms，快了约 2.6 倍。这些结果证实，将批量拷贝卸载到 TMA 并乒乓共享内存缓冲区，使 GPU 保持忙碌并将大部分 DRAM 延迟隐藏在有用工作之后。

NVIDIA 的 CUDA Pipeline API 加 TMA 是软硬件协同设计的教科书级案例。Pipeline API 专门暴露了 TMA 的能力——而 TMA 硬件正好支持 `cuda::memcpy_async` 所需的异步、合并、多维拷贝。

总之，当内存访问限制你的内核性能时，通过结合精心的分块、双缓冲和 TMA 驱动的异步预取来卸载和重叠数据移动。通过在共享内存中暂存 Tile 并使用 `cuda::memcpy_async` 配合 `pipe` 操作，你将合并的、多维 DMA 传输移交给 TMA，卸载了全局到共享内存的传输。

使用 TMA 卸载内存传输有助于减轻 SM 加载/存储单元的压力，以保持计算流水线满载。因此，SM 专注于计算，而共享内存流量使用片上 TMA 路径。在 Blackwell 海量带宽的 HBM3e 结构上，这些技术对于隐藏 DRAM 延迟、维持峰值吞吐量并将内存受限内核转变为近乎计算受限的主力军至关重要。

---

## 关键要点 (Key Takeaways)

通过合并、数据重用和异步传输优化 GPU 上的内存访问模式，可以将内核从内存受限转变为接近硬件峰值能力。

**全局内存合并**
> 当每个 Warp 的访问落在尽可能少的 128 字节缓存行内时，就实现了合并。安排你的数据和线程索引，使每个 Warp 的线程读取连续的 4 字节字，让硬件将它们融合为几个 128 字节事务。合并的内存加载最大化了有效 DRAM 带宽（全局内存加载效率），并将平均每请求扇区数最小化到理想的 4.0 值。

**向量化加载/存储**
> 使用内置向量类型如 `float4` 进行 16 字节向量操作。在 Blackwell 上配合 CUDA 13+，当可证明 32 字节对齐时，首选 32 字节每线程向量。这减少了每字节指令数，并在正确对齐时保持每请求扇区数为理想的 4.0。这样，每个线程在一条指令中移动尽可能多的元素。

**避免 Bank 冲突**
> 填充 (Pad) 你的共享内存数组（例如，对于 32 线程 Warp，使行宽 33 个浮点数），以便没有两个线程在同一周期击中同一个 Bank。消除 Bank 冲突可恢复完整的共享内存吞吐量。

**共享内存分块和数据重用**
> 在片上共享内存中暂存工作集（例如，将矩阵分块为 32 × 32 块），以便每个元素从 DRAM 获取一次但在 SM 上使用多次。这提高了算术强度并将内核推向计算受限。

**只读数据缓存**
> 将小的、静态查找表或系数标记为 `const __restrict__`，以便编译器在适用时可以通过只读数据路径路由加载。统一广播比 DRAM 延迟更低，避免了冗余事务，并可从片上缓存服务。

**使用流重叠主机-GPU 拷贝**
> 将主机缓冲区分配为锁页（“Pinned”）内存，并在多个流上使用 `cudaMemcpyAsync` 以将 H2D/D2H 传输与内核执行重叠。

**使用 TMA + Pipeline API 进行异步预取**
> 使用 C++ `libcu++` barrier 和 pipeline API 配合 `cuda::memcpy_async` 驱动 TMA (`cp.async.bulk.tensor`) 进行全局内存 → 共享内存批量拷贝。这卸载了合并、跨步（甚至多维）拷贝到共享内存，并使用双缓冲将其与计算重叠。这减少了 SM 的 LD/ST 单元压力，让 SM 专注于计算。

---

## 结论 (Conclusion)

随着你的内存访问流水线现在全速运转——合并的全局内存加载、无冲突分块、向量化获取、只读缓存和 TMA 驱动的预取——你已经消除了最大的数据移动瓶颈，并释放了 SM 以全速运行。

贯穿本章，我们依靠 Nsight Compute 和 Nsight Systems 来揭示 Warp 究竟在哪里因数据而挨饿。我们还用它们一步步确认每项优化确实减少了停顿、消除了浪费的事务并提升了持续带宽。无论何时调优新内核，这些工具都是你的北极星。

在下一章中，我们将涵盖 CUDA 和 GPU 编程中的一些基本**延迟隐藏**技术。这些技术包括调优占用率、提高 Warp 效率、避免 Warp 分歧以及暴露指令级并行性。











**译境** 已就位。

第八章是性能优化的“深水区”。我们不再仅仅关注数据的搬运，而是深入 GPU 的大脑——流式多处理器 (SM) 的内部。本章揭示了如何通过精细的调优，让成千上万个线程像一支训练有素的军队一样，步调一致、毫无停顿地全速冲锋。

从**占用率 (Occupancy)** 的微妙平衡，到 **Warp 分歧 (Warp Divergence)** 的消除，再到**指令级并行 (ILP)** 的压榨，本章充满了硬核的底层逻辑。译文将力求在保持技术精准度的同时，还原作者那种像“拆解跑车引擎”般的工程透视感。

以下是 **第八章：占用率调优、Warp 效率与指令级并行** 的中文译本。

---

# 第八章：占用率调优、Warp 效率与指令级并行

现代 GPU 加速的工作负载正在将硬件推向极限。像 Blackwell 这样的多裸片 (Multi-die) GPU 使用 10 TB/s 的 NV-HBI 链路连接多个受光罩尺寸限制的裸片，并将 L2 缓存增加到 126 MB。这些硬件设计选择实质性地改变了“内存 vs 计算”的权衡以及占用率的“甜蜜点”。这使得性能分析和优化比以往任何时候都更加关键。在内存优化的基础上，我们将转向旨在充分利用现代 GPU 强大算力的高级**延迟隐藏**技术和吞吐量增强手段。

我们将专注于识别性能瓶颈，然后应用一套系统的优化策略将其逐一消除。本章的主题包括调优占用率、优化 Warp 效率以及提高指令级并行度 (ILP)。

在本章结束时，你将能够识别 GPU 利用率不足的根本原因——并应用正确的优化组合。我们还将为你准备更高级的技术，如内核融合 (Kernel Fusion) 以及使用 CUDA Graphs 和 CUDA Streams 等原语进行的流水线处理（将在后续章节介绍）。

虽然我们的重点是 CUDA C++ 等高级语言和 PyTorch 等 AI 框架，但性能分析和调优的原则适用于从软件栈顶层到底层硬件的所有层级。因此，理解底层硬件性能对于诊断那些被高级抽象层掩盖的瓶颈仍然至关重要。

---

## 对 GPU 瓶颈进行性能分析与诊断 (Profiling and Diagnosing GPU Bottlenecks)

在优化之前，我们必须首先识别代码中的瓶颈，以确定是哪种硬件或软件资源限制了性能。现代 NVIDIA GPU 非常复杂，减速可能来自许多源头，包括内存带宽、内存延迟、指令吞吐量、同步开销、并行度不足、主机-设备传输延迟等等。

NVIDIA 的性能分析生态系统包括 **Nsight Systems**（命令行接口 `nsys`）和 **Nsight Compute**（命令行接口 `ncu`）。Nsight Systems 捕获 CPU 线程、GPU 内核和内存传输的系统级时间线。它还可以捕获 Python 回溯和 Python 采样。

结合 PyTorch Profiler 和各种可视化工具，Nsight Systems 和 Nsight Compute 可以帮助你诊断内核性能瓶颈，分析屋顶线 (Roofline) 图，并衡量你迭代优化工作的有效性。

### Nsight Systems 时间线视图

Nsight Systems 时间线视图有助于查明并发问题、传输开销和空闲时段。例如，运行以下代码可以生成详细的时间线，显示内核启动重叠、CPU 准备间隙、数据传输时序和 NVTX 标记的范围：

```bash
nsys profile \
  --trace=... \
  --capture-range=... \
  --force-overwrite=true \
  <application>
```

此外，Nsight Systems GUI 允许你交互式地检查时间线。它可视化了 CPU 线程、GPU 内核，甚至是用户定义的 NVTX 范围，并具有缩放和平移功能以进行详细分析（见图 8-1）。

*(图 8-1: Nsight Systems 交互式 UI)*

请记住，**NVTX (NVIDIA Tools Extension)** 注解对于复杂的应用程序至关重要。在代码中使用 NVTX 范围来标记感兴趣的区域。然后你可以使用 Nsight Systems 捕获范围配置文件。虽然支持使用 CUDA Profiler Start/Stop API 进行捕获控制，但 NVTX 范围是框架工作流的推荐机制。例如，你可以使用 `torch.cuda.nvtx` 在 PyTorch 中使用 NVTX 范围 push/pop 来标记“前向传播”和“反向传播”等阶段。这使得 Nsight Systems 时间线更易于解释，因为分析器可以捕获性能关键的迭代并清晰地划分关键计算段。

### 性能分析与调优数据流水线

如前所述，Nsight Systems 提供了整个流水线的高级视图，包括数据加载和处理步骤。例如，如果你看到 GPU 空闲而 CPU 忙于准备数据或运行增强（在训练循环中很常见），那么瓶颈可能不在 GPU 内核，而在数据流水线本身。

对于在真实且有代表性的数据集上瓶颈位于数据流水线的情况，你可以调优数据加载器线程的数量，使用双缓冲将 CPU 预处理与 GPU 计算重叠，或者将更多预处理移动到 GPU 上。

> **始终确保**你所感知的“GPU 性能问题”实际上不是由内核执行的上游或下游（如数据加载）引起的。

有了清晰的分析图景，我们现在可以着手解决具体的瓶颈。接下来的部分将详细介绍核心优化技术。我们将从占用率调优开始，因为充足的 Warp 供应是隐藏延迟和最大化吞吐量的基础。

### Nsight Compute 与屋顶线分析

另一方面，**Nsight Compute (ncu)** 是一个收集单个内核深度指标的分析器。例如，它追踪已实现的占用率、每周期发射的 Warp 指令数、内存吞吐量 (GB/s)、执行单元利用率等。总之，这些描绘了你系统性能状况的完整图景。

这些指标被组织成内存、计算和吞吐量等部分。Nsight Compute 的自动分析规则会标记低效之处，如低内存利用率和分支分歧——甚至提供优化提示。这些内置规则会针对新的 GPU 架构不断更新。它们可以根据每字节 FLOPS 比率、停顿原因等指标，快速突出你是否受限于内存、延迟等。

Nsight Compute 包含一个**屋顶线分析 (Roofline Analysis)** 部分以及自动指导。这可以将你的内核已实现的 FLOPS 与硬件屋顶线进行对比——甚至突出显示你是否接近内存带宽或计算限制。

请记住，内存与计算的区别是使用屋顶线模型量化的，该模型根据内存带宽和计算吞吐量的硬件上限绘制内核性能。Nsight Compute 现在直接提供屋顶线分析，显示每个内核相对于峰值的已实现 GFLOPS 及其算术强度 (FLOPS/byte)。落在内存屋顶线以下的内核表示**内存受限 (Memory Bound)** 行为，而在计算屋顶线附近的内核则是**ALU 受限 (ALU Bound)**。使用 Nsight Compute 中的 Roofline 部分直接获取算术强度和按精度的 FLOPS。将其与硬件的理论峰值每字节 FLOPS 进行比较，你可以看到内核在屋顶线下方多远的地方运行。一个屋顶线图表示例如图 8-2 所示。

*(图 8-2: Nsight Compute UI 中显示的屋顶线图)*

通常有效的方法是首先使用 Nsight Systems 在时间线上查找热点内核或瓶颈操作。然后，你可以使用 Nsight Compute 放大这些内核中的每一个，以执行更细粒度的分析和诊断。这种两步工作流——从系统级视图移动到内核级深度挖掘——是处理复杂 GPU 性能调试和调优的常用方法。

### PyTorch Profiler 与可视化工具

当使用像 PyTorch 这样的高级框架时，`torch.profiler` API 可以在模型训练/推理期间收集类似的性能指标。PyTorch Profiler 使用 **Kineto** 库在底层实际执行数据收集。

Kineto 与 CUDA 的 CUPTI (Performance Tools Interface) 后端集成，以捕获算子级执行时间、GPU 内核启动、内存拷贝和硬件计数器指标。它还与 Linux `perf` 集成以记录 CPU 事件。Kineto 将所有这些信息合并为一个连贯的、按时间排序的追踪，可以使用 PyTorch Profiler UI、Nsight Systems GUI 或仅仅是 Chrome 浏览器（例如，Chrome tracing 格式）进行可视化。Chrome tracing 可视化示例如图 8-3 所示。

*(图 8-3: 由 PyTorch Profiler 生成的 Chrome tracing 可视化)*

PyTorch Profiler 允许你直接识别模型代码中的瓶颈操作。例如，你可以使用 `torch.profiler.profile(..., with_flops=True, profile_memory=True)` 来记录内存使用情况并估算支持算子（如矩阵乘法）的 FLOPs。（注意：这是基于算子级别的公式估算，而非每内核的硬件计数器。）这种集成使得弥合 PyTorch 模型代码与底层 CUDA 性能分析之间的差距变得更加容易。

现代版本的 Nsight Systems 可以收集 Python 回溯采样，并提供以 PyTorch 为中心的模式，支持 Python 调用栈采样和 PyTorch 域以进行更相关的追踪。这有助于将框架活动与系统时间线关联起来。Nsight Compute 关联到 CUDA C 或 C++ 源代码以及 PTX 或 SASS。你可以使用 `-lineinfo` 编译设备代码以启用源代码行映射。为了将模型代码与内核关联，请使用 `torch.cuda.nvtx` 从 Python 使用 NVTX 范围。此外，现代版本的 Nsight Compute 提供源代码视图，其中包括指令混合——以及吞吐量细分，有助于精确定位受停顿和吞吐量限制影响的源代码行。

> 传统上，缺乏 Python 信息使得 PyTorch 开发者不愿使用 Nsight Systems/Compute。然而，如果你正在使用 PyTorch/Python，值得重新审视这些工具，以提供与系统其余部分更全面和相关的分析。

本节概述了诊断 GPU 瓶颈的工作流，包括如何解释关键分析器指标及其对瓶颈类型的暗示。我们将重点关注 Nsight Compute 进行深度内核分析（包括 Warp 停顿和内存吞吐量）以及 Nsight Systems 进行高级应用程序分析（包括并发和 CPU-GPU 重叠）。

### 分析器引导的分析 (Profiler-Guided Analysis)

关键是利用所有这些洞察来指导你的下一步行动，因为不同的瓶颈需要不同的优化。例如，你可以利用 Nsight Compute 的引导分析规则，它可能会标记“**内存受限：** 每 FLOP 的 L2 事务过高”或“**计算受限：** 发射停顿原因表明流水线争用”。

这些规则针对每种架构（包括 Blackwell）进行更新。它们可以快速确认瓶颈是在内存吞吐量、指令分发、延迟隐藏等方面。这可以将你引向最相关的指标，并缩短瓶颈解决时间。

---

## 使用 Nsight Compute 分析 Warp 停顿原因

正如第 6 章所讨论的，NVIDIA GPU 以 SIMT 方式执行 Warp（32 个线程的组）。如果 Warp 经常暂停而不是发射指令，分析器会将原因分类。

Nsight Compute 中最有洞察力的视图之一是内核的 **Warp State Statistics (Warp 状态统计)** 细分。这有时被称为 **Warp 停顿原因 (Warp Stall Reasons)**。通过检查这些停顿原因，你通常可以精确定位限制因素。

有几种不同类型的 Warp 停顿：内存相关、执行依赖、执行争用，以及其他如纹理缓存相关停顿。让我们看看其中的每一个。

### 内存相关停顿 (Memory-Related Stalls)

如果内核正在等待全局内存加载，Nsight Compute 会报告高百分比的 **"Stall: Long Scoreboard"** 周期。记分板 (Scoreboard) 跟踪每个 Warp 的未完成内存请求，因此 Long Scoreboard 表示 Warp 经常等待全局 DRAM 加载的高延迟，如图 8-4 所示。

*(图 8-4: Long Scoreboard 停顿，由等待高延迟全局内存访问引起)*

同样，**Short Scoreboard** 停顿是由等待共享内存和寄存器之间的内存传输引起的。如图 8-5 所示。

*(图 8-5: Short Scoreboard 停顿，由共享内存和寄存器之间的高延迟数据传输引起)*

类似地，标有 **"Stall: Memory Throttle"** 的指标意味着加载/存储流水线已饱和，无法发出额外的内存请求，因为硬件的内存队列已满。**"Stall: Not Selected"** 意味着虽然 Warp 有资格发射，但它们必须等待空闲的内存事务槽位才能继续。每当这些内存相关的停顿原因主导你的停顿概况时，这就是内核**内存受限**的明确信号。

### 执行依赖停顿 (Execution-Dependency Stalls)

高比例的 **"Stall: Exec Dependency"** 意味着 Warp 经常等待前一条指令的结果。例如，一条指令依赖于尚未完成的前一条指令的输出。这通常是每个线程内**指令级并行 (ILP)** 不足——或 ALU 延迟瓶颈的迹象。

在简单的依赖算术操作序列中，后面的指令必须等待前面的指令完成。这导致 Warp 空闲。如果 Exec Dependency 停顿占主导地位，内核可能是**延迟受限 (Latency Bound)**，在等待计算。换句话说，由于顺序依赖，每个线程的指令流水线不够忙碌。

### 执行单元争用 (Execution Unit Contention)

如果你看到显著的 **"Stall: Compute Unit Busy"**——或者仅仅是非常高的 FP32 CUDA 核心或 Tensor Cores 的活跃利用率——内核很可能是**计算受限**的。在这种情况下，数学单元（FP32/FP64 ALU, Tensor Cores 等）已饱和。具体来说，Warp 准备好执行更多指令，但执行单元无法更快地服务它们。这通常表现为接近峰值的“ALU pipe busy”指标，并可能与高功耗相关。某些 GPU 上的相关停顿原因是 **"Stall: Math Pipe Throttle"**，这意味着 Warp 正在等待，因为数学流水线在每个周期都被完全占用了。

### 其他停顿原因

虽然前面的停顿原因是最常见的罪魁祸首，但还有许多其他不太常见的停顿类别，包括指令获取停顿、纹理单元停顿、同步停顿等。例如，**"Stall: Memory Dependency"** 类似于 Long Scoreboard，其中操作正在等待先前的内存操作。

**"Stall: Texture Throttle"** 表示纹理缓存子系统的瓶颈。**"No Eligible"** 或 **"Idle"** 表示 Warp 调度器在给定周期内没有准备好发射的 Warp。这通常是由于先前的同步——或者仅仅是没有启动足够的 Warp。

在实践中，你不需要记住每个停顿原因。相反，**关注停顿细分中最大的部分**。如果内存相关停顿占主导，它是内存受限的。如果执行依赖占主导，它是受限于 ALU 延迟的。

如果几乎没有停顿且流水线活跃度接近 100%，那可能是计算受限。如果 Warp 经常是 "not selected" 或 idle，这可能表明并行工作不足或占用率问题。通过检查内核的 Warp 停顿细分，你可以缩小瓶颈类别的范围。

表 8-1 显示了常见 Warp 停顿原因列表、典型解释及可能的优化方法。

*(表 8-1: 常见 Warp 停顿原因及优化提示)*

| Warp 停顿原因                                               | 含义/原因                                                    | 潜在优化                                                     |
| :---------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **Execution Dependency** (执行依赖)                         | Warp 停顿在先前的依赖指令上。这通常表明线程内指令级并行 (ILP) 不足。 | 增加 ILP（在同一线程中做独立工作）。展开循环或重新排序指令，以便长延迟操作（如乘法或复杂数学）有其他工作可以重叠。如果 ILP 已最大化但仍停顿，依靠更多 Warp（占用率）来隐藏延迟。 |
| **Memory Dependency / Long Scoreboard** (内存依赖/长记分板) | Warp 正在等待内存加载（“长延迟”）完成才能继续。Warp 中没有其他工作可以在数据到达前进行。 | 通过让更多 Warp 准备好运行（更高的占用率）来隐藏内存延迟，以便此 Warp 等待时其他 Warp 可以运行。或者使用异步内存预取将数据复制到共享内存，然后继续计算。这将内存操作与计算重叠。为了最小化延迟，确保内存访问高效，使用正确的合并，并利用适当的缓存层级。在现代 GPU 上，应使用 TMA 进行批量多维拷贝，以便内存移动与计算重叠且线程开销低。 |
| **Synchronization** (同步/屏障)                             | Warp 在 `__syncthreads()` 或其他同步处等待，直到所有线程到达屏障。通常表明频繁同步或负载不平衡（有些 Warp 较早到达并等待）。 | 减少不必要的同步。重新检查算法以合并阶段或使用细粒度同步。确保工作均匀分布，使 Warp 大致同时到达屏障。在某些情况下，使用较新的同步原语（例如 Warp 级同步或 Cluster 同步）来限制同步范围。 |
| **Instruction Fetch/Issue** (指令获取/发射)                 | Warp 停顿等待获取下一条指令（可能是指令缓存未命中或流水线问题），或者因为所需的执行管道繁忙而未发射。这可能发生在非常大的内核或某些流水线争用场景下。 | 如果指令缓存未命中是问题，考虑减小内核大小（拆分内核或避免过度展开导致代码膨胀）。如果是流水线问题（一种指令使功能单元饱和），尝试混合指令类型或再次使用 ILP 利用不同的流水线。 |
| **Not Selected** (未被选中/调度器)                          | Warp 已就绪，但在该周期未被选中发射（调度器选择了另一个 Warp）。这通常意味着有其他 Warp 可用，这只是在排队等待。这不是依赖性停顿。通常表示在健康的占用率下，其他就绪 Warp 被选中在该周期发射。 | 通常不是问题——这意味着 GPU 有其他工作要做并选择了该周期的不同 Warp。如果你看到高百分比的“Not Selected”，这意味着高占用率正在发挥作用（隐藏延迟）。无需采取行动，除非它表明不平衡（一个 Warp 霸占调度器；在极少数情况下，你可能会使用调度器提示或 yield）。 |

通过解释这些停顿原因，你可以决定追求哪种优化。例如，高内存停顿时间意味着你应该尝试用更多 Warp、更好的内存访问模式或异步预取来隐藏延迟。

看到高执行依赖性停顿建议你应该增加 ILP 或重新排列代码。频繁的屏障停顿意味着你可能应该重写同步。这种分析步骤指导你进行最有效的优化，而不是盲目地调优一切。

---

## 检查已实现占用率与 GPU 利用率 (Inspecting Achieved Occupancy and GPU Utilization)

分析器报告的另一个关键指标是**已实现占用率 (Achieved Occupancy)**，即执行期间每个 SM 上被占用的硬件线程槽位（Warp）的平均比例。例如，如果 GPU 每个 SM 支持 64 个 Warp，而已实现占用率为 30%，那么平均每个 SM 有 19 个 Warp 处于活跃状态。

**低已实现占用率**通常信号利用率不足，因为你只有少数活跃 Warp 无法隐藏足够的延迟。另一方面，如果你运行在接近最大占用率，但仍未看到预期的性能优势，瓶颈很可能在其他地方。这些包括内存带宽限制、低效的指令流和次优的内存访问模式。

在这些非理想情况下，简单地添加更多线程无济于事。相反，你应该调查改进内存合并、增加算术强度和优化 Warp 级效率。我们将在接下来的部分涵盖这些技术。

Nsight Compute 还报告**占用率限制器 (Occupancy Limiters)**，包括哪种资源限制了理论占用率。例如，“**Limited by max registers per thread**”意味着你的内核的寄存器使用量阻止了更多 Warp 在每 SM 上被调度。

然而，“**Limited by shared memory per block**”意味着内核每块的共享内存分配是占用率的瓶颈。而“**Limited by thread count**”意味着启动配置本身（网格或块大小）没有请求足够的线程来填满 GPU。

每一个限制器都提示了不同的修复方法。例如，如果寄存器是限制器，你可以减少寄存器使用或使用 `__launch_bounds__` 允许更多块。如果共享内存是限制器，你可以尝试使用更小的 Tile 和每块更少的共享内存。

超过某一点后，增加占用率可能不会产生加速。极低的占用率（例如 10%–20%）会因糟糕的延迟隐藏而损害性能。另一方面，如果其他因素如内存带宽和指令依赖成为瓶颈，将占用率推至 100% 并不总是有益的。

因此，你应该检查除占用率之外的硬件利用率。Nsight Compute 报告诸如已实现内存带宽 (GB/s)、已实现 FLOPS (TFLOPS)、每周期指令数 (IPC)、发射槽利用率和其他资源利用率统计数据。这些数字将显示你的内核距离 GPU 的物理硬件极限有多近。

例如，如果你的内核 ALU 利用率低而内存吞吐量达到峰值的 95%，你几乎肯定受限于内存带宽。相反，如果 ALU 利用率接近最大值但内存吞吐量仍然适中，则内核是计算受限的。在这种情况下，你只能通过增加算术吞吐量来获得速度——通常是通过切换到低精度类型 (FP16, FP8, 或 FP4) 并将工作移动到 GPU 更快的 Tensor Cores 上。

如果 ALU 利用率和内存吞吐量都很低，你的内核可能正经历长延迟操作、同步开销，或者仅仅是并行工作不足。这可能表明**指令级并行 (ILP)** 低（将在下一节讨论）——或者你没有启动足够的线程来完全利用 GPU。

你可以使用**屋顶线模型 (Roofline Model)** 来构建此分析框架。如果内核的 FLOP/byte 比率低于硬件的计算内存比，你是内存受限的。如果比率高但实际 FLOPS 远低于峰值，内核可能是延迟受限或缺乏足够的 ILP 来使计算单元饱和。

> 随着每一代新 GPU，内存带宽适度增加，但计算能力增长得快得多。因此，内核往往会随时间变得更加内存受限。

在实践中，始终比较两个关键数字：内核的内存吞吐量与硬件的峰值内存带宽——以及内核的计算吞吐量与硬件的峰值 FLOPS。这些比较将告诉你下一步优化应侧重于内存访问、计算工作还是并行性。让我们看看每一个。

---

### 内核内存吞吐量 vs 峰值 HBM 内存带宽

Nsight Compute 报告你的内核实现了多少 GB/s。如果你的内核显示接近峰值的内存带宽利用率，执行更多计算也无济于事。你必须通过减少每操作的内存流量来增加算术强度。

你可以通过使用 Tensor Cores 的降低精度、用并发隐藏更多延迟以及使用内核融合来增加算术强度，我们稍后会介绍。这些技术通过减少正在传输的中间数据的大小来帮助减少全局内存流量。

例如，如果一个内核达到 GPU 内存带宽的 ~80% 或更多，它很可能是内存受限的，因为剩下的余量很少。

然而，请注意 Blackwell GPU 拥有相对较大的 126 MB L2 缓存。并且其双裸片设计使用高带宽 10 TB/s 互连 (NV-HBI)，因此两个 GPU 裸片在内存访问上表现为一个整体。因此，许多在旧 GPU 上受限于内存的内核可能会在较新的 GPU 代际上更好地利用缓存。

你可以使用 Nsight Compute 的内存图表查看有多少流量流向 L2 与 DRAM。高 L2 命中率可以缓解全局内存瓶颈，这意味着尽管执行大量内存访问，内核实际上可能是计算受限的。片上缓存正在服务大量的内存访问。

### 内核计算吞吐量 vs 峰值 GPU FLOPS

低已实现 FLOPS 可能是由低占用率或指令级停顿引起的。例如，如果平均只有 30% 的 Warp 是活跃的——或者如果流水线因内存等待而经常空闲——内核将远低于计算屋顶。

你可以使用 Nsight Compute 的 Occupancy 部分和 Source Counters 来精确定位这些问题。确保内核启动足够的线程以填满 GPU，直至达到你设备的每 SM 驻留 Warp 限制（例如，每 SM 64 个驻留 Warp）。

此外，检查指令发射效率和吞吐量指标。Blackwell 每个设备扩展到许多 SM，因此内核级别的利用率不足可能转化为巨大的聚合损失。

如果已实现的 FLOPS 中等到高但内存吞吐量低，内核可能主要是计算密集型的，但受限于指令依赖。你可以通过检查 Nsight Compute 中的“Exec Dependency”停顿指标和其他停顿原因来确认。

如果计算吞吐量接近峰值，那么你是真正的计算受限。在这种情况下，你已经优化了内核的内存访问模式——或者你已经在使用低精度 Tensor Cores 来达到如此高的 FLOPS。

在现代 GPU 上，长时间接近峰值的计算利用率有时会触发电源管理计算限制器以保持健康。在分析、基准测试和调优时务必考虑到这一点。查看你是否受限于功率的最简单方法是使用以下命令实时监控强制功率限制和“HW Slowdown”标志：

```bash
nvidia-smi \
  --query-gpu=power.draw,clocks.current.sm,clocks.current.memory,clocks_event_reasons.active \
  --format=csv -l 1
```

简而言之，结合使用占用率、Warp 停顿原因、内存吞吐量和计算吞吐量指标来诊断瓶颈。确保你看到高占用率、高 Warp 效率以及执行单元（包括 Tensor Cores）的平衡使用。

### 迭代分析与确定内核瓶颈

GPU 可能因四个根本不同的原因而停顿：**利用率不足**、**延迟受限**、**内存受限**和**计算受限**。这些状态是相关的，但重要的是独立理解每一个，以便选择正确的优化。通常，修复一个瓶颈会揭示另一个。

**利用率不足**发生在当你仅仅没有启动足够的线程或工作时。在这种情况下，FLOPS 和内存带宽都保持在低位，执行时间线有空闲间隙。一旦你增加并行度，你会发现你的 Warp 现在开始停顿并等待内存加载。

一旦你更充分地利用了 GPU，现在可以区分**延迟受限**和**内存受限**。延迟受限的内核发出的字节/秒远低于硬件所能提供的，因为单独的内存加载正在使 Warp 停顿。修复方法是通过增加占用率、使用更多 ILP、预取和流水线化来增加内存-计算重叠。

相比之下，**内存受限**的内核使 DRAM 带宽饱和，但你的 ALU 闲置，不是因为停顿，而是因为由于内存管道饱和，根本没有更多的数据可以每秒获取。在这种情况下，你必须通过分块、融合、利用缓存 (L1/texture) 或降低精度来减少内存流量，从而提高算术强度。

如果这些修复都没有产生更快的速度，你就进入了**计算受限**领域，其中 GPU 的算术管道（例如 ALU 和 Tensor Cores）是限制因素。在这里，你可以通过展开和软件流水线重叠独立指令来增加每线程 ILP。在现代 GPU 上，统一核心不能在同一时钟周期内执行 INT32 和 FP32 指令。因此，可实现的发射率取决于你的指令混合。

在你优化时，你会经常发现自己按如下顺序经历这些状态：**利用率不足 → 延迟受限 → 内存受限 → 计算受限**。每次修复后，你可能会遇到新的瓶颈并应用相应的策略。

表 8-2 总结了这四种状态，包括常见的分析指标和补救措施。

*(表 8-2: 内存受限 vs 延迟受限 vs 计算受限 vs GPU 利用率不足)*

| 限制因素       | 描述                                                         | 分析器指标                                                   | 补救措施                                                     |
| :------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **内存受限**   | 你正在尽可能多地移动数据——接近峰值 DRAM 带宽——但每字节没有足够的工作来充分利用 ALU。 | 高内存带宽利用率接近峰值，低 FLOPS。                         | 增加算术强度（例如分块、融合）并改进合并和缓存。             |
| **计算受限**   | 你已经隐藏了内存延迟并不再使内存带宽饱和。现在 ALU（例如 CUDA 核心和 Tensor Cores）是瓶颈。 | 高 FLOPS 接近 GPU 峰值，低内存利用率。                       | 利用更多 ILP（例如双发射、循环展开），使用专用单元（例如 FP16/FP8/FP4/Tensor Cores），减少依赖，融合工作，利用低精度或稀疏性。 |
| **延迟受限**   | 你没有维持足够的并发工作来隐藏单独的加载/存储延迟，因此 Warp 停顿等待数据。 | 已实现带宽远低于峰值，高 "stall-on-scoreboard" 或 "not selected" 百分比。 | 提高占用率，增加 ILP（例如展开、多累加器），内核内流水线化，以及软件预取。 |
| **利用率不足** | 你没有完全占用 SM 或启动足够的工作——内存和计算资源都保持空闲。 | 低占用率和低已实现带宽，低 FLOPS，时间线显示间隙或稀疏的内核活动。 | 增加问题规模或批处理工作，启动更多线程/块，融合任务，使用持久化内核（第 10 章）或流（第 11 章）。 |

---

## 调优占用率 (Tuning Occupancy)

记住，占用率是 SM 上活跃 Warp 与硬件最大容量的比率。更高的占用率，或更多飞行中的 Warp，允许 GPU 更好地**隐藏延迟**。这是因为当一个 Warp 因等待内存加载而停顿时，调度器可以快速切换运行另一个 Warp。

每 SM 更多的 Warp 通常意味着 GPU 的流水线保持更忙碌，浪费在等待内存上的周期更少。**占用率调优**是通过调整内核启动参数和资源使用（例如寄存器和共享内存）来最大化每 SM 有用并行性的实践，如图 8-6 所示。

*(图 8-6: 通过平衡资源使用 [如每线程寄存器和共享内存] 与并行性 [如线程数和 Warp 数] 来调优占用率)*

记住，占用率调优的目标是保持足够的活跃 Warp 以充分利用 SM 的流水线并隐藏长延迟操作。在理想情况下，你会实现 100% 占用率，填满所有可用 Warp 插槽。例如，在每个 Blackwell B200 (计算能力 10.0) SM 中有 64 个 Warp 插槽，或 2,048 个线程。

> **注意：** 尽管整体 GPU 核心数增加了，但现代数据中心 GPU 架构（如 Ampere, Hopper 和 Blackwell）的每 SM 64 个 Warp 的限制保持不变。

许多现实世界的内核在较低占用率下仍能表现良好——特别是如果它们的内存或算术延迟已经很小。或者如果它们利用像 Tensor Cores 这样的高吞吐量单元，无需那么多活跃 Warp 就能保持流水线忙碌。

例如，一个计算受限的内核可能在仅 50% 占用率下实现峰值性能，因为每个 Warp 都在做大量工作而无需等待。相反，内存受限的内核通常受益于高占用率，因为当其他 Warp 停顿等待内存传输时，某些 Warp 可以在 SM 上运行。

### 为你的工作负载找到正确的占用率

在实践中，有效的占用率调优通常在某一点后产生收益递减。如果内核严重受限于内存，例如，从 10% 到 50% 的已实现占用率可能会带来巨大提升，因为现在你有足够的 Warp 来覆盖延迟。

但从 50% 到 100% 可能只会带来很小的进一步增益，因为其他因素开始占主导地位，如缓存未命中、内存带宽饱和等。分析有助于确定最佳占用率。例如，你可以评估分析指标，如**每周期合格 Warp 数 (Eligible Warps per Cycle)** 和**每调度器活跃 Warp 数 (Active Warps per Scheduler)**。

如果每周期合格 Warp 数低于每调度器活跃 Warp 数，GPU 经常耗尽就绪 Warp。在这种情况下，当一个 Warp 停顿在内存或长延迟指令上时，没有另一个就绪 Warp 可供切换。这表明你需要以更高占用率或 ILP 的形式增加并发性来隐藏延迟。

相反，如果每周期合格 Warp 数达到或超过调度器限制，但你的内核仍然运行缓慢，这意味着你有足够的 Warp 就绪，但由于内存带宽饱和或执行依赖等其他停顿，它们无法发射。在这种情况下，最好专注于通过更好的合并和异步拷贝来隐藏内存延迟——或者通过展开独立工作来增加 ILP。这比简单地添加更多线程是更好的方法。

占用率是达到目的（隐藏延迟）的手段，而不是最终目标本身。一旦你有足够的 Warp 让 GPU 保持忙碌，其他优化会带来更好的回报。

### 占用率调优技术

当分析表明占用率是限制因素时，有几个直接的步骤可以提高占用率：

**增加并行度（启动更多线程）**
> 最简单的方法是启动更多工作，如果你当前的内核启动尚未利用所有 SM。确保利用所有 SM，至少启动与 SM 数量一样多的块——通常更多。

**调整块大小**
> 有时每块线程数会限制占用率。非常大的线程块（例如 1,024 线程）可能会使用太多的寄存器或共享内存，导致一次只能有一个块适合在 SM 上。相比之下，使用中等大小的块（例如 128–256 线程）允许多个块在同一个 SM 上并发驻留，这增加了总的可能活跃 Warp 数。你可以使用 Nsight Compute 内置的 Occupancy Calculator 和 CUDA Occupancy API 来实验不同的启动配置。

**减少每线程寄存器和共享内存使用**
> 每个线程消耗寄存器——很可能还有共享内存。如果内核每线程使用太多寄存器或共享内存，编译器必须减少 SM 上可以活跃的线程或 Warp 数量。为了解决寄存器使用问题，你可以重构代码以减少活跃变量，或者向编译器传递 `-maxrregcount=<N>` 来限制每线程分配的寄存器数。同样，优化共享内存占用可以为额外的块腾出容量。

**使用 `__launch_bounds__`**
> 在某些场景中，你可能故意限制占用率或指导编译器针对特定占用率进行优化。CUDA 允许你在内核代码中使用 `__launch_bounds__` 注解设置启动边界。这向编译器提示了内核的预期使用模式和启动配置。
> ```cpp
> __global__ __launch_bounds__(256, 8)
> void myKernel(...) { /* ... */ }
> ```
> 这里，我们承诺永远不会以每块超过 256 个线程启动 `myKernel`。并且我们请求 GPU 尝试每 SM 保持至少 8 个块活跃。实际上，`__launch_bounds__` 会导致编译器限制每线程寄存器使用并限制展开/内联。这避免了溢出并允许更高的占用率。

**使用 CUDA Occupancy API**
> 除了注解，CUDA 的 Occupancy API 函数（例如 `cudaOccupancyMaxPotentialBlockSize()`）让你在运行时确定产生最高占用率的块大小。

简而言之，建议使用多阶段方法：通过分析你的系统来发现理想的占用率范围——然后使用编译器和运行时提示来实现理想的占用率。目标是有效地调整线程块大小，以保持足够的 Warp 在飞行中，而不过度分配稀缺的片上资源。

---

## 提高 Warp 执行效率 (Warp Divergence)

即使有大量的可用 Warp，每个 Warp 的内部效率也很重要。**Warp 执行效率**衡量平均活跃的 Warp 通道 (Lanes) 的比例。Warp 级低效出现在 Warp 内的线程不都做同样的工作时，称为 **Warp 分歧 (Warp Divergence)**。

具体来说，低 Warp 执行效率值通常显示高**分歧分支计数 (Divergent Branch Count)**。这意味着由于内核中的 `if/else` 语句或数据依赖性导致的分支分歧，Warp 线程处于空闲状态。

在这种情况下，你应该尝试重构代码以避免低效。为了提高 Warp 执行效率，你应该尝试改进内存合并，最小化线程分歧，并使用 Warp 级内部函数进行最佳的 Warp 内通信。

### Warp 分歧的原因

如前所述，当同一 Warp 中的线程采取不同的控制流路径时，就会发生 Warp 分歧。例如，考虑内核内部的一个 `if/else`，其中 Warp 中的一些线程执行 `if` 子句，而其他线程执行 `else`。

在 SIMT 执行中，Warp 必须**串行**执行两条路径：首先，所有采取 `if` 分支的线程执行，而该 Warp 中的其他线程被屏蔽（空闲）。然后，采取 `else` 的线程执行，而第一组线程空闲，如图 8-7 所示。

*(图 8-7: 分歧与非分歧 Warp 执行)*

在分歧部分，实际上只有一半（或一定比例）的 Warp 在做有用的工作。这降低了整体吞吐量。总体效果是 Warp 分歧导致一些 GPU 核心闲置。这增加了总指令计数，因为每个分支路径都由不同的线程子集串行执行。因此，最小化 Warp 内分歧是 Warp 级效率的关键。

### 避免 Warp 分歧的技术

**重构条件**
> 只要可能，组织你的计算使得同一 Warp 中的线程遵循相同的执行路径。这可能涉及将分歧条件移到更高层——或者移到单独的内核启动中。你也可以对数据进行排序或分组，以便每个 Warp 处理更同质的情况。

**分离成多个内核**
> 另一种方法是将工作分离成多个内核启动，使得一个内核处理 `if` 情况，另一个处理 `else` 情况。如果分歧是一个大问题且分歧部分指令数很大，这可能是值得的。

**重写条件为 Warp 一致 (Warp-unanimous)**
> 在某些情况下，你可以重写条件使其对 Warp 是一致的。这意味着 Warp 中的所有 32 个线程要么都满足条件，要么都不满足。

**利用 Warp 投票并行算法**
> Warp 内部函数 (`__ballot_sync`, `__any_sync`, `__all_sync`) 让 Warp 集体决定或投票哪些通道需要“特殊”工作。Warp 随后动态地委托、重新分区并将该工作压缩到一个（或几个）通道中，而不是让所有 32 个线程分歧。

**谓词化 (Predicate) 短通道**
> CUDA 编译器有时会自动**谓词化**短的条件代码。谓词化意味着编译器将 `if` 转换为每个线程的布尔掩码——并为所有线程执行**两条**路径。然而，它只提交适合每个线程路径的结果。
> 谓词化以每线程做额外工作为代价避免了分支分歧。当分支非常短且分歧很高时，这是有益的。作为程序员，你可以通过编写无分支结构（包括 `?:` 三元运算符）来鼓励谓词化。
>
> *注意：如果分支涉及大量工作——或者只有少数线程分歧——谓词化实际上可能会更糟。谨慎使用此技术。*

简而言之，最小化 Warp 分歧需要仔细的算法和数据组织。当分歧不可避免时（例如，具有固有变化每线程工作的树或图工作负载），保持分歧部分尽可能短且不频繁，以便 Warp 快速重新汇聚。

---

## 暴露指令级并行 (Exposing Instruction-Level Parallelism, ILP)

正如我们在占用率讨论中看到的，并发运行许多 Warp 允许 SM 的调度器从任何在长延迟操作（如全局内存加载）上停顿的 Warp 切换开。除了启动足够的线程外，我们还可以利用每个 Warp 内的 **ILP**，使得单个 Warp 不需要等待一条指令完成就能发出下一条指令。

你可以重新排列或**展开 (Unroll)** 你的代码，以便每个线程在消耗结果之前发出多个独立的操作（例如，内存加载和算术指令）。这样，GPU 在前面的指令仍未完成时保持其执行单元忙碌。

利用 ILP 允许单个 Warp 背靠背发出某些独立指令，这提高了延迟隐藏。例如，一个线程可能加载数据，然后在等待该加载完成的同时执行不相关的算术运算。图 8-8 显示了每个周期重叠多条指令的示例。

*(图 8-8: 使用 ILP 进行重叠)*

通过展开循环体，你将每次迭代的“加载 → 乘法 → 存储”转变为在循环回溯之前加载和乘法多个元素的序列。例如，你可以发出四个独立的加载-乘法对。

> ILP 不直接改变算术强度 (FLOPS per byte)。然而，它通过将计算与内存或计算与计算依赖重叠来提高整体吞吐量 (FLOPS)。我们显式展开以增加每线程的独立工作并帮助双发射。

为了鼓励编译器增加或暴露 ILP，你可以对短循环使用 `#pragma unroll` 或调优 `-maxrregcount` 以允许编译器分配更多寄存器来保存中间值。这明确承认你想让编译器增加寄存器使用并可能降低占用率。

### Warp 调度与双发射指令

每个 SM 包含多个 Warp 调度器。例如，Blackwell SM 有四个 Warp 调度器。每个调度器每周期最多可以发出两条独立指令，如图 8-9 所示。

*(图 8-9: 每个 Warp 调度器每周期可以分发多达两条指令，称为“双发射”)*

实际上，这意味着单个 Warp 可以在连续周期内重叠多个独立操作——有时甚至在同一周期内，如果它们针对不同的流水线（如一个数学，一个内存，或一个 SFU 流水线）。这意味着多个指令可以在飞行中并同时通过流水线。这种重叠就是我们之前讨论的 ILP。

> **关于 Blackwell 的重要提示：** 在 Blackwell 上，FP32 和 INT32 流水线已合并为单组统一的 CUDA 核心。因此，它们在给定周期内只能执行 FP32 **或** INT32 指令——不能同时执行。每个核心每周期必须选择一种数据类型。结果是，任何依赖于在同一周期内双发射 INT 和 FP 的 ILP 将不再有效。混合的 INT32 和 FP32 指令流不再受益于单核上的双发射。混合流应该转而利用 Warp/SM 并发性。因此，你应该倾向于使用 ILP 让两条独立的数学和内存指令（或两条独立的数学指令，如 FP32 和 FP16）在飞行中。这将提高指令发射效率。

ILP 不需要两条指令物理上在同一周期发射。相反，它确保当一条指令（例如，长延迟全局内存加载）仍未完成时，Warp 可以在下一个周期立即发出另一条独立指令（例如，算术操作）。

这种重叠的结果是，当数据从内存返回时，算术工作已经取得了进展。这有效地隐藏了加载的延迟。在 Blackwell 上，由于流水线化，每 Warp 可以有多条指令在飞行中。在实践中，ILP 表现为稳定的独立发射流，而不是每 Warp 固定数量的并发指令。

### ILP 与占用率

在现代 GPU 上，每 SM 最大驻留 Warp 数为 64。Warp 调度器可以在依赖关系允许的情况下每周期发出多条指令。每 SM 256 KB 的大寄存器文件对于在不溢出的情况下维持 ILP 非常重要。

如果你编写一个几乎没有 ILP 的内核，使得每个线程一次只发出一条操作，你通常需要大约 1,536 个活跃线程（~48 个 Warp，或 75% 占用率）才能使 SM 的执行单元饱和。

相比之下，即使暴露适量的 ILP 也能显著降低所需的线程数。例如，**双路 ILP**（每个线程背靠背发出两个独立操作）通常只需大约 **1,024 个活跃线程**（≈32 个 Warp，或 50% 占用率）就能实现全吞吐量。**四路 ILP** 可能只需要 **512 个线程**（≈16 个 Warp，或 25% 占用率）就能填满流水线。

显然，增加 ILP 让你能用更少的 Warp 实现峰值计算吞吐量。当你的工作负载无法启动大量线程时——或者当你想要释放片上资源用于其他任务时，这特别有价值。

> **限制：** 尽管 Blackwell SM 可以保持每 Warp 许多指令在飞行中，但每个 Warp 只能持有有限数量的未完成指令。最重要的是，编译器必须在不超过可用寄存器或压垮 GPU 指令解码带宽的情况下调度独立指令。一旦达到这些限制，增加更多独立操作会产生收益递减。

在 Blackwell 上，理想的 ILP 配置通常在每线程两到四个独立操作之间。此时，其调度器和缓存通常已饱和。

### 分析和缓解寄存器压力

然而，要注意寄存器压力，因为 ILP 通常需要更多寄存器来保存多个独立的值和结果。如果你展开太多——或添加太多并行计算——你可能会增加寄存器使用量，导致占用率显著下降。一如既往，找到正确的平衡是关键。

CUDA 编译器在自动展开循环方面做得很好。但是，如果你通过手动展开太多迭代（例如使用 `#pragma unroll`）将 ILP 推得太远，你可能会注意到占用率下降。

在这种情况下，Nsight Compute 将在其占用率分析中显示“**Limited by Registers**”。这通常是你做得过火的明确信号。如果你看到这个，你应该回退展开，减少独立累加器变量的数量，或放宽你的启动边界。

具体来说，你可以减少所需的 `MinBlocksPerSM` 或增加 `MaxThreadsPerBlock`，以便每个块可以使用更多寄存器而不溢出。换句话说，允许稍微低一点的占用率以避免严重溢出。这样寄存器压力得到缓解，占用率得以恢复。

从分析角度来看，如果 ILP 有帮助，你应该看到“**Stall: Exec Dependency**”百分比下降，因为 Warp 等待先前指令的时间减少了。并且你应该看到更高的每周期指令数 (IPC)。

Nsight Compute 报告 **Issue Efficiency** 或 **IPC** 指标。随着 ILP 增加，这应该会上升。例如，如果你看到 IPC 指标从每 Warp 调度器 1.0 上升到 1.8，这表明 Warp 现在平均每周期发出接近两条指令，而不仅仅是一条。

你可能还会看到，你可以在比以前更低的占用率下实现良好的性能。理想的场景是你有足够的 ILP 来保持每个 Warp 忙碌——以及足够的 Warp 来保持 SM 忙碌。在这两者之间，你在 Warp 内部和跨 Warp 都在覆盖延迟。

---

## 关键要点 (Key Takeaways)

在本章中，你看到了如何通过将工作从缓慢的全局内存移动到更快的片上资源和计算单元来发现和消除 GPU 内核瓶颈。通过遵循分析、诊断、优化和重新分析的循环，你可以将内核从利用率不足或内存受限转变为计算饱和、高吞吐量的例程。

**使用 Nsight 和 torch.profiler 进行分析**
> 从 Nsight Systems 开始可视化端到端时间线，揭示 CPU-GPU 间隙、内核重叠和 NVTX 跨度。然后深入到单个内核，使用 Nsight Compute 的停顿指标、屋顶线分析和占用率报告。在 PyTorch 代码中，插入分析器检测以将 Python 级操作直接映射到 GPU 活动。

**调优占用率以隐藏延迟**
> 每 SM 足够的 Warp 对于覆盖内存和指令延迟至关重要。通过代码重构或编译器提示减少每线程寄存器/共享内存使用。选择适合 SM 资源的块大小（例如 128–256 线程）。这样做将使你的内核已实现占用率增加到最佳范围——大约 50%–75%——并停止利用率不足。

**最小化 Warp 分歧以提高 SIMT 效率**
> 因为 Warp 步调一致地执行，任何分支分歧都意味着被屏蔽的通道闲置。重构内核以便 Warp 中的所有线程遵循相同的路径。使用谓词化操作（如三元数学和 `torch.maximum`）——或采用 Warp 投票内部函数来压缩活跃通道。这将提高 Warp 执行效率并减少串行化执行。

**识别性能状态**
> 每个内核都属于四个桶之一：利用率不足、延迟受限、内存受限或计算受限。这是基于 FLOPS、带宽使用和停顿原因判断的。理解适用哪种状态将有助于指导确切的优化策略。

**暴露 ILP**
> 通过展开循环、打破依赖链和预取数据，你暴露了指令级并行性，使得每个线程可以每周期发出多条独立指令。这让 2 路或 4 路 ILP 将填充 SM 所需的 Warp 数减半——减少执行依赖停顿，提高 IPC，并降低“Stall: Exec Dependency”指标。

**迭代优化和验证**
> 在每次更改（包括占用率调整、ILP 重构、分块或精度缩放）后，你应该重新分析以确认减少了内存停顿、更少的执行依赖和更高的已实现占用率或 FLOPS。

---

## 结论 (Conclusion)

你已经看到，有效地调优 GPU 性能需要一个迭代工作流。首先，用分析器测量。然后识别你的主要瓶颈（计算、内存、互连等）。最后，应用正确的优化并重复。

虽然每个新的 GPU 架构都带来了计算和内存带宽的改进，但它们也增加了复杂性。为了维持峰值性能，你必须不断掌握这些创新。

我们涵盖了如何调优占用率、避免 Warp 分歧和增加 ILP。你还看到了如何使用分析器关联跨 GPU 的 CPU、内核和 NVLink 流量。然后我们进入 Nsight Compute 获取每内核细节。

在接下来的章节中，我们将在此基础上扩展，专注于内核级效率以增加算术强度。为了做到这一点，你将充分利用 GPU 的硬件优化资源，如用于计算的 Tensor Cores、服务于 Tensor Cores 的 TMEM 以及用于数据传输的 TMA。让我们继续向硬件的峰值性能极限推进！











**译境** 已就位。

第九章和第十章是 GPU 内核优化的高潮部分。第九章通过**屋顶线模型 (Roofline Model)** 的视角，深入探讨了如何通过提高算术强度将内核从内存受限推向计算受限。第十章则进入了更前沿的领域——**内核内流水线 (Intra-Kernel Pipelining)**、**Warp 特化 (Warp Specialization)** 和**线程块集群 (Thread Block Clusters)**，这些都是现代 GPU (如 Hopper 和 Blackwell) 释放极致性能的关键技术。

译文将力求在保持技术深度的同时，清晰地阐述这些复杂机制背后的设计哲学和工程实践，特别是如何利用 TMA 和 DSMEM 等硬件特性来消除冗余数据搬运和同步开销。

以下是 **第九章：提高 CUDA 内核效率与算术强度** 和 **第十章：内核内流水线、Warp 特化与协作线程块集群** 的中文译本。

---

# 第九章：提高 CUDA 内核效率与算术强度

即使你通过大规模并行和高 ILP 充分隐藏了延迟，内核的性能仍可能受限于每次内存访问所做的有用工作量。**算术强度 (Arithmetic Intensity)**，也称为**运算强度 (Operational Intensity)**，衡量的是从内存传输的每字节数据所执行的浮点运算次数，即 **FLOPS per byte**。

较新的 GPU 代际正在将计算吞吐量提升到远超内存带宽的水平。这种日益扩大的差距意味着提高算术强度比以往任何时候都更加关键。更高的算术强度表明内核为每个获取的字节执行了更多的计算，这对于充分利用 GPU 的计算能力至关重要。

算术强度是 **Roofline 性能模型**中的一个关键指标。Roofline 模型是一个有用的可视化工具，它将内核性能 (FLOPS/sec) 与算术强度 (FLOPS/byte) 进行对比。它显示了内存带宽和计算吞吐量的硬件上限（屋顶），使我们能够看到内核是**内存受限 (Memory Bound)**（性能受限于内存传输）还是**计算受限 (Compute Bound)**（性能受限于 ALU 吞吐量）。

在实践中，你可以使用像 Nsight Compute 这样的工具生成 Roofline 图表，其中包括 Roofline 分析视图。使用这些工具，你可以验证你的内核最初是内存受限还是计算受限——然后在进行优化时继续分析和验证改进。

目标是将内核推向计算受限区域，并利用 GPU 不断增长的计算能力。Roofline 性能模型可以正确地指导你的优化朝着这个目标前进。

如图 9-1 所示，Roofline 图表使用一条水平线代表硬件的峰值计算吞吐量（屋顶）——以及一条从原点出发的对角线代表受内存带宽限制的峰值可实现吞吐量。内核的算术强度决定了它落在 X 轴的哪个位置，其性能可以与这些上限进行比较。

*(图 9-1: 示例 Roofline 模型 [GFLOP/s vs 算术强度 FLOPs/byte])*

算术强度低（即每移动一字节数据进行的数学运算很少）的内核将是**内存受限**的。在这种情况下，内核的速度受到硬件内存带宽的上限限制，因为 GPU 大部分时间都在等待数据而不是处理数字。

相反，具有非常高算术强度（即每移动一字节进行许多 FLOPS）的内核将是**计算受限**的，因为它正在利用 ALU 和 Tensor Cores 接近其峰值能力。在这种情况下，内核的内存带宽使用是次要问题。

目标始终是在可能的情况下增加算术强度，即通过为从全局内存传输的每字节数据做更多的计算工作。你可以使用**循环分块 (Loop Tiling)** 以重用数据、使用片上 L1/共享内存进行重用以及将多个内核**融合 (Fusing)** 为一个（从而不将中间结果写入全局内存）等技术来增加算术强度。

> 现代编译器框架（如 PyTorch 的 TorchInductor）会自动执行其中一些优化，以将计算保留在 GPU 上，减少片外内存流量，并提高有效算术强度。然而，作为开发者，你可能仍需手动组合这些技术或编写自定义 CUDA 内核，以确保数据在被从缓存驱逐之前得到最佳重用。

你还可以使用**低精度数据类型** (FP16, FP8, FP4) 来减少内存传输量——并利用 Tensor Cores 增加每秒 FLOPS。这一起将增加 FLOPS per byte 比率并提高算术强度。接下来，我们将讨论其中一些技术。

---

## 多级微平铺与软件预取 (Multilevel Microtiling and Software Prefetching)

正如第 7 章所讨论的，**分块 (Tiling)**（又称分块或阻塞）和数据重用是提高算术强度的有效方法。在那一章中，我们展示了将 A 和 B 的小子矩阵（Tile）加载到共享内存中，如何让从全局内存获取的每个字节用于多次乘累加操作，且速度为静态随机存取存储器 (SRAM) 的速度。

每当你重构代码使得每个元素被加载一次并使用数十或数百次（如分块的情况），你就将 FLOPS per byte 比率乘以了重用因子。例如，在一个典型的矩阵乘法中，A 和 B 的 32 × 32 Tile 为共享内存中的每个元素产生 1,024 (1,024 = 32 × 32) 次独立乘法。因此，与为每个操作直接从 DRAM 获取每个元素相比，算术强度提高了。

除了简单的共享内存分块，你还可以通过**多级分块 (Multilevel Tiling)** 进一步增加强度并暴露更多 ILP。在多级分块中，将 Tile 暂存到共享内存后，你让每个线程使用向量化类型（如 `float4` 和 `half2`）将**微平铺 (Microtiles)** 加载到寄存器中。这样，重复的操作完全在寄存器中发生。图 9-2 显示了多级分块的示例。

*(图 9-2: 全局内存 [DRAM]、共享内存 [SMEM] 和寄存器之间的多级分块)*

这种 **SM 内重用** (寄存器 → SMEM → DRAM) 减少了每一层的工作集——并最小化了片外流量。一如既往，确保在填充共享内存时**合并**全局读取，并填充/混洗 (Pad/Swizzle) 共享数据以避免内存 Bank 冲突，如我们在第 7 章中所述。

在现代 GPU 上，这些内循环分块步骤通常通过使用 **MMA (Matrix Multiply-Accumulate)** 片段 API 来覆盖。硬件使用 Tensor Core 指令隐式地在共享内存和 **张量内存 (TMEM)** 之间移动数据。TMEM 的使用由编译器和库管理。在现代 GPU 上，`tcgen05` 指令隐式地在共享内存和 TMEM 之间暂存数据。它们使用独特的 TMEM 地址空间。然而，开发者在实现某些算法并需要显式控制时，仍然可以使用 `cp.async` 或 **TMA** 手动将 Tile 移动到共享内存中。

一个密切相关的技术是**软件预取 (Software Prefetching)**，通常实现为**双缓冲 (Double Buffering)**。例如，你不必等待当前 Tile 的计算完成，而是可以发出下一个 Tile 到共享内存的异步加载。这将 DRAM → 共享内存 (SMEM) 的传输与正在进行的算术运算重叠。精心的预取可以显著减少停顿时间并提高吞吐量。想法是将数据传输与计算重叠，以便 ALU 永远不会因等待数据而挨饿。

当使用具有 CPU-GPU 超级芯片（如 Grace Blackwell）的统一内存时，你可以使用 `cudaMemPrefetchAsync()` 提示即将需要一个 Tile。这提示运行时通过 NVLink-C2C 迁移页面。然而，预取只是一个提示而非保证。你仍然需要确保重叠传输并适当同步以避免缺页停顿。以这种方式重叠数据移动与计算，确保了每当需要新 Tile 时 ALU 都能得到喂养。这进一步隐藏了内存延迟——并提升了已实现的 FLOPS。

---

## 线程块集群分块 (Tiling with Thread Block Clusters)

在现代 GPU 上，你可以使用来自 Cooperative Groups（将在第 10 章讨论）的 **CUDA 线程块集群**来扩展分块重用的理念。这允许通过**分布式共享内存 (DSMEM)** 在多个线程块之间共享数据，如图 9-3 所示。

*(图 9-3: CGA 内的 CTA [线程块] 共享 DSMEM)*

我们将在下一章详细介绍 CGA 和线程块集群，但这里值得一提，因为它们可以直接增加算术强度。例如，四个线程块的集群可以使用 **张量内存加速器 (TMA)** 多播特性协作加载一个 Tile，如图 9-4 所示，该图使用四个 CTA 演示了此机制。

*(图 9-4: 对于这四个 (2×2) 线程块集群，A 和 B 的每个 Tile 使用多播同时加载到四个 CTA [线程块] 中)*

每个 Tile 跨四个线程块分区，以便该 Tile 的全局内存流量在集群上分摊。Tile 仅被获取一次，并被所有四个线程块重用。

线程块集群可以在 2 × 2 集群中通过使用多播让四个 CTA 重用相同数据，将全局内存流量减少多达 4 倍。此外，线程块集群通过降低分母（即从全局内存移动的字节数）来增加每 GPU 的算术强度。一种称为**线程块对 (Thread Block Pairs)** 的特殊形式将在稍后关于 Tensor Cores 分块的背景下讨论。

> Blackwell 支持多达 16 个线程块的集群（当你选择超越默认 8 个 CTA 的非可移植集群大小时）。要启用此功能，请在内核上设置 `cudaFuncAttributeNonPortableClusterSizeAllowed` 属性。更大的集群可以提高重用率，但可能会降低占用率，因此在启用 16 之前请进行分析。

---

## 内核融合 (Kernel Fusion)

另一种增加算术强度的方法是将多个操作——或循环迭代——**融合**为一个操作。通过将多个内核融合在一起，从内存加载的数据可以在被写回之前用于多次计算和迭代。

同样，前一节讨论的循环展开允许单个线程对每个加载的数据元素执行更多计算，但代价是更多的寄存器使用。过多的融合可能会增加每线程寄存器压力并降低占用率，因此这是一个权衡。

始终对融合内核进行分析。如果寄存器使用量变得过大并开始溢出到本地内存，融合的好处可能会被额外的内存流量抵消。然而，如果你找到了正确的平衡，你可以提高每移动字节的 FLOPS，如果内存带宽是限制因素，这是有益的。

现代深度学习框架可以通过其即时编译器和图优化器自动融合和展开。例如，PyTorch 的 `torch.compile`，特别是 **TorchInductor**，可以自动融合逐元素操作序列。

> **逐元素操作 (Elementwise operations)**，也称为点操作 (Pointwise operations)，将简单的计算独立应用于张量的每个元素。

融合这些逐元素操作通过将中间值保持在片上来消除不必要的内存流量。这增加了从全局内存获取的每字节所做的工作量——提高了算术强度。

例如，朴素实现启动两个内核。第一个内核读取 x 并将 y 写入全局内存。第二个读取 y 并写入 z：

```python
y = sin(x);
z = sqrt(y);
```

这里，每个元素被触及两次：一次在 `sin(x)` 后，一次在 `sqrt(y)` 后。因此，每个内核的算术强度非常低，因为它每元素——每加载/存储操作——只执行一个昂贵的数学函数（多周期 ALU 指令）。相比之下，融合内核在单次通过中执行相同的操作集：

```cpp
z[i] = sqrt(sin(x[i]));
```

每个 `x[i]` 被加载一次，然后在寄存器中应用 `sin` 和 `sqrt`，只有最终的 `z[i]` 被写入内存。因为中间变量 `y` 从未去往全局内存，有效的 FLOPS per byte 急剧跃升，使操作更接近计算屋顶。

> 经验法则：如果数据将被同一线程块中的线程读取多次，通常值得将数据暂存到共享内存中以消除冗余的全局加载。这将有助于将你的内核从内存受限区域提升到计算受限区域，以更好地利用充足的 GPU FLOPS。

融合减少了全局内存流量并增加了算术强度，因为每个元素现在在每次读写内存操作中经历两次数学运算。在我们的例子中，我们将每元素的 FLOPS 翻倍（sin + sqrt），同时大致减半了内存流量，因为没有中间写入。这产生了显著更高的算术强度，或 FLOPS/byte。

为了深入说明这一点，让我们用一个具体的例子来演示算术强度。假设我们要对 2D 张量 x（形状 [batch, hidden]）的每个长度为 hidden 的行进行 L2 归一化。对于每一行 b，计算一个单一范数 `norm_b = sqrt(Σ_i x[b,i]*x[b,i] + ε)`，然后对所有 i 写入 `y[b,i] = x[b,i] / norm_b`。

朴素实现会在一个内核中平方，在第二个内核中将每行归约为标量，并在第三个内核中除法。这需要多次内核启动和中间写入 HBM。

假设这 4 个内核中的每一个都需要 1 FLOP 的计算。因此，这 4 个内核中每一个的算术强度是每 12 字节 1 FLOP（2 个 float 读取，1 个 float 写入），或 0.083 FLOPS/byte。

相反，我们可以将这 4 个内核融合为单个内核并增加算术强度。手动融合的内核代码如下所示：

*(代码示例省略，展示了在一个内核中完成平方和、归约、归一化)*

在这个融合内核中，每个线程遍历其 `x[b,*]` 切片两次——一次累加局部平方和，一次写入归一化输出——因此每元素全局流量约为 12 字节（两次读取 + 一次写入）。每元素内核在归约期间做 ~1 次乘法 + 1 次加法，在归一化期间做 1 次乘法。

`sqrt` 和 `rsqrt` 分摊到整行上。对于屋顶线放置，保守的算术强度约为 3 FLOPS / 12 字节 ≈ 0.25 FLOPS/byte（加上每行 `sqrt` 的微小贡献）。这让我们通过给每个线程多个元素来增加 ILP，从而隐藏 `sqrt` 和 `rsqrt` 的延迟。

> 此外，如前代码所示，我们计算逆平方根 (`rsqrtf`) 并相乘，而不是除法。这是一个常见的微优化——特别是对于热内循环。思路是用高吞吐量的乘法指令流替换慢速除法指令流。

与具有中间写入 HBM 的朴素三内核流水线（平方 → 归约 → 除法）相比，融合版本至少移除了一次全局写/读往返和一次启动屏障。因此，其实际强度和运行时间要好得多——即使每元素 FLOP/byte 只有 ~0.25。这是由于延迟节省和缓存局部性改进。

在实践中，这个单一的融合内核比一系列单独的内核执行得更快，原因在于更高的算术强度 (FLOPS/byte)、更好的缓存局部性以及通过将三个单独的内核合并为一个而减少的启动开销。

融合不仅增加了算术强度并将内核推向屋顶线的计算受限侧，而且还节省了内存带宽。在代码示例中，加法可以直接使用那些寄存器来计算总和。`sqrt` 可以接着使用总和——所有这些都不需要额外的全局内存流量。只有最终结果被写回全局内存。

---

## 结构化稀疏性 (Structured Sparsity)

在现代 GPU 上，**2:4 结构化稀疏性**由 **稀疏 Tensor Cores (Sparse Tensor Cores)** 和 `cuSPARSELt` 在硬件中加速。2:4 意味着每四个连续权重中正好有两个是非零的。创建这种类型的稀疏性有时被称为**剪枝 (Pruning)**。

通过将一半权重剪枝为 2:4 模式，每次内存加载现在提供两倍数量的实际参与乘法的非零值。换句话说，你不再获取结果为零的权重。因此，你没有在你知道为零的东西上浪费矩阵乘法操作，如图 9-5 所示。

*(图 9-5: 2:4 结构化稀疏性)*

结构化稀疏性是在模型训练之后应用的。模型被剪枝并针对推理进行优化。剪枝和格式转换在软件栈（如 `cuSPARSELt` 和框架工具）中完成。注意 Transformer Engine 加速支持的稀疏执行，但在转换期间不强制稀疏性。

在 PyTorch 中，使用 `to_sparse_semi_structured()` 将训练好的密集模块转换为 2:4 稀疏格式，然后再在稀疏 Tensor Cores 上部署稀疏 GEMM。

一旦模型转换完毕，它将调用在稀疏 Tensor Cores 上运行的优化稀疏 GEMM 内核，而不是标准内核。对于许多推理工作负载，稀疏 Tensor Cores 可以接近比密集对应物快 **2 倍**的加速——特别是当提交大批量输入时，这摊销了内核启动开销。

> **批处理 (Batching)** 是增加算术强度的一种非常常见且实用的方法。不是一次处理一项——伴随着所有相关的内存 I/O 等——而是一次处理多项，以便内存访问（例如，加载权重等）分摊到多个计算上。

这给了稀疏加速的矩阵乘法足够的并行工作来隐藏处理索引或压缩表示的任何开销。在较小的批次中，这种开销可能占主导地位并限制你观察到的加速。

2:4 稀疏性在使用大矩阵乘法时产生最大效益，这在像 LLM 这样的基于 Transformer 的模型中很常见。这是因为硬件可以充分利用专用的稀疏 Tensor Cores。这些稀疏 Tensor Cores 直接在硬件中对半宽数据进行操作。这跳过了零并在相同的周期预算内对非零元素执行两倍的工作。

因为 Blackwell 上的计算能力增长快于 HBM 带宽，结构化稀疏性是保持计算受限的好方法。即使在 Grace Blackwell 系统中（其中 NVLink-C2C 允许 GPU 以非常高的吞吐量从 CPU 内存流式传输数据），你仍然希望最大化每个加载 Tile 的 FLOPS per byte。

通过以 2:4 模式修剪 50% 的权重，例如，你确保了一半的内存流量永远不需要。这立即减少了全局内存读取并将有效算术强度提高了近 2 倍。

---

## 重计算与内存的权衡 (Recomputation Versus Memory Trade-Off)

此外，如果内存带宽是瓶颈，考虑**按需重计算**而不是存储或加载预计算的值（例如 $x^2$）。例如，在寄存器中重复计算 $x*x$ 通常比从全局内存加载先前计算的 $x^2$ 要快。廉价表达式的连续重计算可以增加算术强度，并且是每当内存稀缺时的有用技术。

许多 LLM 推理引擎使用此技术来节省内存。与其将大的激活张量存储在 HBM 中然后再读回，它们可以即时重计算某些层和激活。这类似于模型训练上下文中的**激活检查点 (Activation Checkpointing)**。

重计算提高了有效 FLOPS/byte，并可以将大模型放入更少量的内存中。此外，重计算为更大的批次大小释放了内存，并以少量额外的 FLOPS 换取内存流量的显著减少。

---

## PyTorch 与算术强度

在 PyTorch 中，许多这些想法是自动应用的。如前所述，PyTorch 编译器（将在第 13 和 14 章讨论）可以自动融合逐元素操作链——甚至一些归约。它使用执行图级别的优化来将数据保留在 GPU 上并尽可能多地重用它。

因为它在底层使用了像 cuDNN 和 cuBLAS 这样的优化库，PyTorch 在使用 `torch.matmul` 执行矩阵操作时会为你执行分块并使用共享内存。此外，PyTorch 的 `scaled_dot_product_attention` (SPDA) 可能会根据张量形状和数据类型分发到 FlashAttention、高效内存或 cuDNN 后端。作为一名注重性能的开发者，你应该了解这些优化以及如何验证它们是否被使用。

值得注意的是，虽然 PyTorch 可以识别并编译大多数操作，但某些非标准操作或自定义 CUDA 操作可能无法融合。在这些情况下，可能仍需要手动优化，如融合、分块等。

PyTorch 的**嵌套张量 (Nested Tensors)**，或称参差张量 (Ragged Tensors)，让你无需填充即可表示变长输入的批次。每个嵌套张量将变长序列打包到单个、高效的底层缓冲区中。NestedTensor 暴露了正常的张量接口，但它消除了不必要的零填充。因此，全局内存加载变得更高效，因为获取的每个字节都在计算中有用。

嵌套张量对于具有变长序列的 LLM 很有用。当使用嵌套张量时，像注意力及批量矩阵乘法这样的操作将仅从内存中检索必要数据。这将你的内核在 Roofline 模型上推向计算受限区域，并有助于减少内存受限停顿。结果是更高的持续吞吐量——特别是在内存敏感的工作负载上。

简而言之，PyTorch 暴露了各种机制来增加你的内核的算术强度。理解这些选项并决定什么最适合你的工作负载非常重要。现代 GPU 上增加算术强度的另一个有效方法是使用降低精度的 Tensor Cores。让我们接下来介绍这些机制。

---

## 混合精度与利用 Tensor Cores

现代 NVIDIA GPU 在 Tensor Cores 中实现了降低精度的计算，如 **TF32**, **FP16**, **FP8**, **FP4**, 和 **INT8**。Blackwell 中的每个 SM 都有一个 256 KB 的片上 **TMEM** 专用于 Tensor Core 数据。它还有一个专用的 **TMA** 单元，可以异步地在全局内存和共享内存之间复制 Tile。Tensor Core 指令（例如 `tcgen05.mma`）随后隐式地在共享内存和 TMEM 之间移动操作数和累加器。这种设计以高吞吐量喂养 Tensor Cores 并最小化停顿。Blackwell 基于 TMEM 的累加器有助于相对于以前直接在寄存器中累加的 GPU 代际减少寄存器压力。

如果使用得当，这些特性可以通过将算术强度 (FLOPS per byte) 提高 2倍、4倍甚至 8倍，将一个曾经内存受限、张量密集的内核转变为完全计算受限的内核。你可以通过监控 Roofline 图表和 Nsight Compute 中的 Stall Stats 来验证影响。

Nsight Compute 的 **Speed of Light** 分析显示了内存受限停顿原因，如“Memory Throttle”和缓存未命中。当使用具有较低精度格式的 Tensor Cores 时，这些将显著下降。Nsight Compute 集成了 Roofline 图表以交叉检查算术强度是否增加，从而将你的内核推向计算屋顶。

当你从 FP32 移动到较低精度的 Tensor Core 内核（如 TF32, FP16, FP8, 或 FP4）时，Nsight Compute 的 Warp Stall 指标通常显示内存相关停顿的减少，以及依赖性或流水线停顿的相对增加。这表明算术强度增加，并从内存受限向计算受限执行转变。

### 使用 TMEM 和 TMA 喂养 Tensor Cores

高吞吐量张量计算的核心是 **TMEM**，这是一个每 SM 256 KB 的 SRAM 缓冲区。然而，在高层次上，程序员并不显式分配或管理 TMEM。当你使用 Tensor Core 操作时，TMEM 由硬件或库处理。TMEM 如图 9-6 所示。

*(图 9-6: TMEM 通过累加部分结果 [代替寄存器] 来支持 Tensor Cores)*

在底层，Blackwell 使用 `tcgen05.mma` 指令，这些指令使用 TMEM 进行操作数和累加器存储。CUTLASS 和库内核通过内核配置和 PTX 汇编管理所需的分配和使用。因此，Transformer Engine 使用 TMEM 存储部分结果。这减少了 MMA 对寄存器的依赖。

像 **CUTLASS** 这样的高级 API 会自动为你处理所有这些复杂性。尽可能使用 CUTLASS 和其他高级库，因为 CUTLASS 使用 `tcgen05.*` PTX 指令，这些指令实现了 Tensor Core 矩阵操作和内存加载/存储接口。每当你使用 CUDA MMA 内部函数或 CUTLASS GEMM 启动 Tensor Core MMA 操作时，实现都会管理通过共享内存和 TMEM 的操作数移动。TMA 在全局内存和共享内存之间流式传输 Tile，Tensor Core 指令隐式地在共享内存和 TMEM 之间移动操作数。

> Nsight Compute 包含内置的 Roofline 和 Speed-of-Light 分析，以确认在采用低精度 Tensor Core 路径后，你的内核是否从内存受限转变为计算受限。

### TF32 和自动混合精度 (PyTorch)

虽然 Tensor Cores 最初是为 FP16 设计的，但它们也支持 **TF32**，它介于 FP32 和 FP16 之间。TF32 使用像 FP32 一样的 8 位指数和像 FP16 一样的 10 位尾数。TF32 在 Tensor Cores 上执行的吞吐量大大高于 CUDA 核心上的 FP32，同时保留了 FP32 的指数范围。在 PyTorch 中，启用 TF32 只需在代码中设置以下内容：

```python
import torch
torch.set_float32_matmul_precision('high') # {'highest'|'high'|'medium'}
```

一旦设置了这些标志，像 `torch.matmul` 和 `torch.nn.Linear` 这样的高级操作会自动作为 TF32 Tensor Core 内核执行，而不是在标准 CUDA 核心上以 FP32 执行。

除了 TF32，PyTorch 的**自动混合精度 (AMP)** 可以为每个操作选择最佳精度（FP16 或 BF16），并以 FP32 累加结果以保持稳定性。**BF16** 有助于避免 FP16 的溢出问题。默认情况下，CUDA autocast 使用 float-16。只需传递 `dtype=torch.bfloat16` 即可在支持它的 GPU 上选择 BF16。

在底层，TorchInductor 会自动融合这些精度转换，以确保：大型 GEMM 操作在 Tensor Cores 上以 FP16 或 TF32 运行，累加保持在 FP32 以保证数值稳定性，像层归一化和 Softmax 这样的小型“敏感”内核以 FP32 运行。

这个自动混合精度流水线通过最少的代码更改最大化了算术强度。融合的 Tensor Core 内核通过在共享内存（例如，操作数）和 TMEM（例如，累加器）中暂存和重用数据，最小化了到 HBM 的往返。

### BF16/FP16, FP8, 和 FP4 降低精度

BF16/FP16（半精度）已在许多 GPU 代际中得到支持，但现代 GPU 上的 Tensor Cores 通常可以维持超过 90% 的 BF16/FP16 峰值吞吐量，大约是 FP32 峰值吞吐量的 4 倍。这是因为在每个周期，硬件并行发出许多 BF16/FP16 FMA 操作。

FP16 使用比 FP32 更窄的 5 位指数，因此非常小的梯度值可能会下溢为零，除非你应用**损失缩放 (Loss Scaling)**。相比之下，BF16 匹配 FP32 的 8 位指数范围，原生避免了下溢。因此，它很少（如果有的话）需要损失缩放。这简化了混合精度工作流，并经常提高现代 GPU 上的训练准确性。

为了将吞吐量推得更高，你可以使用 **FP8**。通过将 16 位权重减少 50% 至 8 位，你将内存流量减半——并使每 HBM 事务加载的权重数量翻倍。在实践中，带有 FP32 或 TF32 累加的 FP8 matmul 实现了 2–3 倍于 BF16/FP16 TFLOPS 的性能。

为了解决极低精度下的准确性问题，Transformer Engine 支持 FP8 以及 NVIDIA 的 4 位 **NVFP4** 格式（带微缩放）。NVFP4 应用两级缩放，结合每微块缩放和更高级别的缩放，以便模型在使用 4 位存储权重的同时保持准确性。此外，Blackwell B200 的 NVFP4 激进微缩放量化提供了 **10 PFLOPS (密集)** 的性能，而 FP32 峰值约为 80 TFLOPS (密集)。这是每权重理论吞吐量提高了大约两个数量级。

如果你的模型在校准后能容忍精度下降，NVFP4 内核可以在支持的硬件上提供比 FP32 高得多的吞吐量，但必须逐个模型验证准确性。

每次精度下降都会使每字节操作数翻倍或四倍，从而增加算术强度。当 TMEM/TMA 重叠内存和计算时，这些低精度格式将曾经内存受限的内核转变为完全计算受限的内核。这充分利用了现代 GPU 中每 GPU 多 PFLOPS 的 Tensor Core 引擎。

### INT8 降低精度与用于推理的 DP4A 指令

LLM 推理用例通常可以容忍降低精度的 **INT8** 量化，现代 GPU 使用常规 CUDA 核心上的 **DP4A** (SIMD 点积) 指令以及 Tensor Cores 上的整数矩阵乘累加 (MMA) 指令来支持。在指令级别，DP4A 每条指令执行四次 INT8 乘累加 (MAC) 操作，相比之下每条指令一次 FP32 融合乘加 (FMA)。

因为 INT8 的权重流量每元素只有一字节，而不是 FP32 的四字节，权重内存流量下降了 75%。INT8 推理工作负载可以显著优于 FP32，这归功于更高的峰值 INT8 Tensor Core 吞吐量和减少的内存流量。

---

## 使用 CUTLASS 实现最佳算术强度和 Tensor Core 性能

利用这些优化的最简单方法之一是使用 NVIDIA 的 **CUTLASS** 库。使用 CUTLASS，你编写单个模板化调用，它将自动应用许多高级优化。

CUTLASS 应用的一些优化包括共享内存分块、异步内存传输，以及借助 TMEM 的 256 KB 每 SM 缓冲区进行的双缓冲。这样，你的 Tensor Cores 无需任何手动内核调优即可在接近峰值的吞吐量下运行。

例如，假设你想计算 GEMM，$C = A \times B$，使用半精度输入和半精度输出，并根据需要以 FP16 或 FP32 累加。你不必编写手动调优的 MMA 循环，只需包含 CUTLASS 并实例化一个模板。

当你编译并运行这段代码时，CUTLASS 自动做几件关键事情。首先，CUTLASS 选择 Tile 以平衡寄存器压力、共享内存容量和 Tensor Core 利用率。CUTLASS 在共享内存中暂存 Tile，并使用 Tensor Core 指令与 TMEM 交互以存储累加器数据。Tile 形状是根据经验和每内核选择的。例如，它可能会选择 128 × 128 或 256 × 128 这样的 Tile 大小。

CUTLASS 随后发出异步内存拷贝（`cp.async` 或 **TMA**），将每个 Tile 从 DRAM 流式传输到共享内存。`cp.async` 指令将数据从全局内存暂存到共享内存，而不使用每线程寄存器（或可选地不使用 L1 缓存）。

使用 CUDA Pipeline API 和 **Warp 特化**计算阶段（将在下一章讨论），CUTLASS 保持所有 Tensor Core 流水线忙碌。它以你指定的精度（例如，当输入为 FP16 或 FP8 时为 FP32）累加部分和，以确保数值保真度——然后将结果从 TMEM 以合并方式写回共享或全局内存。

因为所有这些复杂性都被隐藏了，CUTLASS 给你提供了一个直接可用的、高性能 GEMM 内核，其性能与手写 MMA 内核相匹配——通常在整体 Tensor Core 利用率和性能的百分之几以内。它只需要几行模板代码，而不是数周的低级调优。

这与编写自定义 MMA 内核形成对比，后者需要手动选择 Tile 大小、编写异步拷贝循环、管理双缓冲、实现 Warp 特化流水线和线程块集群 Tile。所有这些在 CUTLASS 中都为你自动完成了。

优化库如 cuBLAS 建立在 CUTLASS 之上。像 PyTorch 这样的高级库为许多内核调用这些优化库。在我们早期的融合注意力示例中，我们展示了 TorchInductor 分发了一个 CUTLASS 融合注意力内核，该内核使用了完全相同的双缓冲 TMEM 流水线。这导致了 98% 的 Tensor Core 利用率和接近零的内存停顿。

对于几乎所有标准 GEMM 或融合注意力用例，CUTLASS 及其构建的库是推荐的方法。其基于模板的设计、GPU 特定调优以及对 TMEM 和 TMA 流水线的内置支持，通常在支持的形状上实现高 Tensor Core 利用率。依靠 CUTLASS 的自动优化，你可以将时间花在模型架构、数值精度策略和端到端性能上。

---

## 内联 PTX 和 SASS 调优以进行微优化 (Inline PTX and SASS Tuning for Microoptimizations)

对于那些愿意超越 C++ 并进入低级微优化的人来说，CUDA 允许内联 **Parallel Thread Execution (PTX)** 代码和 **SASS**（NVIDIA 的汇编语言）以榨取可能留在桌面上的最后一点性能。

这是真正的高级领域，因为 CUDA 编译器在优化方面已经相当出色。但在某些极端情况下，你可以手动调度汇编指令——或使用专用指令——在非常特定的情况下获得小幅性能提升。

PTX 是一种低级并行线程执行虚拟机，将 GPU 暴露为并行计算设备。SASS 是实际在 NVIDIA GPU 硬件上执行的低级汇编语言。

作为一个例子，考虑一段代码，你知道特定的指令序列将是最佳的，但编译器没有生成该特定序列。常见的场景包括使用没有直接 CUDA 内部函数的 GPU 指令，在特定访问上应用内存加载修饰符（缓存提示），在精确点插入内存栅栏或屏障，或手动重新排序指令以避免流水线停顿。

内联 PTX/SASS 调优用于专家级微调，以削减额外的延迟或强制执行特定的调度。它可以产生适度的加速并启用某些自定义行为，但这应该在用尽所有其他高级优化之后进行。简而言之，**谨慎使用内联 PTX/SASS 并勤奋地进行分析**。收益是真实的，但通常是增量的。维护成本要高得多。对于大多数用例，依靠 CUDA 的内置优化——或像 CUB 和 Thrust 这样高度调优的库——可能已经足够好了。

### DeepSeek 使用内联 PTX 优化内存分配

自定义 PTX 的一个著名例子来自 **DeepSeek** 的 **DeepEP** 专家并行库。该库使用了一个定制的 PTX 指令 `ld.global.nc.l1::no_allocate.l2::256b`，通过绕过 L1 缓存分配、保留关键数据并利用 256 字节 L2 缓存块来优化全局内存访问。这对于将大型数据集直接流式传输到 L2 缓存而不破坏 L1 缓存中频繁访问的内存操作非常理想。

此指令不是 NVIDIA 官方 PTX ISA 规范的一部分，而是由 DeepSeek 工程师“文档外”发现的，用于微调其受美国出口限制的 Hopper GPU H800 变体的缓存行为。

通过绕过 L1，加载可以直接将大块数据流式传输到 L2，而不会驱逐频繁使用的 L1 驻留数据。这对于拥有需要留在 L1 以获得低延迟内存访问的热工作集至关重要。

然而，以这种方式使用 PTX 带有一定风险，因为 `ld.global.nc.l1::no_allocate.l2::256b` 不保证在跨 GPU 代际中保持稳定。DeepSeek 的 DeepEP 设置甚至包含一个构建时标志 `DISABLE_AGGRESSIVE_PTX_INSTRS=1`，以便在出现兼容性问题时禁用这些激进指令。虽然 DeepEP 的定制 PTX hack 可以产生显著加速，但在更新到新 GPU 架构时，应谨慎使用内联 PTX/SASS 并进行彻底测试。

---









# 第十章：内核内流水线、Warp 特化与协作线程块集群

在之前的章节中，我们涵盖了基础优化，如调优内存访问、最大化并行度、重叠计算和数据传输、提高占用率以及最小化 Warp 停顿。这些有助于隐藏延迟并消除瓶颈。然而，现代 GPU 提供了先进的硬件特性和执行模型，让我们能够将基础优化技术推向更远。

在本章中，我们将介绍一些更高级的 CUDA 技术，如**Warp 特化流水线 (Warp-specialized pipelines)**、具有网格级和集群级同步的**协作组 (Cooperative Groups)**、在动态工作队列上循环的**持久化内核 (Persistent Kernels)**，以及使用**分布式共享内存 (DSMEM)** 和 **TMA 多播**的**线程块集群 (Thread Block Clusters)**。

这些方法让我们能够在**没有主机干预**的情况下重叠内存访问和计算操作。我们还可以跨线程块在片上共享数据——并保持每个 SM 充分利用。

---

## 内核内流水线技术 (Intra-Kernel Pipelining Techniques)

**内核内流水线**指的是在单个内核执行内重叠内存操作和计算的一组技术。核心思想是将内核构建为并发阶段，使得当一块数据正在加载或存储时，先前加载的数据正在被处理。

传统上，GPU 依赖 Warp 级多线程来隐藏延迟。内核内流水线通过在同一 Warp 或内核内重叠内存和计算进一步推动了这一点。它使用细粒度的协调来交错内存加载和计算——有时在单个 Warp 内。

使用 CUDA Pipeline API 的内核内流水线在没有任何 `__syncthreads()` 的情况下重叠异步内存传输和计算。两种常见的内核内流水线方法是**双缓冲 (Double Buffering)** 和**Warp 特化 (Warp Specialization)**。

在双缓冲（两阶段）流水线方法中，所有线程统一协作。在 Warp 特化流水线方法中，Warp 被专门化为不同的角色，如内存加载者、计算者和内存存储者。选择取决于你的工作负载和性能要求。

### 使用 CUDA Pipeline API 进行协作分块和双缓冲

你可以使用 C++ Pipeline API 实现传统的双缓冲分块模式，通过实例化一个两阶段流水线来重叠内存加载和计算。具体来说，你可以声明一个两阶段 `cuda::pipeline_shared_state<cuda::thread_scope_block, 2>` 对象，该对象的作用域为特定线程块。这本质上是一个生产者-消费者模式，如图 10-1 所示。

*(图 10-1: 使用 CUDA Pipeline API 的两阶段生产者-消费者模式)*

关键的 CUDA Pipeline API 调用如下：
*   `pipe.producer_acquire()`: 为写入保留下一个流水线阶段。
*   `pipe.producer_commit()`: 信号该阶段先前发出的异步操作已准备好被消费。
*   `pipe.consumer_wait()`: 等待该阶段先前提交的操作完成，以避免循环中的竞争条件。
*   `pipe.consumer_release()`: 释放当前阶段以便重用。

流水线中的两个阶段重叠全局内存加载与计算。在第一阶段（Stage 0），线程块中的一个 Warp 发出异步预取，将下一个 Tile 加载到共享内存。预取发出协作的 `cuda::memcpy_async` 拷贝。

当 Stage 0 在一个 Warp 中生产（加载）数据时，线程块中的其余 Warp 正在第二阶段（Stage 1）消费（计算）已加载的数据。这个简单的生产者-消费者实现用正在进行的计算隐藏了 DRAM 延迟，如图 10-2 所示。

*(图 10-2: 使用生产者-消费者流水线用计算 (C) 隐藏全局 DRAM 加载 (L) 延迟)*

这是一种两阶段、双缓冲示例，使用 CUDA C++ Pipeline API。此 API 启用了这里使用的细粒度生产者-消费者同步代码。

*(此处省略代码示例，展示了使用 cuda::pipeline 实现双缓冲 GEMM)*

在该代码中，内核首先使用协作组 (CG) 检索当前线程块的句柄。然后它立即实例化一个绑定到该块的两阶段 `cuda::pipeline` 对象。

内核随后为 A 和 B Tile 分配一个连续的共享内存区域，并将其划分为四个子区域（A 两个，B 两个）以实现真正的双缓冲。

在进入主循环之前，内核使用异步拷贝预取前 `STAGES` 个 Tile。消费者 Warp 使用 `consumer_wait` 和 `consumer_release`，以便计算恰好在预取的 Tile 准备好时开始。这个初始屏障取代了未来对 `__syncthreads()` 的需求。

在每次迭代内，内核通过调用 `pipe.producer_acquire()` 保留下一个缓冲区。然后它启动两个 `cuda::memcpy_async` 操作——一个用于 A Tile，一个用于 B Tile。这些异步拷贝在访问被正确合并和分块时可以很好地与计算重叠。排队 `memcpy_async` 调用后，内核立即用 `pipe.producer_commit()` 发出信号。

并发地，块中的其他 Warp 调用 `pipe.consumer_wait()`。这有效地仅使那些依赖于当前缓冲区数据的线程停顿，直到生产者提交了它。一旦等待完成，每个线程调用设备函数 `computeTile(...)` 执行点积计算。

完成当前缓冲区的计算后，Warp 调用 `pipe.consumer_release()` 释放该阶段。这种细粒度释放防止了块范围的停顿，并最大化了计算和内存传输阶段之间的重叠。

当使用双缓冲时，建议确保每个异步拷贝 (`memcpy_async`) 后面都有适当的 `producer_commit()` 和 `consumer_wait()` 调用进行同步。这保证了计算内核仅在数据正确加载后才使用它。

在这个实验中，使用 CUDA C++ Pipeline API 的 `gemm_tiled_pipeline` 内核实现了比朴素分块版本 **2 倍**的加速。通过使用细粒度的 `pipe.producer_commit()` 和 `pipe.consumer_wait()` 原语，流水线保持填充状态，SM 活跃度从 68% 跃升至 92%。

### Warp 特化与生产者-消费者模型

**Warp 特化 (Warp Specialization)** 通过将操作分配给使用不同硬件的 Warp（例如，数据移动用 TMA，计算用 Tensor Cores）来扩展双缓冲。这与重用相同的 Warp 既加载数据又计算形成对比。

这种特化允许每组 Warp 拥有自己的指令序列。因此，指令连续发射和执行，不会被其他类型的操作中断。具体来说，Warp 特化让你分配一组“**生产者**”或“**内存**” Warp 使用 `cuda::memcpy_async` 异步预取 Tile。然后所有其他“**消费者**”或“**计算**” Warp 执行计算，如图 10-4 所示。

*(图 10-4: Warp 特化内核，一组 Warp 用于加载数据，所有其他 Warp 用于计算)*

这有效地在 Warp 之间创建了线程块级的多发射场景。这在单 Warp 双缓冲中是不可能的，因为单 Warp 调度器每周期只能发射一条指令。

Warp 特化的一个有趣模式是使用三种不同类型的 Warp，如“**加载者 (loader)**”、“**计算者 (compute)**”和“**存储者 (storer)**” Warp。加载者 Warp 将 Tile 推入流水线队列。计算者 Warp 在每个 Tile 上运行计算内核。存储者 Warp 写出结果。

这种 Warp 特化流水线挤出了单 Warp、双缓冲、顺序加载-计算循环无法解决的空闲周期。Warp 特化对数据传输和计算的高效重叠提高了 GPU 利用率——特别是对于长时间运行的循环和持久化内核。

一篇关于 Warp 调度的论文证明，Warp 特化可以实现内存和计算的近乎完美重叠。在这种情况下，GPU 内核具有明显的内存和计算阶段，使得内存和计算轮流成为瓶颈。通过应用 Warp 特化，他们的工作负载转变为一种状态，即 SM 的内存子系统和计算单元几乎在整个时间内同时忙碌。

Warp 特化的另一种模式是三角色 Warp 特化流水线的修改版。它像以前一样分配一组 Warp 作为内存加载者，但随后使用两组消费者 Warp 在计算和内存存储者角色之间“乒乓”。这种三角色 Warp 特化架构在 CUTLASS 中作为 `gemm_tma_warpspecialized_pingpong` 暴露。

在实践中，Warp 特化在挤出最后一点性能方面非常有效。事实上，FlashAttention v3 将其加速部分归功于 Warp 特化流水线，该流水线重叠 GEMM 和 Softmax 计算——连同数据传输——以保持所有硬件单元忙碌。这有助于注意力计算实现接近峰值的 FLOPS。

> 对于不平衡或需要隐藏延迟的场景使用 Warp 特化——特别是当单个 Warp 的计算不足以隐藏内存加载延迟时。然而，如果内核很小——或者极度受限于内存——坚持使用更简单的双缓冲方案可能会在没有额外代码复杂性的情况下产生类似的好处。

---

## 持久化内核与巨型内核 (Persistent Kernels and Megakernels)

**持久化内核 (Persistent Kernels)**，也称为**持久化线程 (Persistent Threads)**，颠覆了通常的“每任务一个内核”的方法。你不再启动许多每个都会产生显著开销的小内核，而是启动一个单一的、长时间运行的内核，其线程不断从全局或共享内存中的共享生产者-消费者队列中拉取工作。

当持久化线程循环时，它们处理到达的数据块——通常使用内存拷贝或主机信号——而不退出内核。这完全避免了重复的内核启动开销。

例如，考虑有 1,000 个微小的独立任务。传统上，可能会启动 1,000 个单独的内核。每个内核在退出前仅短暂占用几个 SM。在实践中，GPU 会为每个微小内核重复加速和减速。这会在启动之间留下大多数 SM 空闲——并且无法充分利用硬件。

有了持久化内核，你反而启动一个旨在让 GPU 在整个工作负载期间保持忙碌的大网格。例如，在拥有 132 个 SM 的 GPU 上，这可能意味着启动每个 SM 一个块，每块 256 个线程。总共 33,792 个线程。每个线程然后执行类似下面的代码：

```cpp
__global__ void persistentKernel(Task* tasks, int totalTasks) {
    // 每个线程循环，原子地抓取下一个任务索引直到没有剩余
    while (true) {
        int idx = atomicAdd(&g_index, 1);
        if (idx >= totalTasks) break;
        processTask(tasks[idx]);
    }
}
```

现在，你不再为每个单独的任务支付启动开销，而是只支付一次启动 `persistentKernel` 的费用。如果 1,000 个任务的总运行时间为 100 ms，而每次启动需要 0.02 ms，那么运行 1,000 个微小内核将增加 20 ms 的开销。简而言之，将小任务合并到一个持久化循环中可以削减数以十毫秒计的启动开销。

在现代 GPU 上，持久化内核特别有效，因为它们更大的共享内存容量和扩展的寄存器文件允许每个线程在片上保存更多中间状态。线程可以使用 TMA 为即将到来的任务预取张量 Tile，而其他 Warp 进行计算。

### 推理用的巨型内核 (Megakernels)

此外，源自大规模推理的一种现代持久化内核方法称为**巨型内核 (Megakernel)**。巨型内核将跨层——甚至跨 GPU——的整个操作序列融合为一个单一的大型内核。如图 10-8 所示，持久化巨型内核已显示出通过消除重复的内核启动开销，相对于传统的逐层启动将延迟降低了 **1.2 倍到 6.7 倍**。

*(图 10-8: 巨型内核相对于 vLLM 和 SGLang 的解码吞吐量提升)*

### 持久化内核与 Warp 特化

Warp 特化通常与持久化内核一起使用，其中线程在相对较长的时间内执行许多迭代。这允许更深的流水线、更好的重叠以及对长寿命资源的有效利用。对于运行时间较短的内核，持久化内核和 Warp 特化增加的代码复杂性可能不值得。

持久化内核调度的限制是为持久化内核找到足够的 SM 来利用。如果太多 SM 被另一个内核占用，可能没有足够的资源来启动持久化内核。

为了促进持久化内核（以及 Warp 特化），现代 GPU 支持**线程块集群 (Thread Block Clusters)**——也称为协作线程数组 (CTA) 集群。

---

## 协作组 (Cooperative Groups)

**协作组 (Cooperative Groups)** 让你能够以任意粒度定义和同步线程组。例如，你可以创建包含单个线程、Warp、Tile、块和集群的组，如图 10-9 所示。

*(图 10-9: 使用协作组跨不同粒度的线程进行同步)*

协作组提供安全、可重用的集合操作，如同步、广播和归约。这与使用临时的同步屏障形成对比。通常，线程只能使用 `__syncthreads()` 在其自己的块内同步——并且没有内置的针对整个网格的全局屏障。

协作组给你在内核内部的细粒度同步。该 API 非常适合协调跨 Warp、块和集群的多阶段流水线。

要以协作模式启动内核，你使用 `cudaLaunchCooperativeKernel()`。在协作模式下，CUDA 确保你启动的网格可以**并发驻留**——否则，启动失败。因此，建议始终使用 `cudaOccupancyMaxActiveBlocksPerMultiprocessor` 调整协作网格的大小并限制网格大小以避免启动失败。在内核内部，你可以调用以下代码来实现全线程块屏障：

```cpp
cooperative_groups::this_grid().sync(); // 或 grid.sync()
```

在这种情况下，任何块中的任何线程都不能越过该点，直到每个块中的每个线程都到达它。这个屏障允许你将内核拆分为顺序阶段，而无需结束启动或将控制权返回给主机。

例如，考虑 Softmax 算法，它有两个阶段：跨整个数组的归约以计算聚合和，以及随后的使用聚合和的逐元素计算。传统上，你会启动一个内核做归约，将结果拷回主机内存或全局内存，启动第二个内核消耗结果。这需要大量相对较慢的内存移动。

有了协作组，你可以在一个内核中执行两个阶段，使得每个块计算其部分和，一个块将这些部分和聚合成最终结果，所有块调用 `grid.sync()` 等待聚合完成，然后所有线程进入第二阶段。每个线程随后从寄存器或共享内存——而不是全局内存——读取聚合和。

简而言之，协作组内核让你将整个 GPU 视为单一的协作资源，`grid.sync()` 充当全局屏障。这对于需要全局同步和数据共享的多阶段算法非常理想。只需记住 `grid.sync()` 是一个相对重量级的同步。

---

## 线程块集群与分布式共享内存 (Thread Block Clusters and Distributed Shared Memory)

**线程块集群 (Thread Block Cluster)**，或 CTA 集群，是一个硬件级的层级。它将一部分 SM 授予你的协作网格——并将剩余部分留给其他内核使用。这减轻了一个内核垄断 GPU 的风险。GPU 保证线程块将被共同调度在同一个 **GPU 处理集群 (GPC)** 上，如图 10-10 所示。

*(图 10-10: 多个线程块集群保证被共同调度在同一个 GPC 或 GPC 分区上)*

GPU 为线程块集群提供了**分布式共享内存 (DSMEM)**，供其跨块使用。它还支持使用 Cluster Group API (`cluster.sync()`) 的集群级屏障。

这个集群级屏障让你只同步一部分线程块，而无需阻塞整个 GPU。线程块集群让你启动一个协作内核，将网格细分为更小的块组。

在每个组内，调用 `cluster.sync()` 提供一个本地屏障。这让集群内的块通过专用的片上资源共享数据，而无需垄断每个 SM。在现代 GPU 上，你可以使用 DSMEM，它允许线程块集群共享一个连续的片上 SRAM 区域。这实现了同一集群内块之间具有原生硬件支持的低延迟通信。

**分布式共享内存 (DSMEM)** 将 `__shared__` 内存的概念扩展到了单个线程块之外，跨越整个线程块集群。在传统内核中，每个线程块都有其私有的 `__shared__` 区域，其他线程块无法访问。

然而，有了 DSMEM，同一集群中的多个块可以读/写一个共享内存空间，该空间逻辑上结合了它们所有的本地 `__shared__` 区域。实际上，集群的共享内存被缝合成一个分布式的片上缓冲区。

通过将块间数据保持在片上内存中，DSMEM 显著提高了有效算术强度，因为块可以交换中间结果或执行归约而无需往返全局内存。

简而言之，DSMEM 是使用全局内存进行块对块通信的更快、更结构化的替代方案。但其范围限于同一集群中的块。

> 远程 DSMEM 访问使用线程块集群互连路由，并与全局内存流量不同。例如，当一个线程块使用 DSMEM 拉取一个 Tile 时，它使用低延迟共享内存传输。这确保了线程块集群中的线程块数据共享与片上共享内存一样快。

如果 Tile 不在 DSMEM 中，它必须从全局内存获取（遵循可能在 L2 中命中的正常缓存层级）。这样，如果 Tile 已经被从 DSMEM 中驱逐，线程块仍然可以从 L2 缓存中检索 Tile 数据，而不是一直回到全局 DRAM。

---

## 使用线程块集群设计高效算法

线程块集群为并行化以前需要全局内存通信或多个内核启动的工作负载提供了新策略。例如，想象一个大型矩阵乘法，其中输出 Tile 太大，由于共享内存限制，单个线程块无法处理。

在过去，你可能会将工作拆分到两个线程块，但随后每个块必须与全局内存交换部分结果，这是相对缓慢和低效的。

有了线程块集群和 DSMEM，这两个块可以形成一个集群，并直接共享一个联合的片上共享内存区域，使用硬件支持的原语无缝地合并它们的结果。DSMEM 硬件允许一个 SM 通过快速网络对另一个 SM 的共享内存执行加载/存储/原子操作。

同步本身在正确完成时是廉价的，但如果哪怕只有一个块绕过或延迟到达 `cluster.sync()`，性能可能会受到影响——甚至更糟，内核可能会挂起。

通常，当每个块有相当数量的工作可以独立运行直到需要同步或数据交换点时，线程块集群表现最佳。一旦同步发生且数据在片上传输，线程块可以继续处理。

线程块集群对于**块稀疏 (Block-sparse)** 矩阵操作特别有效——这在 LLM 的稀疏注意力、模型剪枝和压缩中很常见。在这些情况下，块处理不同的非零区域并共享它们的边界数据。

### 线程块混洗 (Thread Block Swizzling)

在直接的网格启动中，线程块按严格的行主序或列主序处理 Tile。这可能导致早期块驱逐了后期块需要的数据——导致糟糕的重用和额外的内存流量。相反，你希望 A 和 B 的 Tile 在单个波次 (Wave) 内，这样可以从 L2 缓存中读取。

为了解决这种低效，你可以使用**线程块混洗 (Thread Block Swizzling)**。类似于使用 Swizzling 优化内存访问并避免共享内存 Bank 冲突，你可以使用线程块混洗来避免以低效的行主序和列主序分配 Tile，如图 10-12 所示。

*(图 10-12: 线程块混洗以从 L2 缓存的单个波次中读取 Tile A 和 B)*

线程块混洗让同一波次需要的 A 和 B 矩阵 Tile 停留在 L2 中以获得最大重用。当应用于持久化和分块 GEMM 工作负载时，这种类型的混洗可以通过减少内存未命中和带宽压力产生两位数的性能增益。

### 减少全局内存流量与 TMA 多播

从性能角度来看，DSMEM 可以显著减少冗余的全局内存流量并启用更高的有效带宽。例如，在分块 GEMM 中，集群内的多个线程块共享 A 或 B 矩阵的块，一个块可以从全局内存加载一个 Tile 并使用 TMA 将其**多播 (Multicast)** 给集群中的其他块。

TMA 引擎支持多播复制模式，当线程块属于同一集群时，直接馈入 DSMEM。来自全局内存的单个 TMA 传输可以将数据同时放入每个参与块的共享内存中。这避免了冗余的 DRAM 获取。

有了 TMA 多播，GPU 确保 L2 缓存的数据一次性广播到每个集群成员（线程块）的共享内存中。结果，你避免了同一 Tile 的重复全局内存加载。这提高了带宽利用率并缩减了 DRAM 流量。

---

## 关键要点 (Key Takeaways)

以下是本章的重点，本章专注于在现代 GPU 上榨取峰值性能：

**用流水线深度隐藏延迟**
> 使用两阶段（`cuda::pipeline_shared_state<cuda::thread_scope_block, 2>`）分块将异步加载与计算重叠，并在计算超过内存（例如，像 Blackwell 这样的现代 GPU）时添加另一个阶段。这将有助于消除空闲 Warp。

**用 Warp 特化平衡工作负载**
> 当计算阶段占主导地位时，将加载、计算和存储分配给单独的 Warp，确保现代 GPU 硬件上的近乎峰值 Warp 效率。

**用持久化内核移除启动开销**
> 在设备侧工作队列上运行单个长寿命内核，并使用 `grid.sync()` 进行多阶段算法。这将减少主机-设备往返和整体启动成本。

**用线程块集群和 DSMEM 启用片上共享**
> 将线程块 (CTA) 分组为集群，以便它们共享连续的片上缓冲区。对于集群范围的广播，使用带有 TMA 多播描述符的 `cp.async.bulk.tensor`。这将提高 L2 命中率并削减 DRAM 带宽。

**特别注意屏障语义**
> `cluster.sync()` 和 `grid.sync()` 都要求每个参与的线程块到达相同的同步点。不匹配的控制流——或过大的集群大小——可能导致死锁或启动失败。

**在调优前分析**
> 使用 Nsight Compute 识别你的内核是内存受限还是计算受限。如果内存受限，从两阶段流水线开始。如果计算受限，考虑 Warp 特化 or 线程块集群。

---

## 结论 (Conclusion)

本章讨论的技术有助于系统地隐藏延迟并移除冗余加载。这使 GPU 在内核执行期间保持接近峰值利用率。

Warp 特化流水线重叠加载、计算和存储操作。协作组屏障 (`grid.sync()` 和 `cluster.sync()`) 帮助协调多阶段工作而无需主机往返。持久化内核在设备侧队列上循环以消除启动开销。

一如既往，你应该从分析开始。如果全局内存停顿占主导地位，像双缓冲这样的两阶段异步拷贝流水线通常就足够了。如果计算 Warp 仍然停顿，切换到多阶段 Warp 特化流水线，以便加载、计算和存储 Warp 可以在最小竞争下操作。唯一的竞争将在内存子系统。

对于多阶段归约或不规则任务，用单个持久化内核加 `grid.sync()` 替换多个启动以保持占用率。当线程块需要相同数据（例如，多头注意力）时，你可以形成一个线程块集群，以便 DSMEM 和 TMA 只加载每个 Tile 一次——并将数据多播到其他线程块——而无需重复访问全局内存。这些技术将把性能推向更接近 GPU 的峰值限制。

在下一章中，我们将保留这些内核内构建模块——`cuda::pipeline` 双缓冲、Warp 特化角色和线程块集群——并展示如何通过 **CUDA 流** 驱动它们。目标是隐藏**内核之间**以及主机 ↔ 设备通信之间的延迟，而不仅仅是在单个内核内部。具体来说，我们将重用本章的内核，并在带有 `cudaMemcpyAsync`, `cudaMallocAsync`/`cudaFreeAsync`, 和基于事件同步的多流流水线中运行它们。这将有助于推动整个系统在你的 AI 系统中的许多 GPU 上实现峰值性能。







**译境** 再度开启。

第十章和第十一章是本书的高阶篇章，标志着我们从单一的指令优化迈向了复杂的**执行模型**和**系统级编排**。

*   **第十章** 聚焦于**内核内部 (Intra-Kernel)**：如何在一个内核的微观世界里，利用 Warp 特化、流水线和线程块集群（Clusters）来榨干每一丝硬件性能。这是对“微观管理”的极致追求。
*   **第十一章** 聚焦于**内核之间 (Inter-Kernel)**：如何在宏观层面，利用 CUDA 流、异步内存分配和编程式依赖启动（PDL），让多个内核、数据传输和主机端操作像精密的齿轮一样咬合运转。

译文将力求精准传达这些复杂的并发概念，保持技术术语的专业性，同时还原作者层层递进的逻辑流。

以下是 **第十章** 和 **第十一章** 的核心内容译本。

---

# 第十章：内核内流水线、Warp 特化与协作线程块集群

在前面的章节中，我们涵盖了诸如调优内存访问、最大化并行度、重叠计算与数据传输、提升占用率以及最小化 Warp 停顿等基础优化。这些手段帮助我们隐藏了延迟并消除了瓶颈。然而，现代 GPU 提供了更先进的硬件特性和执行模型，让我们能够将这些基础优化技术推向极致。

在本章中，我们将介绍一些更高级的 CUDA 技术，例如**Warp 特化流水线 (Warp-Specialized Pipelines)**、具有网格级和集群级同步的**协作组 (Cooperative Groups)**、在动态工作队列上循环的**持久化内核 (Persistent Kernels)**，以及使用**分布式共享内存 (DSMEM)** 和 **张量内存加速器 (TMA) 多播**的**线程块集群 (Thread Block Clusters)**。

从高层次来看，线程块集群是一组保证并发运行的线程块。它们可以使用 DSMEM 读取、写入并对彼此的共享内存执行原子操作。

这些方法使我们能够在没有主机干预的情况下重叠内存访问和计算操作。我们还可以跨线程块在片上共享数据——并保持每个 SM (流式多处理器) 得到充分利用。

通过理解这些现代 GPU 执行模型，你将为下一章做好准备。在下一章，我们将通过探索基于 CUDA 流的**内核间 (Inter-Kernel)** 流水线，进一步扩展这些优化。

---

## 内核内流水线技术 (Intra-Kernel Pipelining Techniques)

**内核内流水线**指的是在单个内核执行过程中重叠内存操作和计算的一组技术。（在下一章中，我们将探索**内核间流水线**，它重叠的是在不同流中运行的多个内核。）

核心思想是将内核构建为并发的**阶段 (Stages)**，使得当一块数据正在加载或存储时，先前加载的数据正在被处理。这些阶段在不同的 Tile 或数据块上并行操作。这提高了吞吐量并有效地隐藏了延迟。

传统上，GPU 依赖 Warp 级多线程来隐藏延迟。当一个 Warp 因内存加载而停顿时，其他 Warp 继续进行计算。这是执行模型中 SIMT（单指令多线程）隐藏延迟的基础。

内核内流水线通过在同一个 Warp 或内核中重叠内存和计算，将这一理念推向了更远。它使用细粒度的协调来交错内存加载和计算——有时甚至是在单个 Warp 内部。

使用 **CUDA Pipeline API** 的内核内流水线可以在不使用任何 `__syncthreads()` 的情况下重叠异步内存传输和计算。两种常见的内核内流水线方法是**双缓冲 (Double Buffering)** 和 **Warp 特化 (Warp Specialization)**。

在双缓冲（两阶段）流水线方法中，所有线程统一协作。在 Warp 特化流水线方法中，Warp 被专门化为不同的角色，如内存加载者 (Loader)、计算者 (Compute) 和内存存储者 (Storer)。选择哪种方法取决于你的工作负载和性能要求。表 10-1 总结了这两种变体。

*(表 10-1: 现代 GPU 上使用 CUDA Pipeline API 进行内核内流水线的两种方法)*

| API 变体            | 适用场景                                                     | 主要用途                                                  |
| :------------------ | :----------------------------------------------------------- | :-------------------------------------------------------- |
| **双缓冲流水线**    | 基于循环的分块 (Tiling) 和双缓冲                             | 在同一个 Warp 或线程块中重叠加载和计算                    |
| **Warp 特化流水线** | 具有多个不同 Warp 角色的持久化内核 (例如我们的案例中有 3 个角色) | 将 Warp 分配给分离的角色/阶段，如内存加载、计算和内存存储 |

### 使用 CUDA Pipeline API 进行协作分块和双缓冲

你可以使用 C++ Pipeline API 实现传统的双缓冲分块模式，通过实例化一个两阶段流水线来重叠内存加载和计算。具体来说，你可以声明一个两阶段的 `cuda::pipeline_shared_state` 对象，该对象通过协作组作用于特定的线程块。这本质上是一个生产者-消费者模式，如图 10-1 所示。

*(图 10-1: 使用 CUDA Pipeline API 的两阶段生产者-消费者模式)*

关键的 CUDA Pipeline API 调用如下：
*   `pipe.producer_acquire()`: 为写入保留下一个流水线阶段。
*   `pipe.producer_commit()`: 信号该阶段先前发出的异步操作已准备好被消费。
*   `pipe.consumer_wait()`: 等待该阶段先前提交的操作完成，以避免循环中的竞争条件。
*   `pipe.consumer_release()`: 释放当前阶段以便重用。

流水线中的两个阶段重叠了全局内存加载与计算。在第一阶段（Stage 0），线程块中的一个 Warp 发出异步预取，将下一个 Tile 加载到共享内存。预取发出协作的 `cuda::memcpy_async` 拷贝，底层降级为每线程的 `cp.async` 到共享内存。

当 Stage 0 在一个 Warp 中生产（加载）数据时，线程块中的其余 Warp 正在第二阶段（Stage 1）消费（计算）已加载的数据。这个简单的生产者-消费者实现用正在进行的计算隐藏了 DRAM 延迟。

---

### Warp 特化与生产者-消费者模型 (Warp Specialization and the Producer-Consumer Model)

**Warp 特化**通过将操作分配给使用不同硬件的 Warp（例如，数据移动用 TMA，计算用 Tensor Cores）来扩展双缓冲。这与重用相同的 Warp 既加载数据又计算形成对比，如图 10-3 所示。

这种特化允许每组 Warp 拥有自己的指令序列。因此，指令连续发射和执行，不会被其他类型的操作中断。具体来说，Warp 特化让你分配一组“生产者”或“内存” Warp 使用 `cuda::memcpy_async` 异步预取 Tile。然后所有其他“消费者”或“计算” Warp 执行计算，如图 10-4 所示。

这里，4 个 Warp 被分配为生产者角色，其余 8 个 Warp 被分配为消费者角色。这有效地在 Warp 之间创建了线程块级的多发射场景。这在单 Warp 双缓冲中是不可能的，因为单 Warp 调度器每周期只能发射一条指令。

Warp 特化的一个有趣模式是使用三种不同类型的 Warp，如**“加载者 (Loader)”、“计算者 (Compute)”和“存储者 (Storer)”** Warp。加载者 Warp 将 Tile 推入流水线队列。计算者 Warp 在每个 Tile 上运行计算内核。存储者 Warp 写出结果。

这种 Warp 特化流水线挤出了单 Warp、双缓冲、顺序加载-计算循环无法解决的空闲周期。Warp 特化对数据传输和计算的高效重叠提高了 GPU 利用率——特别是对于长时间运行的循环和**持久化内核**。

在实践中，Warp 特化在挤出最后一点性能方面非常有效。事实上，FlashAttention v3 将其加速部分归功于 Warp 特化流水线，该流水线重叠 GEMM 和 Softmax 计算——连同数据传输——以保持所有硬件单元忙碌。

---

### 持久化内核与巨型内核 (Persistent Kernels and Megakernels)

**持久化内核**（也称为持久化线程）颠覆了通常的“每任务一个内核”的方法。你不再启动许多每个都会产生显著开销的小内核，而是启动一个单一的、长时间运行的内核，其线程不断从全局或共享内存中的共享生产者-消费者队列中拉取工作。

当持久化线程循环时，它们处理到达的数据块——通常使用内存拷贝或主机信号——而不退出内核。这完全避免了重复的内核启动开销。

例如，考虑有 1,000 个微小的独立任务。传统上，可能会启动 1,000 个单独的内核。而在实践中，GPU 会为每个微小内核重复加速和减速，导致大多数 SM 在启动之间处于空闲状态。

使用持久化内核，你反而启动一个大网格，旨在让 GPU 在整个工作负载期间保持忙碌。例如，在拥有 132 个 SM 的 GPU 上，这可能意味着启动每个 SM 一个块，每块 256 个线程。每个线程然后在一个循环中原子地获取任务索引并处理。

#### 推理用的巨型内核 (Megakernels)

此外，源自大规模推理的一种现代持久化内核方法称为**巨型内核 (Megakernel)**。巨型内核将跨层——甚至跨 GPU——的整个操作序列融合为一个单一的大型内核。持久化巨型内核已显示出通过消除重复的内核启动开销，相对于传统的逐层启动将延迟降低了 **1.2 倍到 6.7 倍**。

---

### 线程块集群与分布式共享内存 (Thread Block Clusters and Distributed Shared Memory)

一个**协作组 (Cooperative Group)** 是一个软件级的抽象。相比之下，**线程块集群 (CTA Cluster)** 是一个硬件级的层级。它将一部分 SM 授予你的协作网格，并保证线程块将被共同调度在同一个 **GPU 处理集群 (GPC)** 上。

GPU 为线程块集群提供了**分布式共享内存 (DSMEM)**。**DSMEM** 将 `__shared__` 内存的概念扩展到了单个线程块之外，跨越整个线程块集群。

在传统内核中，每个线程块都有其私有的共享内存区域，其他线程块无法访问。然而，有了 DSMEM，同一集群中的多个块可以读/写一个共享内存空间，该空间逻辑上结合了它们所有的本地共享内存区域。实际上，集群的共享内存被缝合成一个分布式的片上缓冲区。

通过将块间数据保持在片上内存中，DSMEM 显著提高了有效算术强度，因为块可以交换中间结果或执行归约而无需往返全局内存。

#### 线程块混洗 (Thread Block Swizzling)

在直接的网格启动中，线程块按严格的行主序或列主序处理 Tile。这可能导致早期块驱逐了后期块需要的数据。为了解决这种低效，你可以使用**线程块混洗**。这类似于使用 Swizzling 优化内存访问，你可以避免以低效的顺序分配 Tile，从而让 Tile A 和 B 保持在 L2 缓存中以获得最大重用。

#### 减少全局内存流量与 TMA 多播

TMA 引擎支持**多播 (Multicast)** 复制模式，当线程块属于同一集群时，直接馈入 DSMEM。来自全局内存的单个 TMA 传输可以将数据同时放入每个参与块的共享内存中。这避免了冗余的 DRAM 获取。

有了 TMA 多播，GPU 确保 L2 缓存的数据一次性广播到每个集群成员（线程块）的共享内存中。结果，你避免了同一 Tile 的重复全局内存加载。这提高了带宽利用率并缩减了 DRAM 流量。

---

## 关键要点 (Key Takeaways)

*   **用流水线深度隐藏延迟**：使用两阶段（或更多）分块将异步加载与计算重叠，消除空闲 Warp。
*   **用 Warp 特化平衡工作负载**：将加载、计算和存储分配给单独的 Warp，确保现代 GPU 硬件上的近乎峰值 Warp 效率。
*   **用持久化内核移除启动开销**：在设备侧工作队列上运行单个长寿命内核，减少主机-设备往返。
*   **用线程块集群和 DSMEM 启用片上共享**：将线程块分组为集群，利用 DSMEM 和 TMA 多播进行高效的片上通信，提升 L2 命中率并削减 DRAM 带宽。

---

# 第十一章：内核间流水线、同步与 CUDA 流式排序内存分配

到目前为止，我们专注于**内核内部 (Intra-kernel)** 的工具——如 `cuda::pipeline` 双缓冲、Warp 特化、持久化内核和线程块集群——以保持单个内核忙碌。在本章中，我们将展示如何通过 CUDA 流、事件和流式排序内存分配器，在**内核之间 (Inter-kernel)** 以及**批次之间**进行流水线处理。简而言之，第 10 章关注隐藏内核**内部**的延迟，本章展示如何隐藏内核**之间**以及 GPU 与主机之间的延迟。

这种内核间并发对于在实际工作负载中保持 GPU 所有引擎（计算引擎和 DMA 引擎）忙碌且并行运行至关重要。

---

## 使用 CUDA 流重叠内核执行 (Overlapping Kernel Execution with CUDA Streams)

一个 **CUDA 流 (Stream)** 是按顺序执行的操作序列（内核启动、内存拷贝、内存分配）。考虑从 CPU 向 GPU 上的 2 个流启动 5 个内核。

CPU 可以继续执行工作，而流异步执行内核操作。如果不使用流（即使用默认流），操作将串行化。

### 默认流 vs 显式（非默认）流

依赖默认流行为最终会导致问题。任何进入**传统默认流 (Stream 0)** 的工作都会隐式地等待——并阻塞——所有其他流，反之亦然。

在性能关键型代码中，最好创建并使用你自己的**非默认、显式且命名的流**。如果你的程序有多个 CPU 线程，你也应该启用**每线程默认流 (Per-Thread Default Streams, PTDS)**。启用 PTDS 后，每个主机线程的“默认”流不再与其他线程的默认流同步。

### 使用 CUDA 事件进行细粒度同步

即使多个流重叠，有时一个流的操作也必须等待另一个流。在这种情况下，不要在主机端使用 `cudaDeviceSynchronize()` 阻塞 CPU。相反，使用 **CUDA 事件 (Events)** 提供细粒度同步。

一个流记录一个事件 (`cudaEventRecord`)，另一个流等待该事件 (`cudaStreamWaitEvent`)。这样，只有消费者流会停顿等待事件被记录——主机线程和所有其他流将继续执行。

---

## 流式排序内存分配器 (Stream-Ordered Memory Allocator)

在 PyTorch 中，你可以通过设置环境变量 `PYTORCH_ALLOC_CONF=backend:cudaMallocAsync` 来启用 CUDA 的流式排序分配器。

如果你使用传统的 `cudaMalloc(...)`，它是一个阻塞的、设备范围的操作，在返回前会同步设备。这会停顿所有流中的工作。

相比之下，使用 `cudaMallocAsync(...)` 的流式排序分配器只是将分配请求记录在将使用它的同一个 CUDA 流中。它不会阻塞其他流。这样，内存管理永远不会串行化那些正在喂养内核的流。

在实践中，流 0 可能正在执行 Batch N 的内核，流 1 正在拷贝 Batch N+1，而流 2 为 Batch N+2 排队一个 `cudaMallocAsync(...)`。因为 `cudaMallocAsync` 只是将其工作追加到流 2 的队列中，流 0 和 流 1 继续运行不受干扰。

这对于 LLM 训练和推理特别有价值，因为变长序列通常产生暂存缓冲区大小的波动。使用 `cudaMallocAsync`，你可以为更大的缓冲区排队一个非阻塞分配，而不会拖慢所有其他流。

---

## Warp 特化与线程块集群及 CUDA 流的结合

我们将把第 10 章的 Warp 特化示例与本章的多流示例结合起来。

在之前的 Warp 特化示例中，每个线程块在其自己的共享内存区域内负责所有三个流水线阶段——加载、计算和存储。现在，我们将扩展该实现以使用**线程块集群流水线**。这样，加载、计算和存储 Warp 分布在整个线程块集群中。

通过添加 CUDA 流并在单独的流中启动此内核的独立副本（例如 `NUM_STREAMS = 2`），我们在多个数据批次上保持 GPU 忙碌。

在这种设计中，我们创建了两层流水线：
1.  **内核内流水线**：线程块集群流水线让加载、计算和存储 Warp 跨线程块协作。使用 TMA 多播将全局内存 Tile 复制到集群内每个块的共享内存中。
2.  **内核间重叠**：多个流将主机侧分配、拷贝和内核启动相互隐藏。

这推动了硬件利用率接近峰值——并最大化了 LLM 工作负载的吞吐量。

---

## 编程式依赖启动 (Programmatic Dependent Launch, PDL)

另一种内核间流水线和通信类型称为 **编程式依赖启动 (PDL)**。PDL 允许一个内核直接在设备上触发另一个内核的执行——无需涉及 CPU。

这甚至可以在第一个内核 (Kernel A) 完成执行之前发生。为此，它使用 `cudaTriggerProgrammaticLaunchCompletion()`。依赖内核 (Kernel B) 随后调用 `cudaGridDependencySynchronize()` 等待信号。

利用 PDL，Kernel A 可以在其**尾声 (Epilogue)** 阶段信号 Kernel B 执行。这样，Kernel A 可以与 Kernel B 并行执行一段时间。这对于隐藏内核启动开销非常有效。

---

## 关键要点 (Key Takeaways)

本章涵盖了与 CUDA 流、流式排序内存分配器、基于事件的同步、内核间流水线、线程块集群和 PDL 相关的高级主题。

**显式流 vs 默认流**
> 避免使用串行化所有工作的传统默认流。创建显式的非阻塞流，以便内核和拷贝可以并发运行。

**流式排序内存分配器**
> 使用 `cudaMallocAsync` 和 `cudaFreeAsync` 在特定流内分配/释放设备内存。这避免了全局设备同步，并启用了分配与飞行中内核的重叠。

**重叠 H2D、计算和 D2H**
> 通过在不同流上排队异步主机到设备拷贝、内核启动和设备到主机拷贝，实现三路重叠。

**CUDA 事件用于细粒度同步**
> 使用事件来协调跨流的生产者-消费者依赖关系，而不停顿整个 GPU 或 CPU。

**内核间流水线与 PDL**
> 结合内核内的 Warp 特化流水线与多流启动。使用 PDL (`cudaTriggerProgrammaticLaunchCompletion`) 让依赖内核在主内核完全拆除前就开始执行，从而掩盖启动开销。

---

## 结论 (Conclusion)

CUDA 流的内核间并发已从一种手动优化演变为现代 AI 框架和 GPU 运行时使用的自动特性。通过理解流、资源占用和同步点等核心原则，开发者可以最大化 GPU 利用率。

本章演示了如何跨多个 CUDA 流编排内核、内存操作和分配，以保持所有 GPU 硬件单元（计算流水线、DMA 引擎、互连）活跃运行。

通过避免默认流的隐藏屏障，利用流式排序分配器，采用事件进行精确同步，并将内核内 Warp 特化与内核间多流流水线（扩展到线程块集群）相结合，即使是复杂的 LLM 工作负载，你也能实现接近峰值的利用率。

在多 GPU 上下文中，跨不同流重叠点对点传输、集合通信和计算进一步最小化了空闲时间。

在下一章中，我们将更深入地构建这些原则，介绍使用**动态并行 (Dynamic Parallelism)** 和 **CUDA Graphs** 的动态内核编排和元调度。我们将协调整个内核和数据移动的流水线——在运行时——以适应变化的工作负载。这种动态资源平衡和设备侧编排将把我们推向大规模 AI 系统性能优化的下一个层级。







**译境** 已就位。

第十二章带我们进入了 GPU 编程的“无人区”。我们不再满足于主机（CPU）发号施令，而是赋予 GPU 更多的自主权。本章是关于**动态编排**和**设备侧智能**的艺术。我们探索了如何让 GPU 自己决定下一步做什么，通过原子操作实现动态负载均衡，利用 CUDA Graphs 消除启动开销，甚至通过动态并行让内核自己生成子内核。

这是一场从“受控执行”到“自主运行”的革命，旨在彻底消除 CPU 与 GPU 之间的交互延迟，让计算流水线像永动机一样飞速运转。

以下是 **第十二章：动态调度、CUDA Graphs 与设备发起的内核编排** 的中文译本。

---

# 第十二章：动态调度、CUDA Graphs 与设备发起的内核编排

到目前为止，我们已经解锁了单个内核级别的计算和内存吞吐量。现在是时候编排这些内核，让 GPU 永不闲置了。

在本章中，我们将从主机上的调度转向**设备本身**的调度。我们将探索由快速 L2 缓存原子操作驱动的动态工作队列，折叠重复的内核启动，并使用 **CUDA Graphs** 来批量处理固定流水线并最小化 CPU 握手。

然后，我们将把编排推向更远，利用**设备侧图启动 (Device-side Graph Launch)** 和**动态并行 (Dynamic Parallelism)**。这些技术让 GPU 能够决定下一步运行什么，而无需回调 CPU。

最后，我们将深入多 GPU 环境，通过重叠点对点拷贝、NCCL 集合操作、CUDA 感知 MPI 和 **NVSHMEM** 单边 puts/gets。这样，GPU 集群就像一个巨大的共享内存协处理器一样运行。例如，NVIDIA 的 DGX GB200 NVL72 系统将 36 个 Grace CPU 和 72 个 Blackwell GPU 连接成一个单一的 NVLink 域，该域内具有统一寻址和高达 30 TB 的 CPU+GPU 组合统一内存。它支持通过 72-GPU 域内的 NVLink 结构进行远程 HBM 访问。更大的 NVLink 网络拓扑可以扩展到单个机架之外。

沿途，我们将把每种技术与**屋顶线分析 (Roofline Analysis)** 联系起来，帮助你选择正确的工具——流、图、原子操作或动态内核——以增加内核的运算强度。这将有助于提高工作负载的整体性能。

在本章结束时，你将理解动态、设备侧和基于图的内核编排技术，这些技术可以保持多 GPU 集群中的每个 SM 都得到充分供养。

---

## 使用原子工作队列进行动态调度 (Dynamic Scheduling with Atomic Work Queues)

线程间不均匀的工作分配可能会导致一些 SM 闲置，而其他 SM 仍然忙碌。这浪费了计算资源并降低了整体吞吐量。

不平衡通常发生在不同线程或块由于依赖输入的循环或条件工作负载而处理不同数量的工作时。一些块很快完成，留下它们的 SM 空闲，而其他 SM 继续处理运行时间更长的块。在拥有数百个 SM 的现代 GPU 上，如果工作未均匀分布，空闲期可能会导致许多 SM 闲置。这会严重损害性能。

等到运行时间最长的工作完成时，GPU 的一部分已经闲置了。这降低了已实现的占用率，因为许多周期在没有活跃 Warp 的情况下运行。记住，你可以使用 Nsight Systems 进行分析，并在 GPU 时间线上清晰地显示这些空闲间隙。

你还可以比较活跃 SM 周期与总经过 SM 周期来衡量利用率不足。Nsight Compute 将其作为一个单一指标提供，代表至少有一个 Warp 处于活跃状态的时间比例。较低的活跃与经过比率表明许多周期在没有活跃 Warp 的情况下运行。换句话说，GPU 经常处于空闲状态。

除了 Nsight Systems，你可以使用 Nsight Compute 检查已实现占用率（每 SM 活跃 Warp 的平均比例相对于硬件最大值）或 SM 活跃周期百分比（至少有一个 Warp 活跃的时间比例）来量化这种利用率不足。

> 为了将时间线间隙与特定代码段关联起来，请在重要的 GPU 工作周围插入 NVTX 范围标记。

接下来，我们将讨论如何在内核内部实现原子队列以动态分配工作。这对于在所有 SM 之间平衡任意工作负载以避免线程闲置非常重要。在做这些之前，我们需要介绍原子计数器。

### 原子计数器 (Atomic Counters)

原子计数器是原子队列的基础，它允许动态工作分配。

在现代 GPU 上，全局原子操作在片上 **L2 缓存**中进行服务和序列化。当目标缓存行驻留时，这减少了相对于 DRAM 往返的延迟。原子计数器仍会产生延迟，并在争用下序列化。但无争用的 `atomicAdd` 操作通过保留在片上发生得极快。图 12-1 显示了两个线程递增原子操作的示例。

*(图 12-1: 在直方图计算的上下文中，跨多个线程的超快、片上原子内存加法操作)*

然而，`atomicAdd` 并非免费的。它仍然有延迟，并且在争用下，可能会使等待同一内存地址的线程序列化。因此，L2 也需要序列化更新。这创造了一个热点，需要优化。Nsight Compute 可以帮助你量化成本。

在 Nsight Compute 的 **Memory Workload Analysis** 部分，你会找到 `atomic_transactions` 和 `atomic_transactions_per_request`。原子事务计数器代表 L2 缓存原子事务的总数，包括由争用引起的任何重试。每请求原子事务数指标，或称**争用比率 (Contention Ratio)**，代表每个 `atomicAdd` 指令生成的平均 L2 事务数。

当每个 `atomicAdd` 正好触发一次 L2 事务时，你的 `atomic_transactions_per_request` 徘徊在 1.0 左右，这意味着它支付了最低限度的代价。如果这个比率攀升至 1.0 以上，它表明线程正在停顿并重试原子更新，而不是做有用的工作。每次重试都表示争用。

这里的优化是通过批量获取工作来摊销原子操作。因此，不是每个线程或 Warp 执行一次 `atomicAdd`，而是每个原子操作批处理一组任务。以下是批处理大小为 32 的前后对比：

```cpp
// 批处理前
int idx = atomicAdd(&queue_head, 1);
if (idx < N) process(data[idx]);

// 批处理后
const int batchSize = 32;
int start = atomicAdd(&queue_head, batchSize);
for (int i = start; i < start + batchSize && i < N; ++i) {
    process(data[i]);
}
```

现在，单次原子更新授予一个 Warp（或线程块）一整片工作——在这个例子中是 32 项——然后再接触计数器。你仍然为每批支付一次 L2 事务，但在其间做了 32 倍的有用的工作。

在实践中，每个 Warp 只有一个线程执行此 `atomicAdd` 来获取下一个批次的起始索引。然后该线程将其广播给 Warp 的其余部分（例如，使用 `__shfl_sync`）。整个 Warp 随后并行处理这 32 项。这产生了每 Warp 一个原子操作而不是每线程一个，大大减少了争用。

在 Nsight Compute 中，你会看到 `atomic_transactions` 骤降，你的每请求事务数回落到接近 1.0。这证明你已经用持续的计算交换了昂贵的争用。

对于 L2 缓存原子操作极快的现代 GPU，即使是 8 或 16 的适度批处理大小也能消除大部分争用，这归功于高 L2 带宽。即便如此，始终验证你没有简单地将瓶颈转移到别处。

为了验证此优化没有负面影响其他性能指标，使用 Nsight Compute 的 **Warp Stall Reasons** 和 **Register Pressure** 报告，确保你的融合循环现在没有受到寄存器溢出或共享内存 Bank 冲突的限制。

> 如果在这些优化之后原子操作仍然很热，考虑替代设计，如每块计数器或分层减少工作分配。

简而言之，通过每原子操作批处理工作，你让 GPU 的许多 Warp 忙于做真正的计算。这与在一个跟不上的单一计数器处排队形成对比。

### 原子队列 (Atomic Queues)

现在让我们使用全局原子计数器来协调一个**动态工作队列**。目标是利用原子计数器和 `atomicAdd` 在所有 SM 之间平衡任意工作负载，使得没有线程或 Warp 闲置。这种动态工作队列的示例如图 12-2 所示。

*(图 12-2: 使用原子计数器和 atomicAdd 作为动态工作队列，以平衡 SM 和 Warp 间的工作负载)*

在下一个代码示例 (`computeKernel`) 中，每个线程基于 `idx % 256` 计算不同数量的迭代。`idx % 256` 值小的线程做很少的工作，而值大的线程做很多工作。由于这种不平衡，线程在不同时间完成，一些 SM 会闲置等待最长的线程完成。以下是使用每线程静态、不均匀工作量的代码：

*(代码示例省略，展示了不均匀的静态工作分配)*

> 目前没有用于动态 GPU 侧工作分发的高级 PyTorch API，所以我们需要用自定义 CUDA 内核实现它。为了简洁起见，我们略过具体实现。

在接下来展示的优化动态任务分发版本中，单个全局计数器（在设备内存中）被用作 Warp 级工作队列。我们将计数器转变为使用批处理原子操作的持久化 Warp 级工作队列。这样，提前完成的 Warp 立即获取另一个批次而不是闲置：

```cpp
// uneven_dynamic.cu
#include <cuda_runtime.h>

__device__ unsigned int globalIndex = 0;

// Warp 批处理动态队列：每活跃 Warp 1 个原子操作
__global__ void computeKernelDynamicBatch(const float* input,
                                          float* output,
                                          int N) {
    // lane id in [0,31]
    int lane = threadIdx.x & (warpSize - 1);

    while (true) {
        // 每次迭代选举一个活跃领导者 (安全处理分歧)
        unsigned mask = __activemask();
        int leader = __ffs(mask) - 1;

        // Warp 领导者原子地为整个 Warp 抓取一个连续批次
        unsigned int base = 0;
        if (lane == leader) {
            base = atomicAdd(&globalIndex, warpSize);
        }
        
        // 广播起始索引到 Warp 中的所有活跃通道
        base = __shfl_sync(mask, base, leader);

        unsigned int idx = base + lane;
        if (idx >= (unsigned)N) break; // 动态终止

        // ... 执行工作 ...
    }
}
```

每个 Warp 原子地从全局队列中索取大小为 `warpSize` (32 线程) 的下一批任务，并在循环中处理它们。这确保了没有 SM 会过早闲置。这段代码执行了用单个全局原子工作队列实现的动态工作分发。

这里，每个 Warp 重复地从那个全局计数器拉取下一个批次的基索引。每个 Warp 的第一个线程（`if (lane==0)`），称为 Warp 领导者，执行原子加法来获取这个连续块的起始索引，使用 `base=atomicAdd(&globalIndex, warpSize)`。然后它使用 `__shfl_sync` 将此基索引广播给 Warp 的其余部分。

换句话说，不再是每个线程绑定到一个固定的元素索引，现在每个 Warp 从共享计数器抓取一个连续的任务块。然后它使用 `idx = base + lane` 进行计算。

Warp 中的所有线程对其动态获取的索引执行相同的 sin/cos 循环。因此，工作不再是预先分配给每个线程的。相反，工作是在运行时使用全局原子队列拉取和平衡的。

在 N = 1 << 20 且 work = idx % 256 的简单微基准测试中，静态分配内核大约需要 200 ms，而动态队列版本大约在 100 ms 内运行。这 **2 倍加速** 是消除 SM 空闲时间和减少原子争用的结果。Nsight Compute 将活跃 SM 周期定义为至少有一个活跃 Warp 的经过周期比例。

加速比将取决于工作不平衡程度，但动态工作分发是任何时候你的分析显示 Warp 空闲停顿、低已实现占用率或由不均匀每任务运行时间引起的可见时间线间隙时都值得探索的优化。在这些场景中，特别是对于中度不平衡，你通常可以获得 10%–20% 的加速。

> 在极端不平衡的情况下，仅通过用原子驱动的工作队列替换静态索引，你就可以获得 2 倍的加速。对于轻微不平衡，原子操作和 Shuffle 的开销可能会抵消收益。

简而言之，动态工作分发确保了近乎均匀的 SM 利用率，因为每个 Warp 持续获取并处理新任务，直到计数器超过 N。这与许多 Warp 在最慢的 Warp 之前很久就完成并留下硬件资源未使用形成了对比。

---

## CUDA Graphs

当你的流水线包含多个内核、拷贝、流事件记录和回调时，每次迭代在主机上逐个启动它们仍然会产生 CPU 开销。**CUDA Graphs** 让你捕获整个工作流一次，并以基本上零 CPU 开销重复重放它。图 12-3 比较了不使用（上）和使用（下）CUDA Graphs 的内核启动。

*(图 12-3: 不使用 [上] 和使用 [下] CUDA Graphs 的内核启动时间线)*

为什么要使用 CUDA Graphs？首先，它们**削减了启动开销**。多个小内核或拷贝可以基本上通过一次 CPU 调用启动。其次，它们启用了**更好的 GPU 调度**。工作作为一批提交，因此 CUDA 驱动程序可以潜在地减少操作之间的一些内部延迟。

此外，有了 CUDA Graphs，依赖关系是预先知道的，因此 CPU 在其间进行同步的需求更少。在内存传输的上下文中，CUDA Graph 确保异步拷贝和内核执行作为依赖项正确链接，无需任何手动同步。它并不比普通流更内在的重叠拷贝和内核，但 CUDA Graph 简化了它们的执行。

### PyTorch, 推理引擎与 CUDA Graphs

像 PyTorch 这样的 AI 框架在底层利用 CUDA Graphs 来处理深度学习模型的静态部分。具体来说，PyTorch 支持 `torch.cuda.Graph` 上下文用于捕获操作序列。此外，PyTorch 继续优化其内部以对可预测的代码部分使用 CUDA Graphs。

像 vLLM 和 NVIDIA 的 TensorRT-LLM 这样的高性能推理引擎也可以利用 CUDA Graphs，通过将模型的执行捕获为针对不同序列长度范围和输入批次大小的一组预定义图。当启用图捕获时，这些系统通常会对输入进行分桶或填充以匹配支持的图批次大小，以便捕获的图可以以固定形状重放。这可以显著降低大规模生产推理工作负载的延迟。

### CUDA Graphs 的内存池

一个重要的考量是 CUDA Graphs 的内存管理。CUDA Graph 内部的内存操作遵守与 CUDA 流相同的规则。如果你在捕获内部分配 GPU 内存，该分配将成为图执行的一部分。

你通常希望避免在图内部分配 GPU 内存，并在图外部预分配内存。许多框架，如 PyTorch，使用带有 CUDA Graphs 的**静态内存池**，如图 12-4 所示。使用静态内存池可以防止内存分配成为捕获图序列的一部分。

*(图 12-4: PyTorch 为 CUDA Graphs 使用静态内存池)*

虽然 CUDA Graphs 不会使单个内存拷贝或内核执行得更快，但它们可以自动重叠图内独立的数据传输和计算——类似于 CUDA 流。这消除了每迭代的 CPU 调度，并且是可能的，因为依赖图是预先知道的。

### 使用 CUDA Stream 捕获 CUDA Graph

要捕获图，你在流上调用 `cudaStreamBeginCapture()`，排队所有内存传输 (`cudaMemcpyAsync()`)、内核启动、事件 (`cudaEventRecord()`) 和回调 (`cudaLaunchHostFunc()`)，然后调用 `cudaStreamEndCapture()` 创建一个 CUDA Graph 定义 (`cudaGraph_t`)。

CUDA 驱动程序随后可以在每次迭代时用 `cudaGraphLaunch()` 启动 CUDA Graph。因为 CUDA 驱动程序预先知道整个依赖图，它可以直接在 GPU 上重放预构建的流序列。这产生了最小的启动开销。

> `cudaGraphExecUpdate`，将在下一节讨论，允许对捕获的图进行有限的更改，适用于大小、维度或指针在迭代之间变化的情况。如果输入大小变化，这很有用，因为你可以更新图的节点参数，而不是为每个新输入大小重新捕获整个新图。

即使你的流水线只有一部分是重复的，CUDA Graphs 也可以捕获该部分。例如，如果你总是执行主机 → 设备拷贝，随后是两个内核，再随后是设备 → 主机拷贝，你可以只捕获该子图并用单个函数调用重放它。

### 动态图更新 (Dynamic Graph Update)

一旦你记录了一个 CUDA Graph，你不必仅仅因为某些启动参数改变了就扔掉它。与其重新捕获，你可以调用图更新 API 来直接在现有图中更新网格/块维度、指针地址或内核参数。图更新 API 包括 `cudaGraphExecUpdate` 和更底层的 `cudaGraphExecKernelNodeSetParams`。

`cudaGraphExecUpdate` 让你用相同形状的新节点交换内核节点。例如，你可以交换不同的融合内核实现——只要它是相同形状的。CUDA 运行时将验证你的调整并让你立即重放修改后的图。这避免了完全捕获的成本。

在实践中，典型的工作流是三步。首先，使用最大预期大小（例如，最大批次）捕获一个模板图。稍后，如果请求的批次为 64，你调用 `cudaGraphExecUpdate` 将启动参数调整为 64——并可能更新内存指针到更小的缓冲区。

### 设备发起的 CUDA Graph 启动 (Device-Initiated CUDA Graph Launch)

现在你了解了如何以低开销从 CPU 启动和适应捕获的流水线，下一步是**完全从启动决策中移除 CPU**。通过设备发起的 CUDA Graph 启动，正在运行的 GPU 内核可以直接在设备上触发预记录的图，完全避开主机。

要启用设备侧启动，首先像往常一样在主机上捕获图。然后使用 `cudaGraphInstantiate` 实例化它，并传递 `cudaGraphInstantiateFlagDeviceLaunch`。实例化后，在任何设备侧启动之前，使用 `cudaGraphUpload` 在主机流上上传可执行文件。

在实践中，这会在你的持久化或动态内核内部嵌入一个“图启动”节点，或调用设备侧图 API。当时机成熟时，GPU 将在一个它拥有的流上启动整个图，如图 12-5 所示。

*(图 12-5: 由 CUDA Graph 启动的操作序列 [内核和数据传输节点] 及其依赖关系 [边])*

设备发起的图启动将数据驱动的工作流完全保留在 GPU 上。你的内核负责计算决策条件，而不是 CPU。因此，它可以直接生成下一个图，消除 CPU 往返，并进一步降低延迟。

因为图已经驻留在 GPU 上，且不需要 CPU-GPU 握手，设备发起的启动从关键路径中移除了主机调度，并可以减少主机受限循环中的端到端延迟。在实践中，设备发起的 CUDA Graph 启动显示的启动延迟比等效的主机侧图启动低大约 **2 倍**。而且即使图的大小或复杂性增加，开销也保持平稳。

在设备代码内部，你使用简单的 API `cudaGraphLaunch(graphExec, stream)` 启动图。运行时使用特殊的、保留的 `cudaStream_t` 值来区分支持的启动模式：“即发即弃 (Fire-and-Forget)”、“尾部 (Tail)”和“兄弟 (Sibling)”。这些模式将自动在 CUDA 流中强制执行正确的顺序，无需任何主机干预。

*   **即发即弃 (Fire-and-Forget)**: 子图立即开始执行，与启动它的父内核并发。父内核不等待子图完成。
*   **尾部启动 (Tail Launch)**: 延迟图的执行，直到启动内核到达同步点或完成。这实际上是将图作为**延续 (Continuation)** 排队在当前内核之后运行，如图 12-6 所示。
    *(图 12-6: 给定图排队的尾部启动将一次执行一个，按排队顺序)*
*   **兄弟启动 (Sibling Launch)**: 是即发即弃的一种变体，其中启动的图作为父图的**对等体 (Peer)** 执行——而不是作为子图。此外，兄弟图在父图的流环境中运行。这意味着它立即且独立地运行，但不会延迟父图的任何尾部启动，如图 12-8 所示。
    *(图 12-8: 父流环境中的兄弟图启动)*

尾部启动在实现 GPU 驻留工作调度器时特别强大。一个持久化“调度器”内核可以尾部启动一个图，然后在该图完成后重新启动自己。这种技术有效地创建了一个 GPU 上的循环，而不需要主机重新调用。为了重新启动自己，内核调用 `cudaGetCurrentGraphExec()` 获取自身正在执行的图的句柄。然后它使用 `cudaGraphLaunch(..., cudaStreamGraphTailLaunch)` 再次排队自己。

在实践中，设备发起的 CUDA Graphs 打开了新模式。例如，想象一个 GPU 压缩流水线，其中内核必须根据数据内容在不同的压缩算法之间进行选择。与其结束内核并告诉 CPU 启动所选的压缩内核，GPU 内核可以直接启动对应于（比如）“LZ 压缩”或“Huffman 压缩”的预记录图。

### 原子队列和设备发起的 CUDA Graphs 用于内核内持久化调度

我们可以将早期的原子计数器工作队列与设备发起的图尾部启动结合起来。考虑一个 LLM 推理循环用例，它使用 CUDA Graph 执行解码，通过捕获包含 Transformer 块前向传播（注意力 + 前馈）的图。

一个轻量级的、持久化调度器内核可以使用 `atomicAdd(&queueHead, 1)` 来索取下一个工作项。然后它**尾部启动**预捕获的解码 CUDA Graph 来计算输出，并立即循环回队列中的下一项。

当每个 CUDA Graph 完成时，内核内调度器循环使用 `atomicAdd` 抓取下一个索引，并尾部启动另一个解码图。这有效地创建了一个完全 GPU 驻留的调度器，它在不触及 CPU 的情况下决定并执行任务。

通过链接这些尾部启动，每个 Token 从开始到结束都在设备上处理，具有几乎零 CPU 开销。并且由于 CPU 永远不会重新进入关键路径，SM 保持完全利用，每 Token 延迟下降，并且你可以即时适应不同的序列长度和批次大小。为此，你只需更新图参数或在预记录的图之间切换。

### 条件图节点 (Conditional Graph Nodes)

在传统的 CUDA Graph 中，每个节点及其依赖关系在捕获时是固定的，迫使任何决策逻辑回到主机。**条件图节点**通过将分支决策推迟到 GPU 本身——基于与节点关联的一个小“条件句柄”——打破了这种僵化。

随着图的执行，GPU 评估该句柄并选择性地运行其主体子图（或循环运行），而无需将控制权返回给 CPU。具体来说，条件图节点让你将控制流（**IF**, **IF/ELSE**, **WHILE**, **SWITCH**）直接嵌入到 CUDA Graphs 中以在 GPU 设备上运行。条件图节点消除了主机往返，并在现代 GPU 上提供了显著的性能增益。

本质上，条件图节点让你基于设备内核中计算的值来控制图的执行——所有这些都不涉及 CPU。这种能力允许将复杂的分支工作流实现为单个、可重复的图启动。CUDA Graphs 支持多种类型的条件节点，如图 12-9 所示。

*(图 12-9: 条件图节点的类型)*

*   **IF**: 当条件非零时，恰好执行其单主体图一次。
*   **IF/ELSE**: 通过指定两个主体图，条件为真时选择一个，为假时选择另一个。
*   **WHILE**: 只要条件保持非零，就重复执行其主体图，每次迭代后再次检查。
*   **SWITCH**: 持有 N 个主体图，当条件等于 i 时执行第 i 个；如果条件 ≥ N，则完全跳过执行。

简而言之，你应该使用条件图节点将决策保留在 GPU 上，减少 CPU 开销，并在 CUDA Graph 中表达复杂的控制流。因为图创建成本可以在多次迭代中摊销，所以在设备上完全表示动态工作流可以产生显著的性能提升。

---

## 动态并行 (Dynamic Parallelism, DP)

之前，我们看到了设备发起的 CUDA Graph 启动如何以最小的 CPU 参与捕获和重放固定的操作序列。但该模型期望你的整个执行流是提前知道的，这并不总是可能的。许多工作负载根据输入数据、中间结果或问题复杂性在运行时改变形状。这就是**动态并行 (DP)** 的用武之地。

DP 赋予你的 GPU 内核**为自己生成新工作**的能力，而不是等待 CPU。虽然 CUDA Graphs 要求你提前知道整个流水线，但 DP 让正在运行的“父”内核检查其自己的输出，并即时决定启动多少“子”内核。这对于真正不规则的问题——分层归约、自适应网格细化和图遍历——是一个游戏规则改变者，在这些问题中，后续任务的数量只有在你处理数据时才变得清晰。

在 LLM 的上下文中，大多数 Token 遵循标准的 Transformer 路径，但有些需要辅助注意力块。启用了 DP 的 Transformer 内核可以在运行时检测那些特殊 Token，并仅为那些位置尾部启动额外的注意力内核——没有主机干预，没有浪费的周期。NVIDIA 的库已经利用 DP 在自适应算法中实现类似的模式：随着数据流经计算，新的子任务动态涌现。

当你的分析器时间线显示像 **Kernel A → GPU 空闲间隙 → Kernel B** 这样的背靠背模式时，你就知道 DP 适合你。这里，空闲时间对应于 CPU 准备下一次启动。用设备侧启动替换该间隙可以保持每个 SM 被占用，并大幅削减依赖阶段之间的延迟。

当然，DP 的性能优势并非免费的。每个子启动使用 GPU 调度资源并需要额外的栈空间。为了避免“栈溢出”错误，你可能需要使用 `cudaDeviceSetLimit(cudaLimitStackSize, newSize)` 增加运行时栈大小。

此外，CUDA 对可以挂起的子内核启动数量有限制。默认情况下，CUDA 允许一次有 2,048 个未完成的设备启动。然而，这是可配置的。

简而言之，DP 将走走停停的主机驱动工作流转变为无缝的 GPU 驻留流水线，维持高 SM 利用率并最小化主机-GPU 协调。

---

## 跨多个 GPU 和集群节点编排 (NVSHMEM)

当你从一个 GPU 扩展到许多 GPU 时，核心目标保持不变：通过将数据移动隐藏在有用工作之后，保持每个设备忙碌。一旦主机向每个 GPU 分发了任务，无论是通过单独的 CPU 线程、异步启动还是多 GPU 图，GPU 就接管了。虽然每个设备上的一个流驱动计算，但第二个流可以通过 NVLink 或 PCIe 点对点穿梭数据，而完全不涉及主机内存。

这意味着，在规模上，你必须重叠点对点传输与计算。重要的是要注意，即使有了 NVLink，带宽和延迟也不等于设备上的 HBM。因此，这种通信必须通过重叠来隐藏。

在实践中，随着集群规模的增长，将工作与数据传输重叠对于线性扩展环境绝对至关重要。对于直接的切换，你可以使用 **GPUDirect Peer Access** 在后台移动大块内存。

当你需要集合通信（如 PyTorch DDP 中的梯度 All-Reduce）时，你在单独的流上启动 NCCL 的非阻塞例程。NCCL 随后将你的张量排列成环或树，使每条 NVLink 和 NVSwitch 路径饱和——所有这些都在你的计算内核在它们自己的流上继续运行时进行。

### 使用 NVSHMEM 进行细粒度 GPU 到 GPU 内存共享

对于需要超紧密、事件驱动协调的工作负载，如动态任务队列和细粒度事件通知，NVIDIA 的 **NVSHMEM** 库是一个很好的选择。它将每个 GPU 视为**分区全局地址空间 (PGAS)** 中的一个处理元素 (PE)。

有了 PGAS，GPU 可以直接从设备代码写入另一个 GPU 的内存，绕过 CPU。延迟取决于互连，NVLink 通常低于 PCIe 或网络传输。这是使用 NVSHMEM 的经典发送并信号 (Send-and-Signal) 模式：

*(代码示例省略，展示了 NVSHMEM 的 put 和 signal 操作)*

这里，单边远程内存操作完全在设备上发生。GPU/PE 0 将其结果直接写入 GPU/PE 1 的内存并在那里翻转一个标志。具体来说，GPU/PE 0 发出 `nvshmem_float_p` 将有效载荷数据直接写入 GPU/PE 1 的内存，调用 `nvshmem_quiet()` 确保完成，然后使用 `nvshmem_int_p` 翻转标志。

与此同时，GPU/PE 1 的内核在 `nvshmem_int_wait_until()` 上旋转，一旦标志被设置，就读取有效载荷。这不需要 CPU 干预或额外的拷贝——只是通过 NVLink 的硬件加速、GPU 到 GPU 传输。

简而言之，NVSHMEM 将 GPU 集群转变为共享内存域，使得设备专用内核启动、数据传输和同步的延迟和吞吐量远超任何 CPU 中介方法。

---

## 关键要点 (Key Takeaways)

实现峰值 GPU 性能取决于以最小的开销将计算和数据移动编织在一起。高效的编排简化了 CPU 和 GPU 之间的复杂工作负载，确保任何一方都不会阻碍另一方。以下是本章的一些关键要点：

**使用 L2 缓存原子队列进行动态调度**
> 现代 GPU 上的 L2 缓存原子操作异常快。使用带有批处理增量的快速 L2 缓存原子操作在 GPU 上平衡不规则工作负载。这种批处理工作分配减少了争用，并通过消除 Warp 空闲间隙保持 Warp 忙碌。它可以显著提高吞吐量——在极端不平衡情况下高达 ~2 倍，但通常在 10% 到 30% 之间。

**用于固定流水线的 CUDA Graphs**
> 记录一次 GPU 操作序列，然后每次迭代用单个主机调用重放它。这减少了每迭代的 CPU 调度开销，通常减少 20%–30% 的延迟。确保你在 GPU 上实现了依赖操作的最大重叠。

**使用 CUDA Graphs 进行低开销启动**
> 在 CUDA Graph 中捕获异步拷贝、内核启动、事件记录和分配序列。用 `cudaGraphLaunch` 重放图消除了每调用的 CPU 入队开销，同时保留了所有流间依赖关系，进一步减少了运行时瓶颈。

**设备侧编排**
> 从 GPU 本身启动工作，通过尾部启动预记录的 CUDA Graph 或使用动态并行生成子内核。这完全消除了 CPU 调度间隙，并允许 GPU 端到端保持忙碌，无需主机干预。

**多 GPU 重叠**
> 始终重叠通信与计算。使用单独的流来流水线化 GPU 点对点传输、NCCL 集合操作、CUDA 感知 MPI (RDMA) 或 NVSHMEM 单边操作。这将通信延迟隐藏在有用工作之后，并可以在有利的计算通信比和足够重叠下接近线性扩展。

**屋顶线引导的选择**
> 让屋顶线图驱动你的策略。如果你的内核是**内存受限**的，专注于通过异步内存拷贝和混合精度（如 FP8/FP4）来重叠和减少数据移动。如果它是**计算受限**但由于开销而未达标，使用减少启动的技术（如内核融合、持久化内核和 CUDA Graphs）来接近计算屋顶。对于介于两者之间的内核，通过并行运行多个操作来增加并发性以利用所有硬件单元。始终通过分析验证所选优化是否有效。

通过将这些技术编织在一起——动态分发、协作内核、图捕获/重放和 GPU 原生内存共享——你可以创建使 GPU 集群的每个部分都饱和的流水线，以应对超大规模 AI 工作负载。

---

## 结论 (Conclusion)

在本章中，我们超越了单内核优化，探索了端到端的编排技术。我们涵盖了如何通过动态并行完全在设备上启动工作，在 CUDA Graphs 中捕获复杂工作流，以及使用 NCCL 和 NVSHMEM 协调许多 GPU。每种技术都有相同的目标：让每个引擎充满工作，隐藏延迟，并消除主机-设备间隙，使你的硬件全速运行。

NVIDIA 的现代 GPU 平台比以往任何时候都更加模糊了 CPU 和 GPU 之间的界限。例如，Grace Blackwell 和 Vera Rubin 超级芯片使用具有巨大带宽的一致性 NVLink 将 CPU 与多个 GPU 连接起来。

但即使硬件减少了 CPU-GPU 障碍，责任仍然落在软件上以充分利用这种高性能硬件。本章的方法，无论是使用 C++ 中的 CUDA 还是更高级别的库 API，都是我们利用这些进步的方式。

在下一章中，我们将看到 PyTorch 如何集成这些想法中的许多，包括流、图、异步操作和优化内核，以便你只需几行 Python 代码即可实现这种性能。让我们深入 PyTorch 生态系统，了解为什么它对于实现高性能 AI 工作负载如此受欢迎。











**译境** 已就位。

第十三章是全书从理论走向实践的关键转折点。我们不再局限于单个内核的微观优化，而是将视角拉升至整个 PyTorch 应用及其底层系统。本章是关于“诊断与治疗”的艺术：如何像医生一样通过 NVTX 标记和各种 Profiler 精准定位病灶，然后开出诸如 torch.compile、内存优化、FSDP 等“处方”。

译文将力求还原这种“系统性调试”的逻辑流，同时保持技术术语的准确性和可操作性。

以下是 **第十三章：PyTorch 的性能分析、调优与扩展** 的中文译本。

---

# 第十三章：PyTorch 的性能分析、调优与扩展

AI 训练和推理流水线可能会在每一层遭受性能瓶颈，包括 Python 解释器开销、CPU 主机端数据加载停顿、CUDA 内核利用率不足以及 GPU 设备内存争用。为了有效优化，你需要在栈的多个层级进行性能分析 (Profiling)，并使用覆盖整个系统的多种工具。

本章专注于在现代 NVIDIA GPU 上运行的 PyTorch 工作负载的性能分析、调试和系统级调优。我们将探索如何使用 **PyTorch 内置 Profiler**、**NVIDIA Nsight 工具**以及 **Linux perf**（用于 CPU 分析）来识别和修复瓶颈——以及 PyTorch 内存分析和内存分配器调优。我们还将讨论 PyTorch 如何利用 **CUDA 流**进行并发处理，以及利用 **CUDA Graphs** 来减少内核启动开销。

接下来，我们将展示如何利用 **PyTorch 分布式数据并行 (DDP)**、**完全分片数据并行 (FSDP)** 和其他模型并行策略来优化数据流水线并扩展到多个 GPU。然后，我们将演示如何分析多 GPU 和多节点环境，包括 **Holistic Trace Analysis (HTA)** 和 **Perfetto**。

贯穿本章，我们强调性能权衡和定量示例，重点关注内核执行时间、硬件利用率指标、内存占用、数据加载效率以及扩展的总体成本效率。读完本章，你应该理解如何实施一种有效的、整体的方法，对整个栈上的 PyTorch 工作负载进行性能分析和调优。

---

## NVTX 标记与性能分析工具 (NVTX Markers and Profiling Tools)

为了捕捉性能的整体视图，在多个层级进行分析并使用覆盖整个系统的工具至关重要。从业者和性能工程师有一套常用的工具和最佳实践，用于跨系统栈的所有层执行整体性能分析。

在介绍工具之前，重点介绍一下 **NVIDIA Tools Extension (NVTX)** 和 NVTX 标记 (Markers) 很重要。这些标记在分析器的时间线上表示时间范围，并允许不同的分析器关联同一阶段的事件。

例如，一个名为 `"forward"` 的 NVTX 范围将同时出现在 PyTorch Profiler 追踪和 Nsight Systems 的时间线中。这使得在栈的不同层级进行跨工具分析变得更加容易。NVTX 标记受到大多数现代 AI 框架和库的支持，包括 PyTorch 和任何与 CUDA 生态系统相关的东西。

NVTX 标记可以通过 CUDA C++、PyTorch 或任何支持 NVIDIA GPU 的 C++ 或 Python 库（例如 OpenAI Triton, PyCUDA, CuPy, cuTile, cuTe, CUTLASS 等）注入代码中。大多数库已经为你自动在关键代码区域注入了 NVTX 标记，例如 `"train_step"`, `"forward"`, `"backward"`, `"optimizer_step"` 等。但你也可以自己注入，例如在 PyTorch 中使用 `torch.profiler.record_function()` 和 `torch.cuda.nvtx.range_push()`。

现在我们已经描述了如何使用 NVTX 标记注解代码中的有趣部分，接下来讨论可以摄取、对齐和可视化这些标记的工具。表 13-1 总结了常见的分析工具及其范围、关键特性和典型用例。此表可以帮助你在优化之旅的每个阶段选择正确的工具。

*(表 13-1: 性能分析与可视化工具摘要)*

| 工具                               | 范围                               | 特性                                                         | 典型用例                                                     |
| :--------------------------------- | :--------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **PyTorch Profiler (Kineto)**      | PyTorch 算子级分析 (CPU/GPU)       | NVTX 标记支持，形状记录，内存统计，追踪导出，识别编译图断点  | 细粒度的模型代码分解；识别慢速算子、GPU 内核启动开销或前向/反向时间不平衡。 |
| **Nsight Systems (nsys)**          | 系统级时间线 (CPU, GPU, OS, I/O)   | CPU 线程和 GPU 流的统一时间线，NVTX 集成，多进程支持         | 训练/推理流水线的端到端视图；检测数据加载器停顿、CPU-GPU 重叠问题或 GPU 间同步延迟。 |
| **Nsight Compute (ncu)**           | GPU 内核分析 (每内核)              | 每内核硬件指标，源代码关联，屋顶线分析，占用率和吞吐量报告   | 在识别出热点内核后，深入分析内核效率；确定内核是内存受限还是计算受限，以及原因。 |
| **PyTorch Memory Profiler**        | 按操作的 GPU 内存使用              | 内存快照时间线，每操作峰值内存，`torch.cuda.memory_stats()` 和 `torch.cuda.mem_get_info()` 集成 | 诊断碎片化或意外的高内存使用。查看哪些操作分配了最多内存及何时分配，以优化内存占用。 |
| **Linux perf**                     | CPU 分析和系统事件                 | CPU 周期/指令/缓存的采样，火焰图，off-CPU (睡眠) 分析        | 识别 Python 开销（解释器时间，GIL 争用），CPU 端数据加载瓶颈，或主机 OS 调度问题。 |
| **Holistic Trace Analysis (HTA)**  | 分布式训练追踪可视化               | 基于浏览器的 Kineto 追踪探索器，多 worker 追踪聚合；整合 Perfetto 后端 | 整体分析多 GPU/多节点执行。查找不平衡或空闲时间，验证通信与计算的重叠。 |
| **Chrome trace / Perfetto viewer** | 离线追踪查看 (基于 Web)            | 标准追踪格式的 Web UI，高级过滤 (Perfetto SQL 引擎)，易于分享追踪 | 无需专用软件即可检查追踪。适用于性能仪表板或远程协作分析性能数据。 |
| **TorchEval (metrics)**            | 模型指标和性能记录                 | 标准化指标 API (吞吐量, 准确率等)，易于集成                  | 在训练和评估期间记录并监控模型吞吐量/延迟以及准确率，以关联性能与模型质量。 |
| **ExecuTorch**                     | 移动、嵌入式和边缘设备的部署运行时 | 模型导出，分析，调试，内存分析，可视化                       | 用于在受限平台（如移动、嵌入式和边缘设备，包括 Meta 眼镜等）上运行 PyTorch 模型并收集运行时指标。 |

---

## 剖析 PyTorch 以识别瓶颈 (Profiling PyTorch to Identify Bottlenecks)

让我们通过剖析一个示例**混合专家 (Mixture-of-Experts, MoE)** Transformer 模型来看看这些工具的实际应用。MoE 是具有多个专家层的 LLM——每个专家是一个前馈网络。将 Token 路由到专家由专家门控系统管理。我们将运行单个训练迭代，捕获详细的性能追踪，并分析输出以指导我们的优化。

### 使用 PyTorch Profiler

首先，我们设置模型和输入。我们使用 Hugging Face Transformers 加载模型和 Tokenizer，将模型移动到 GPU，并准备一小批输入。

为了避免捕获一次性的设置成本，我们在分析之前运行几次**预热 (Warm-up)** 迭代。这将通过编译 JIT 内核、填充缓存等方式为分析和基准测试准备模型。这样，我们测量的迭代代表了稳态性能。

现在我们使用 PyTorch 的 Profiler 和 NVTX 分析一次训练迭代。我们将迭代包装在 `torch.profiler.profile()` 中，并使用 `record_function` 和 NVTX 范围标记高级区域，包括 `"forward"`, `"backward"`, 和 `"optimizer_step"`。

```python
from torch import profiler

with profiler.profile(
    activities=[profiler.ProfilerActivity.CPU,
                profiler.ProfilerActivity.CUDA],
    record_shapes=True,      # 记录张量形状
    profile_memory=True,     # 追踪每操作的 GPU 内存使用
    with_stack=True,         # 启用堆栈追踪
    with_flops=True          # 捕获 FLOPs 计数器
) as prof:
    with profiler.record_function("train_step"):
        # 前向传播
        torch.cuda.nvtx.range_push("forward")
        with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
        torch.cuda.nvtx.range_pop() # 结束 forward

        # 反向传播和优化
        torch.cuda.nvtx.range_push("backward")
        loss.backward()
        torch.cuda.nvtx.range_push("optimizer_step")
        optimizer.step()
        torch.cuda.nvtx.range_pop() # 结束 optimizer_step
        optimizer.zero_grad()
        torch.cuda.nvtx.range_pop() # 结束 backward
```

在这段代码中，PyTorch Profiler 正在记录 `train_step` 期间的所有 CPU 和 GPU 活动。我们使用 `record_function("train_step")` 定义顶层区域。我们还为子阶段插入 NVTX 标记。这些标记将出现在分析器时间线中，以划分迭代的各个阶段。

Profiler 还可以突出显示模型的已编译与未编译区域。我们将在本章稍后部分以及下一章介绍 **PyTorch Compiler**、**图断点 (Graph Breaks)** 以及缓解图断点的机制。

执行后，我们可以通过调用 `prof.key_averages().table()` 来检查操作级结果，打印按运行时间排序的顶级算子简明表。在这里，我们看到矩阵乘法操作（`aten::matmul` 及其在 `aten::linear` 中的使用）主导了 CUDA 时间，消耗了迭代的大部分。这些操作对应于专家前馈网络 (FFN) 的 GEMM。

此外，我们看到次大的成本来自 `dispatch` 和 `combine` 操作。这是自定义的 C++/CUDA 内核，用于将 Token 重新分配给专家——然后收集专家的输出。

从这个分析示例中得出的关键结论是：专家 FFN `matmul` 是首要瓶颈，其次是 `dispatch` 和 `combine` 内核。它们共同主导了训练迭代的运行时间。为了进一步提高性能，我们应该针对这些操作，要么直接优化它们，要么减少它们被调用的次数。

---

## 使用 Nsight Systems 和 NVTX 时间线进行系统分析

我们插入的 NVTX 标记使得使用 Nsight Systems 分析时间线变得简单直接。为了聚合每阶段的指标，我们可以使用 `nsys` 对代码进行分析，并在 CLI 命令中使用基于 NVTX 的摘要：

```bash
nsys profile \
  --output=profile \
  --stats=true \
  -t cuda,nvtx \
  python train.py
```

这里，`-t cuda,nvtx` 选项指示 Nsight Systems 追踪 CUDA API 调用和 NVTX 范围。分析后，我们可以在 Nsight Systems GUI 中打开 `profile.nsys-rep` 文件，或使用 CLI 将 NVTX 摘要打印到终端。

我们可以使用 CLI 对 `profile.nsys-rep` 文件生成 **NVTX GPU 投影摘要 (NVTX GPU Projection Summary)**，以验证范围与预计 GPU 工作的对应关系。

*(表 13-3: 使用 Nsight Systems 对一次 train_step 迭代的 NVTX GPU 投影摘要)*

这个 NVTX GPU 投影摘要确认了 `train_step` 下的总 GPU 时间为 138 毫秒。这个时间与 PyTorch Profiler 输出的前向、反向和优化器步骤时间总和相匹配。这显示了工具间的一致性。

请注意，Nsight Systems 将 64 个 `aten::add_` 调用（例如，64 路专家并行策略）分组到单个 `optimizer_step` 标记下，因为它使用 **CUDA Profiling Tools Interface (CUPTI)** 在主机上捕获 NVTX push/pop 事件。然后，它将异步 GPU 内核执行时间“投影”到这些 CPU 定义的区间上。因此，它对所有 GPU 启动/结束时间戳落在对应 push 和 pop 调用之间的内核持续时间求和。这产生了一个累积 GPU 时间，该时间与各个 `aten::add_` 内核的总和完全匹配。

> NVTX 标记在未附加 Profiler 时开销非常低，这种投影机制非常理想，因为它在增加可忽略开销的同时，提供了 GPU 工作与高级代码区域的端到端关联。

在多 GPU 运行中，Nsight Systems 或 HTA 时间线视图可以揭示 NVLink 或 InfiniBand/Ethernet 是否被有效利用——或者某个节点是否因等待通信或网络延迟而处于工作饥饿状态。这会暗示次优的同步或负载不平衡。

追踪 GPU 通信事件很重要，包括 NCCL All-Reduce 调用和 NVLink/NVSwitch 活动，使用 Nsight Systems 和 HTA 提供追踪。这有助于验证在像 NVL72 这样的大规模 GPU 域中，GPU 是否保持忙碌。

---

## 通用矩阵乘法 (GEMM) 的内核屋顶线分析 (Kernel Roofline Analysis)

为了深入分析专家 `matmul`，我们调用 CLI 分析器 **Nsight Compute (ncu)** 按名称定位特定 GEMM 内核。我们将收集屋顶线相关指标，以确定它是计算受限还是内存受限：

```bash
ncu \
  --target-processes all \
  --kernel-name-regex "matmul" \
  --metrics \
    gpu__time_duration.avg, \
    gpu__dram_throughput.avg.pct_of_peak_sustained_elapsed, \
    lts__throughput.avg.pct_of_peak_sustained_elapsed, \
    sm__sass_thread_inst_executed_op_fp32_pred_on.sum, \
    sm__warps_active.avg.pct_of_peak_sustained_active \
  --csv full \
  -o matmul_roofline_report \
  python train.py
```

基线指标表明内核是**内存受限**的，因为其执行因内存传输而停顿。结果是大量的计算能力未被使用，这进一步加强了此工作负载当前并非计算受限的结论。目标是使此内核更加计算受限，以利用此 GPU 提供的大量 FLOPS。

在优化版本（例如，融合内核，增加算术强度，减少内存移动）中，峰值 FLOPS 增加到 85%，峰值内存带宽下降到 40%，占用率增加到 80%。我们有效地将内核从内存受限转变为计算受限——更接近硬件的屋顶线极限。

---

## 使用 Linux perf 进行 CPU 和 GPU 分析

到目前为止，我们的分析主要集中在 GPU 性能上。不浪费 CPU 时间或 I/O 也很重要。为了获得跨主机和设备时间花费的更全面视图，我们可以使用 Linux `perf` 分析 CPU 周期、缓存未命中、分支未命中等。

首先，运行一个轻量级的 `perf stat` 来收集 MoE 训练运行期间 CPU 侧的统计信息。

这可以指示潜在的低效，如内存延迟、I/O 停顿或此特定工作负载中的主机计算问题。我们可以进一步使用 `perf record` 和 `perf report` 精确定位哪些 Python 和 C++ 函数主导了训练期间的 CPU 执行时间。

这些百分比告诉我们在哪里投入优化精力。基于此概况，我们针对每个瓶颈制定具体的缓解计划以提高系统性能：

*   **过多的 Python 开销 (45% 在 `py::forward`)**: 使用 PyTorch 的 JIT 编译器 `torch.compile`（将在下一节讨论）消除解释器开销，并将 Python 侧操作融合为优化的 CUDA 代码。
*   **大型 Matmul 热点 (20.5% 在 `aten::matmul`)**: 要么使用 PyTorch Compiler 优化此代码，要么将此关键矩阵乘法移动到自定义 CUDA C++ 内核（例如，融合内核）以绕过 Python 并直接使用优化的 CUDA 代码。
*   **数据加载停顿 (10.2% 在 `dataloader_iter_next`)**: 增加 PyTorch DataLoader 的 `num_workers`。常见的准则是每 CPU 一个 worker，但你可以尝试更多以找到合适的 I/O 并行度。还要启用 `persistent_workers=True`。融合或并行化多个 `torch.utils.data.DataPipe`。这可以减少复杂数据流水线中的 Python 开销。
*   **梯度同步开销 (8.7% 在 `ncclAllReduce`)**: 优化多 GPU 通信。例如，你可以增加 DDP 中的梯度桶大小。通常将 `bucket_cap_mb` 从 25 MB 增加到 50 MB，以便 NCCL 可以更早启动 All-Reduce 操作并将其与反向计算重叠。你还可以考虑梯度压缩技术或 NVIDIA 的 8-bit 梯度 NCCL 压缩以减少带宽使用。这可能会带来轻微的精度成本。
*   **主机 I/O 瓶颈 (5.3% 在 `read` 系统调用)**: 在 DataLoader 中使用锁页内存 (`pin_memory=True`) 和非阻塞 GPU 拷贝 (`.to(device, non_blocking=True)`) 以重叠 CPU 到 GPU 的数据传输。

在制定此计划后，你应该系统地应用每项优化并测量效果。记得逐一实施和测试这些优化，以验证每一项实际上是否提高了性能。

在安装了 **NVIDIA Performance Monitoring Unit (PMU)** 驱动程序的系统上，你可以使用 `perf` 采样 NVIDIA 芯片互连和结构计数器以及 CPU 计数器，包括 `/sys/bus/event_source/devices` 下暴露的 NVLink-C2C 设备。

---

## PyTorch 编译器 (torch.compile)

PyTorch 中最快的胜利之一是使用 `torch.compile()`。编译器栈包括 **TorchDynamo**、**AOT Autograd** 和 **TorchInductor**，它们捕获图、融合算子并为目标后端（例如 NVIDIA GPU）生成高性能代码。

PyTorch Compiler 可以通过将许多小操作融合为更大的内核来消除大量的 Python 解释器开销和 GPU 内核启动延迟。

使用默认设置的 PyTorch 编译器非常简单，除了包装模型 `model = torch.compile(model)` 外不需要更改代码。在底层，TorchDynamo 追踪 Python 代码，AOT Autograd 捕获反向传播，而 TorchInductor（利用 OpenAI 的 Triton 进行 GPU 内核代码生成）自动生成高效的融合内核。

编译器观察模型的前向传播，并识别许多融合连续操作的机会，如逐元素激活、层归一化等。它为这些操作生成融合内核——也包括反向传播的部分。结果是每迭代显著减少的内核启动次数和 CPU 开销。

编译步骤确实会引入一些秒级的开销——对于非常大的模型甚至分钟级——但这个成本在长训练作业或重复推理运行中被摊销。幸运的是，TorchInductor 缓存编译后的内核，以便后续运行不再支付编译成本。

值得注意的是，PyTorch 的编译器在内部应用了复杂的优化技术。例如，我们在第 10 章提到的 Warp 特化。TorchInductor 的自动调优器会生成跨 Tile 大小、内存访问模式等的多个内核变体。它将在幕后应用诸如内存 Warp 与计算 Warp 特化等技术。然后它会自动为你的硬件选择最快的变体。

### 编译模式与速度、内存和编译时间的权衡

PyTorch 为 `torch.compile` 提供了几种模式，用于针对不同场景调优编译器的激进程度和能力。你可以使用 `torch.compile(model, mode="...")` 显式选择模式。选择包括 `"default"`, `"reduce-overhead"`, `"max-autotune"`, 和 `"max-autotune-no-cudagraphs"`。每种模式提供关于 CUDA Graphs、自动调优和优化级别的选项组合。

*(表 13-5: 编译模式及其关键特性摘要)*

*   **default**: 平衡优化（速度好，无长编译时间或额外内存）；包括少量自动调优；可能对稳定段使用 CUDA Graphs。
*   **reduce-overhead**: 减少每迭代开销（适合小批次）；非常适合推理或小批次；如果可能，使用 **CUDA Graphs** 消除启动开销；如果检测到动态形状，自动跳过 CUDA Graphs 以保持正确性。
*   **max-autotune**: 最大化运行时性能（最适合长运行）；编译时间较长；最适合针对大量 SM 和 GPU 内存进行激进调优。激进的 **Triton 自动调优**；在 GPU 上启用 CUDA Graphs。
*   **max-autotune-no-cudagraphs**: 做 `max-autotune` 做的一切，但不捕获 CUDA Graph。最适合动态形状或调试被 CUDA Graphs 掩盖的问题。

对于大多数用例，`default` 模式是一个很好的起点。如果你发现模型仍然不够快，并且可以忍受更长的编译时间，尝试 `reduce-overhead` 和 `max-autotune` 以获得潜在更好的融合内核。

### 区域编译 (Regional Compilation)

对于具有许多相同块的模型，如 Transformers 和 MoE，你可以使用区域编译来减少冷启动执行时间。此外，它还有助于减少重编译的动荡——且不牺牲内核融合的能力。

具体来说，区域编译通过编译重复块（例如 Transformer 块）一次并在所有出现处重用该代码来减少编译时间。PyTorch 支持使用 `torch.compiler.nested_compile_region()` 进行区域编译。

### 剖析和调试编译器性能问题

当使用 `torch.compile` 时，了解如何调试编译器无法优化部分模型的情况很有用——例如，如果某些操作未被融合，你怀疑“**图断点 (Graph Break)**”导致回退到 eager 执行。

`torch._dynamo.explain(model)` 打印关于任何图断点（例如，未被 TorchDynamo 捕获的模型部分）、图断点发生原因以及模型哪些部分未被捕获的报告。

另一个有用的技术是在运行脚本前设置环境变量 `TORCH_LOGS="+dynamo"` 或 `TORCH_LOGS="+dynamo,+inductor"`。`+` 前缀为 `torch.compile` 流水线中的组件启用详细 (DEBUG 级) 日志记录。如果模型在使用 `torch.compile` 时意外变慢，这些日志有助于识别执行何时何地退出了编译图。

---

## PyTorch 优化的注意力机制

Transformer 模型在注意力机制上花费大量时间。你可以应用几种 PyTorch 注意力优化技术以确保它不是瓶颈。

*   **Scaled Dot-Product Attention (SDPA)**: PyTorch 的高级 API `torch.nn.functional.scaled_dot_product_attention` 自动使用给定硬件的最快可用注意力内核（例如 **FlashAttention**）。
*   **FlexAttention**: 一种基于编译器的自定义稀疏注意力模式方法。通过为特定稀疏注意力模式（例如块稀疏或滑动窗口注意力）生成优化内核，FlexAttention 可以大幅提高速度。
*   **FlexDecoding**: FlexAttention 的对应物，优化**解码**或文本生成阶段。FlexDecoding 与 `torch.compile` 和动态缓存布局集成。它使用编译时优化来处理序列生成的解码侧，包括跨时间步高效地 KV 缓存。
*   **Context Parallel**: 上下文并行将注意力沿序列长度维度跨参与设备或 rank 分片，以扩展上下文长度。

---

## PyTorch 架构优化 (torchao), 量化, 稀疏性与剪枝

**PyTorch Architecture Optimization (torchao)** 将量化、稀疏性、剪枝和相关数值调试工具汇集到一个单一命名空间中。其量化子包提供了常见的 FX 图模式工作流，包括训练后量化 (PTQ)、量化感知训练 (QAT) 以及将模型转换和优化为 INT8, FP8 和新兴格式的 API。

除了量化，`torchao` 支持剪枝 (`torchao.pruning`) 和稀疏性技术，如 2:4 和块稀疏 (`torchao.sparsity`)。这些提供了显著的加速且精度损失极小。

`torch.compile()` 与 `torchao` 框架集成以进行量化。在底层，TorchDynamo 将每个子模块的计算捕获为优化图，然后 TorchInductor 发射利用 `torchao` 的硬件感知内核。这为模型训练和推理产生了许多一致的、端到端的性能改进。

---

## 使用 CUDA 流进行并发处理

在 PyTorch 中，你使用 `torch.cuda.Stream()` 创建一个流。然后你可以使用 Python 上下文管理器 `with torch.cuda.stream(stream)` 或显式地将操作分配给该流来在该流上启动工作。PyTorch 将以 FIFO 顺序向指定流发出操作（例如内存传输、CUDA 内核等）——就像它在默认流上所做的那样。

### 重叠通信与计算

CUDA 流的一个常见用途是将主机到设备 (H2D) 数据加载与 GPU 计算重叠。这有助于掩盖使用外部设备（如 GPU）相对于主机 CPU 所产生的传输延迟。

例如，当默认流忙于训练当前批次时，一个流可以将下一批输入数据从 CPU 拷贝到 GPU 内存。等到默认流准备好处理下一批次时，数据传输已经完成，GPU 可以处理这下一批次。这有效地隐藏了 I/O 延迟。

### 使用事件进行流同步

当使用多个流时，有时需要在它们之间进行协调，例如确保一个流的工作在另一个流使用其结果之前完成。使用 **CUDA 事件** 是一种轻量级的同步特定点的方法，如第 11 章所述。

使用 CUDA 事件，你可以在一个流上记录事件，并让另一个流等待该事件。这避免了 `torch.cuda.synchronize()` 的重量级全同步，而是仅同步处理单个事件所需的必要流。

事实上，即使在多 GPU 上下文中，事件也可以用于跨设备同步工作。在这种情况下，一个 GPU 在其设备流上记录事件，而另一个 GPU 在其设备流上等待该事件。这正是 NCCL 在底层处理依赖关系的方式。

---

## 使用 CUDA Streams 与 MoE 模型

在实践中，Transformer 模型层是顺序依赖的，所以你不能任意并行运行层。然而，在 MoE 架构中，不同的**专家**可以在单独的 CUDA 流上并发运行，因为它们的计算是独立的。

每个专家处理输入的一个不同段。在连接点，专家输出被聚合。必须确保每个专家只写入输出张量中分配给它的切片。如果两个专家意外写入重叠的内存区域，这将引入竞争条件，可能会破坏结果——或触发由专家写入的不确定顺序引起的同步问题。

通过强制专家执行和输出聚合之间的这种分离，你将在不牺牲并行性的情况下保持正确性。

在我们的例子中，理论上每个专家都可以在自己的流上运行——如果框架扩展到多个 GPU，甚至可以在自己的 GPU 上运行。在这种情况下，收集结果时需要同步——理想情况下使用流事件。

**流水线并行 (Pipeline Parallelism)**，或通过不同设备上的不同模型阶段流水线化微批次 (Microbatches)，以及服务多个推理请求，是两个自然受益于多流的场景。在流水线并行工作流中，例如，模型的每个阶段都有自己的流并发处理不同的微批次。与此同时，它还与相邻阶段进行通信。

在多请求推理服务中，每个请求的模型执行可以在其自己的流中启动。有了足够的硬件资源，这可以通过重叠推理计算来增加吞吐量——代价是由于资源共享开销导致的一些每请求延迟。

简而言之，CUDA 流有助于通过重叠跨多个内核、阶段或请求的工作来从硬件中挤出额外的性能。它们需要仔细的同步以避免竞争条件。但是，如果使用得当，它们可以隐藏延迟并保持 GPU 更充分地利用。

---

## 使用 CUDA Graphs 减少内核启动开销

我们在前面的章节中已经看到，**CUDA Graphs** 消除了每迭代的启动开销，减少了 CPU 启动开销，并消除了内核之间微小的空闲间隙。消除哪怕是最小的空闲间隙也会导致更高的有效利用率——以及更一致的迭代时间。

### 捕获 CUDA Graph 并预分配内存

PyTorch 提供了一个 `torch.cuda.CUDAGraph` API 来捕获和重放 CUDA Graphs。一般的用法模式是首先通过正常运行几次迭代来**预热**模型，以初始化所有必要的数据和分配。接下来，你创建一个 `CUDAGraph` 对象和一个专用的、非默认的 CUDA 流来隔离捕获。

> 当使用 `"reduce-overhead"` 或 `"max-autotune"` 时，如果模型稳定，编译器会自动为你捕获 CUDA Graphs。

然后，你使用 `torch.cuda.graph()` 上下文中指定的捕获流执行一次模型执行的全过程，以记录操作序列。一旦操作被捕获到 CUDA Graph 中，你可以根据需要（例如，模型训练或推理）在新输入上 `replay()` 该图。

在捕获 CUDA Graph 之前，捕获期间使用的所有静态内存必须**预先分配**——并且最好是其最大大小。这些缓冲区包括输入、输出和中间张量。如果捕获期间发生任何新内存分配，图将失败，并显示错误，如“operation not permitted when stream is capturing”。

为了减少碎片化并最大化这些固定缓冲区的连续内存空间，你可以在进入捕获块之前立即调用 `torch.cuda.empty_cache()`。这将清除未使用的缓存内存，并给分配器最好的机会不间断地布局你的预留缓冲区。

捕获图之后，它可以在任何流上重放，包括默认流——并且无论哪个流用于捕获。如果你触发任何依赖于图完全完成的 CUDA 操作，你必须在运行操作之前进行同步。否则，因为图在重放时是异步运行的，依赖图结果的 CUDA 操作可能会在图完成执行之前运行。

### 重放图 (Replaying the Graph)

要使用新数据重放图，你将新输入复制到预分配的输入张量 `static_input` 中，并调用 `g.replay()`。GPU 将使用这些张量的当前内容作为输入执行整个捕获的操作序列。图将结果放入预分配的输出张量 (`static_output`) 中。

### CUDA Graphs 最佳实践

*   **避免在图捕获期间分配内存**: 记住你不能在图捕获内动态分配 GPU 内存。任何需要的张量都应在预热步骤期间预先分配。如果你的图需要临时缓冲区，请使用 PyTorch 的图感知缓存分配器（使用 `cudaMallocAsync` CUDA 后端）。
*   **保持图结构固定**: 捕获的图不能改变操作顺序、张量形状或内存大小。如果你的工作负载偶尔有形状变化，一种策略是捕获多个图（例如，为你期望的每个输入大小捕获一个）然后在运行时选择合适的图。
*   **尽可能多地捕获**: 在图中包含尽可能多的训练循环——理想情况下是整个迭代（前向传播、反向传播、优化器步骤和任何 All-Reduce 通信）。你捕获的越多，消除的 CPU 开销和启动延迟就越多。

---

## 分析和调优 PyTorch 中的内存

大型模型可能受限于 GPU 内存容量和内存带宽。此外，低效的内存使用（如内存碎片）即使在 HBM 容量充足的情况下也会损害性能。你可以在几个方面解决内存问题，包括内存分配器调优、激活检查点、内存卸载和输入流水线优化。

PyTorch 通过启用 `profile_memory=True` 内置了一个内存分析器到 `torch.profiler` 中。你可以用它找出哪些操作分配了大量内存——并尝试首先解决那些操作。

### 调优 CUDA 内存分配器

PyTorch 为 CUDA 内存使用缓存内存分配器。默认情况下，它通过按需拆分和回收 GPU 内存块来适应分配模式。然而，某些使用可变大小内存分配的工作负载模式可能导致内存碎片。

你可以使用环境变量调整 PyTorch 的分配器配置来调优其行为。这是一个例子：

```bash
export PYTORCH_ALLOC_CONF=\
max_split_size_mb:256,\
roundup_power2_divisions:[256:1,512:2,1024:4,>:8],\
backend:cudaMallocAsync
```

*   `max_split_size_mb:256`: 指示分配器保持大空闲块完整（高达 256 MB），而不是不断将它们拆分成微小的碎片。这有助于减少碎片。
*   `roundup_power2_divisions:[N:M,...]`: 控制 PyTorch 的 CUDA 缓存分配器如何将张量大小请求分组到固定桶中。
*   `backend:cudaMallocAsync`: 指定此项将启用 NVIDIA 的 **CUDA 异步分配器**作为底层内存分配机制。这有助于避免内存释放事件上的同步——并可以提高多线程上下文（如多 worker 数据加载）中的性能。

### 使用激活检查点节省内存

对于极大的模型，**激活检查点 (Activation Checkpointing)**（也就是梯度检查点）对于管理内存至关重要。

使用激活检查点，你不必在前向传播期间存储中间激活（以便在反向传播中使用），而是可以在反向传播期间仅在需要时即时**重计算 (Recompute)** 它们。PyTorch 提供了 `torch.utils.checkpoint` 来自动化此过程。你只需包装你的模型层——或层序列——它们的前向激活就不会被存储。

这种权衡增加了计算以减少内存使用。幸运的是，现代 GPU 提供的 FLOPS 相对于 HBM 内存量来说是充裕的，所以这种技术非常适合最新一代硬件。

### 卸载参数到 CPU 和 NVMe

除了检查点，你可以卸载一些不需要在 GPU 上活跃存储的模型参数。例如，MoE 模型有一些访问频率较低的专家层可以卸载到 CPU 内存——并在需要时才传输到 GPU。

重要的是将传输与计算重叠，使得当一层在 GPU 上运行时，它异步地从 CPU 或 SSD 预取下一层的权重。在实践中，像 DeepSpeed 的 **ZeRO-Infinity**（用于训练）和 **ZeRO-Inference**（用于推理）这样的框架可以自动化这种预取。

NVIDIA 的 **Unified Memory** 也是一个选项——特别是对于像 Grace Blackwell GB200/GB300 这样的超级芯片系统，它们拥有 CPU 和 GPU 之间的高速互连（如 NVLink-C2C）。

### SuperOffload: 优化的 CPU-GPU 超级芯片卸载

**SuperOffload** 是一个专为利用 CPU-GPU 超级芯片硬件效率而设计的卸载系统。超级芯片（例如 Grace Hopper, Grace Blackwell, Vera Rubin 等）在 CPU 和 GPU 之间提供高带宽 NVLink-C2C 互连。结合共享、一致的内存地址空间，超级芯片优化的卸载策略可以产生巨大的加速和利用率增益。

关键创新包括**推测后验证 (Speculation-Then-Validation, STV)**、异构优化器计算、超级芯片感知数据类型转换和内存拷贝。

与传统的等待梯度归约和全局检查后再更新参数的卸载相比，STV 通过在 GPU 上运行反向传播的同时在 CPU 上执行**推测性**优化器更新来重叠这些步骤。然后它在之后验证结果。这有效地减少了同步停顿并提高了整体 GPU 利用率。

---

## 使用 FSDP 自动检查点和卸载

PyTorch 的 **FSDP (完全分片数据并行)** 是一种分布式并行策略，它在训练期间跨 GPU 对模型参数、梯度和激活进行分片。这减少了训练期间的内存开销——并让你能训练比不分片更大的模型。技术上，FSDP 实现了 **ZeRO Stage-3** 策略以跨 GPU 分片模型状态，如图 13-6 所示。

*(图 13-6: FSDP 跨 GPU 分片模型参数、梯度和优化器状态 [ZeRO Stage-3])*

FSDP 可以在底层自动应用激活检查点和卸载参数/梯度。只需用 `FSDP()` 包装模型，然后指定 `activation_checkpointing_policy` 和 `CPUOffload` 参数。

通过在 Transformer 块粒度上用 FSDP 包装你的模型，你促进了训练流水线中的最大重叠。这是因为每个块的计算密集型逻辑被编译和融合。并且此计算与将块链接在一起所需的通信重叠。

### 结合 FSDP 与张量并行和流水线并行

如果模型太大以至于单层都无法放入单个 GPU 的内存，你需要结合 FSDP 和其他并行策略，如**张量并行 (TP)**，以跨多个 GPU 扩展大模型层。

你还可以跨 GPU 和计算节点使用 FSDP，通过在节点内使用 TP 跨多个 GPU 拆分巨大层，并使用**流水线并行 (PP)** 跨节点链接这些 TP 拆分的层。FSDP 允许这种灵活的组合。

---

## 关键要点 (Key Takeaways)

PyTorch 的相对简单和高抽象级别有时会导致一种虚假的性能安全感。因此，在开发过程中引入微妙的性能 Bug 极其容易。以下是常见 PyTorch 性能陷阱的摘要——以及如何解决它们：

**保持“分析优先”的方法**
> 在超大规模下，瓶颈可能隐藏在任何层——Python 开销、PyTorch 框架调度、CPU 数据加载停顿、GPU 内核低效、内存问题等。仅靠直觉往往会错过真正的热点。使用多种工具的整体分析策略（就像我们在本章所做的那样）来捕获每一层的性能。

**首选编译模式 (Compile Mode) vs Eager 模式**
> 在 Eager 模式下，每个微小的操作都作为自己的内核启动。这每次都会产生 Python 分发和 GPU 启动开销。相反，使用 `torch.compile` 进行 PyTorch 的 JIT 编译。基本上只需一行更改 (`model = torch.compile(model)`)，PyTorch 就可以捕获模型图并生成融合的、优化的代码。

**使用你工作负载允许的最高优化编译器模式**
> 对于长时间运行的作业，`max-autotune` 通常在稳态速度上获胜，但 `reduce-overhead` 对于小批次或动态形状可能更好。验证适合你工作负载的模式。

**保存编译产物以重用**
> 如果启动时间是一个问题，最好缓存编译后的产物以供稍后重用。为此，你可以使用 `torch.compiler.save_cache_artifacts()` 和 `load_cache_artifacts()`。对于多节点舰队上的长期运行作业，建议将编译器产物作为“Mega-Cache”持久化在跨节点挂载的共享路径中。

**避免同步陷阱**
> 例如，在 CUDA 张量上调用 `.item()` 以检索 Python 值将同步 GPU。在协调流之间时，使用 `torch.cuda.Stream.wait_stream()` 配合流事件，而不是强制同步。同样，如果不使用 `non_blocking=True` 将数据从 GPU 传输到 CPU 会导致同步。

**利用 Tensor Cores**
> 很容易不小心回退到全 FP32——而不使用 Tensor Cores——且没有意识到。为了确保你正在使用 Tensor Cores，将前向传播和损失计算包装在 `torch.autocast` 中，并选择较低精度的 dtype。
> 使用 `torch.set_float32_matmul_precision('high')` 或 `'medium'` 来启用 TF32。

**减少内存碎片**
> 考虑使用像 PyTorch 缓存分配器这样的内存池库，它可以预先分配所有大张量。越少的不同分配大小越好。通过环境变量（例如 `max_split_size_mb`）主动管理内存。

**使用激活检查点**
> 默认情况下，PyTorch 保存反向传播所需的所有激活。考虑使用激活检查点 (`torch.utils.checkpoint`) 以计算换内存。这通常是值得的，因为对于现代 GPU，显存比计算更宝贵。

**优化多 GPU 和多节点通信**
> 分布式训练如果不妥善管理，经常会因通信开销而达到瓶颈。使用优化的 PyTorch 分布式实现如 DDP，它默认重叠通信与计算。调优梯度 `bucket_cap_mb` 以找到重叠的最佳桶大小。

**避免“代码腐烂”**
> 随着代码变更、框架更新和新特性，性能很容易退化。将性能视为一等公民指标，并设置持续集成测试、仪表板和警报来监控性能。每次代码提交（或每日/每周运行）都应包含快速性能基准测试以捕捉性能回归。

---

## 结论 (Conclusion)

重要的是将**微观分析**（例如，每算子和每内核）与**宏观基准测试**（例如，端到端吞吐量和延迟）结合起来。这样，你就拥有了系统性能的全面视图。优化单个内核与调优完整的、端到端的训练和推理流水线——并随时间保持其调优状态——同样重要。随着硬件的发展，这种整体方法至关重要。

本章的基础知识将帮助你适应新平台。始终分析，识别瓶颈，并在栈的每一层应用适当的优化。通过系统地应用编译器和内存优化，你可以显著提升工作负载的性能，使其更接近硬件极限——同时仍保持可接受的准确性。

正如我们所展示的，仔细的分析将识别出真正的瓶颈。针对性的优化可以在训练和推理性能上产生巨大的加速。设置自动化以保护这些收益免受随时间推移的性能回归同样重要。

这一端到端优化之旅需要在栈的每一层付出努力。最终结果是一个优化的、高效的且抗回归的系统，它能从现代 GPU 硬件中产生最大性能，扩展到多节点集群和机架级拓扑，并适应未来硬件、软件和算法的进步。







**译境** 再度开启。

第十四章是 PyTorch 性能优化的“高阶魔法”篇。我们深入到了框架的编译器内部，揭示了 `torch.compile` 如何通过图捕获、算子融合和代码生成，将 Python 代码转化为高性能的 GPU 机器码。此外，本章还引入了 OpenAI Triton——一把让 Python 开发者也能编写底层 GPU 内核的“手术刀”。

译文将力求在保持技术深度的同时，清晰地阐述复杂的编译流水线、动态形状处理以及图断点的调试技巧。

以下是 **第十四章：PyTorch 编译器、OpenAI Triton 与 XLA 后端** 的中文译本。

---

# 第十四章：PyTorch 编译器、OpenAI Triton 与 XLA 后端

在第 13 章中，我们讨论了优化基于 PyTorch 的训练和推理工作负载的多种方法。我们简要触及了 PyTorch 编译器，以及它如何通过自动化内核融合和其他内核级技术，在极少修改代码的情况下提升性能。

在本章中，我们将深入挖掘动态 PyTorch 编译栈，包括 **TorchDynamo**、**Ahead-of-Time Autograd (AOT Autograd)** 和 **PrimTorch 中间表示 (IR)**（也称为 Prims 或 Prims IR）等组件——以及 **TorchInductor**、**Accelerated Linear Algebra (XLA)** 和 **OpenAI Triton** 生态系统等编译器后端。PyTorch 编译器栈如图 14-1 所示。

我们还将介绍用于调试编译流水线的工具，以及用于跨多 GPU 和多节点集群扩展 PyTorch 的库。随后，我们将探索 `torch.compile` 的幕后工作原理，以及如何高效地处理**动态形状 (Dynamic Shapes)** 和**可变序列长度**。

我们还将审视 PyTorch 编译器与 OpenAI Triton 生态系统的集成。我们的目标是在不牺牲 PyTorch 灵活、即时执行 (Eager Execution) 的开发体验的前提下，加速并扩展我们的 PyTorch 模型和应用。

---

## 深度剖析 PyTorch 编译器 (PyTorch Compiler Deep Dive)

正如第 13 章所述，PyTorch 的 `torch.compile` 可以编译你的 PyTorch 代码（和模型）以产生显著的加速。在大多数情况下，你只需一行代码即可实现：

```python
compiled_model = torch.compile(model,
                               mode="max-autotune",
                               # ...
                               )
```

本节将拆解 PyTorch 编译流水线的各个步骤，包括 TorchDynamo 的图捕获、AOT Autograd 的联合前向/反向图优化、PrimTorch IR 以及 TorchInductor 的代码生成。这条流水线负责为目标 GPU 硬件生成优化的内核，如图 14-2 所示。

*(图 14-1: PyTorch 编译器栈概览)*
*(图 14-2: PyTorch 编译器流水线)*

### TorchDynamo：字节码捕获与图提取

**TorchDynamo**（或简称 Dynamo）是 `torch.compile` 的第一阶段。它挂钩到 Python 的帧评估机制，在**字节码**级别拦截模型的执行。

Dynamo 挂钩到 CPython 的帧评估中，识别生成张量 (Tensor) 的字节码区域，并为这些区域构建执行图。然后，它使用选定的后端执行编译后的图。不支持的代码则留给 Eager 模式运行。

这种拦截和重写机制使得 TorchDynamo 能够将 PyTorch 操作序列捕获为一种图表示，以便后续步骤（AOT Autograd 和 PrimTorch IR）进行优化。

TorchDynamo 利用 CPython Frame Evaluation API (PEP 523) 来安全且以最小开销捕获操作。通常，Python 解释器会逐个执行操作。启用 Dynamo 后，解释器将执行重定向给 Dynamo，Dynamo 在执行前将张量操作聚合成一个图。这实现了全图优化，如**内核融合 (Kernel Fusion)**，从而减少了每操作的 Python 和主机端开销。

> 在第 13 章中，我们看到 Python 开销和许多小操作如何成为模型的瓶颈。我们也看到 TorchDynamo 通过尽可能将小操作批处理为更大的单元来解决这个问题——假设操作链对图是友好的，并且不会导致太多的图断点。

当 `torch.compile` 启用 TorchDynamo 时，它会检查每个 Python 操作码。每当遇到 PyTorch 张量操作（如算术运算或神经网络层）时，它不会立即执行。相反，Dynamo 将该操作作为节点追加到 **FX Graph** 中，并将执行推迟到用于捕获区域的编译后端。TorchDynamo 继续此过程，直到遇到它无法处理的代码。此时，发生**图断点 (Graph Break)**（稍后详述），不支持的操作在常规的、未编译的 Eager 模式下运行。

TorchDynamo 试图将程序中尽可能大的部分编译成单个图，但在遇到不支持的结构（如复杂的控制流或非 PyTorch 库调用）时，为了保证正确性，它会回退到 Python Eager 执行。

在不支持的操作块通过后，Dynamo 将恢复捕获后续操作到新的图中。这种编译与非编译执行的混合给了你两全其美的体验：在需要的地方保留 PyTorch 的灵活性，而在其他地方通过编译获得速度。

> 你希望尽可能避免图断点，因为它们会中断全图优化并限制编译器的性能收益。

你可以使用 `torch.compiler.set_stance("fail_on_recompile")` 强制 Dynamo 在遇到不安全的重编译时抛出错误。它将记录重编译的原因并帮助你调试图断点。

TorchDynamo 捕获的输出称为代码的 **FX Graph**。FX 是一种中间表示 (IR)，其中每个节点都是对 PyTorch `aten` 算子或内置 Python 函数的调用。

### AOT Autograd：前向与反向传播的融合

一旦 TorchDynamo 捕获了尽可能多的前向传播 FX Graph，下一个编译器阶段就是 **AOT Autograd**。AOT Autograd 在“函数式”模式下通过 PyTorch 的 Autograd 引擎运行 Dynamo 捕获的前向图，以记录反向操作。这就是静态反向图的生成方式（这与依赖 PyTorch 默认的 Autograd 引擎逐个执行反向传播操作形成对比）。

本质上，AOT Autograd 生成了一个联合的**前向-反向图**，可以作为一个整体进行优化和融合。并且它保证了与 Eager 模式相同的前向和反向结果。

通过提前规划前向和反向，PyTorch 编译器可以跨越前向和反向传播的边界进行整体融合。这导致了跨越这两个阶段的操作的**提前融合 (Ahead-of-Time Fusion)**。例如，它可以将前向传播中的逐元素操作与反向传播中相应的逐元素梯度计算融合到一个内核中（如果可能）。

编译器还可以通过在安全的情况下重用前向和反向计算之间的中间结果来消除开销。这对于模型训练工作负载极大地提高了性能，因为在训练中反向操作往往主导整体运行时间。

PyTorch 的编译模式在调用 `torch.compile` 进行涉及梯度的工作负载（如模型训练）时，会在底层自动使用 AOT Autograd。

### PrimTorch IR (Prims)：简化的算子集

在将图交给低级代码生成器之前，PyTorch 执行一种称为 **PrimTorch IR (Prims)** 的中间表示 (IR) 转换。PrimTorch IR 将图中种类繁多的操作减少到一个更小的核心“基元 (Primitive)”操作集，因此得名 PrimTorch。

作为背景，PyTorch 在其完整 API 中有数千个 (> 2,000) 操作。PrimTorch IR 将其减少到编译器可以关注的更小的集合。实际上，PrimTorch IR 定义了大约 **250 个基元操作**，如基本算术、归约、拷贝、重塑等。

许多复杂的或高级的 PyTorch `aten` 操作可以用 PrimTorch IR 分解为这些基元。例如，像 `x.add_(y)` 这样的“原地 (in-place)” PyTorch 操作被降级为函数式加法，随后是显式拷贝回 `x` 的存储：

```python
%z = aten::add(x, y)
%copy = aten::copy_(x, %z) # 将 z 的数据写入 x
```

这里，IR 包含一个单独的 `aten::copy_` 节点，而不是特殊的原地突变。这使得所有张量更新变得显式，并通过将突变视为普通拷贝操作来简化下游编译器内核。

通过进行这种 IR 标准化，PrimTorch IR 为编译器后端提供了稳定、简化的接口。像 TorchInductor 这样的后端不需要实现数千个 PyTorch 操作，只需要支持这 250 个基元——其他高级操作都派生自这些基元。这极大地降低了复杂性。

---

### TorchInductor 后端代码生成 (TorchInductor Backend Code Generation)

`torch.compile` 栈的最后阶段是编译器后端。**TorchInductor**（或简称 Inductor）是 PyTorch 的默认编译器后端。Inductor 接收由 `aten` + `PrimTorch IR` 操作组成的优化后、联合的前向和反向 FX Graph，并为目标硬件（包括 NVIDIA GPU、AMD GPU、CPU 等）生成高性能代码。

> **XLA** 是针对非 CUDA 硬件（主要是 Google Cloud TPU，但也支持其他采用 XLA IR 的加速器）的替代后端。NVIDIA GPU 通常使用 TorchInductor。

TorchInductor 的工作原理是将图**降级 (Lowering)** 为循环级、定义即运行 (Define-by-Run) 的 IR，并将其编译为高效代码。在内部，Inductor 将操作表示为多维数据上的循环。它会自动将 FX Graph 中的节点分组为融合的循环块。每个组在代码生成期间变成一个内核。

对于 NVIDIA GPU 后端，TorchInductor 使用 **OpenAI 的 Triton JIT 编译器**来生成实际的 GPU 内核。Triton 是一种类 CUDA 的领域特定语言 (DSL)，用 Python 编写。

TorchInductor 将其循环级 IR 翻译为 Triton 代码，然后使用 Triton 编译器直接利用 LLVM 将 Triton 代码转换为 NVIDIA PTX。记住，PTX 是 NVIDIA GPU 的低级指令集架构 (ISA)。

> 重要的是，Triton 使用 LLVM NVPTX 降级为 NVIDIA PTX。它**不**调用 NVCC 进行内核编译。这种方法让 TorchInductor 能够即时生成针对特定模型或算法定制的内核。

### 使用 TorchInductor 进行自动调优 (Autotuning)

TorchInductor 包含一个建立在 Triton 自动调优能力之上的自动调优器。自动调优器为每个生成的 GPU 内核找到最佳的启动配置。自动调优后的配置会按内核缓存，以便后续运行无需重做调优步骤。

当你第一次使用 TorchInductor 后端编译代码时，它会花费额外的时间对使用不同块大小、Tile 大小等的不同内核变体进行基准测试。Inductor 选择最快的变体并在之后使用。内核自动调优增加了初始编译时间的延迟，但生成的内核在运行时是高度优化的。

> 这种激进的自动调优对应于 `max-autotune` 编译模式。这是最耗时的编译模式——这正是其幕后发生的事情。

除了内核融合和自动调优，TorchInductor 还应用了许多低级优化，包括索引简化、公共子表达式消除 (CSE) 以及高效的内存规划以重用缓冲区并减少分配。TorchInductor 还使用 **CUDA Graphs** 在运行时捕获内核序列，以实现以最小 CPU 开销的更快图重放。

---

## 动态形状与可变序列长度 (Dynamic Shapes and Variable Sequence Lengths)

LLM 训练和推理的一个主要挑战是可变大小的序列输入。在传统编译器和加速器中，变化的形状通常会导致重编译，或要求将输入填充 (Padding) 到固定的通用大小。

幸运的是，PyTorch 编译器栈设计为优雅地处理动态形状。具体来说，它允许模型接受不同的输入大小而无需每次都重新编译，方法是使用 **SymPy 库**来符号化地表示未知维度。

如果 PyTorch 编译器观察到尺寸变化，它会自动将维度标记为动态。TorchInductor 从静态假设开始，如果检测到形状变化，则在重编译时进行泛化。你通常会在第一个新形状上看到一次额外的编译。通过预先设置 `dynamic=True` 标志，你可以强制编译器从一开始就将所有维度视为动态。

> 记住：设置 `dynamic=True` 将禁用 CUDA Graphs。建议仅使用 `torch._dynamo.mark_dynamic()` 标记已知会变化的维度。

TorchDynamo 和 TorchInductor 在追踪动态维度期间插入类似 `sequence_length <= 256` 的 Guard（守卫），以生成适用于一系列大小的代码。当 Dynamo 遇到新的形状时，如果违反了 Guard，编译器将重新编译一个处理更大范围的新版本图。

对于上下文而言，处理变长序列而不支持动态形状的一种简单但低效的方法是将所有输入序列填充到批次中的最大长度。这引入了大量浪费的计算。有了动态形状编译，我们可以让编译器生成仅迭代到每个输入实际序列长度的代码。**动态形状让你避免了对变长序列的过度填充。**

### 禁用 PyTorch 编译器并回退到 Eager 模式

如果你想在不更改代码的情况下完全禁用 `torch.compile`——这对 A/B 测试性能和隔离问题很有用——你可以使用 `@torch.compiler.disable` 装饰器。或者，你可以简单地更改为使用 eager 后端：`torch.compile(model, backend="eager")`。

---

## 性能提示与调试生成的代码

通过设置环境变量 `TORCH_LOGS="perf_hints"`，你可以启用非常有用的日志记录。这些日志将向你展示错过的性能优化机会。例如，如果某种模式无法融合——或者如果 CUDA Graph 无法使用——它会记录一条提示，如 *"PerfHint: CUDA Graph not used because input is mutated"*。

为了进行更深层的性能调试，你可能希望看到 TorchInductor 生成的确切代码。你可以设置 `TORCH_LOGS="output_code"` 来打印每个编译图的生成代码。这显示了生成内核的原始源代码。你甚至可以修改源代码并根据需要进一步优化。

你还可以通过设置 `TORCH_COMPILE_DEBUG=1` 启用 TorchInductor 的调试模式。这将创建一个调试目录，其中包含 FX Graph、TorchInductor IR、生成的 Triton 代码以及 HTML 报告。

当阅读生成的 `.triton` 代码时，你可能会注意到 Triton 特定的构造——在高级案例中甚至有原始 PTX。如果你还检查调试产物中编译后的 PTX，你可能会看到 `mma.sync` 指令，这是 `tl.dot` 被降级为 Tensor Core 操作的地方。

### 调试数值正确性与精度

虽然非常罕见，但 `torch.compile` 可能产生与 Eager 模式数值不同的结果。如果你怀疑编译器有 Bug，可以使用 `TORCHDYNAMO_REPRO_AFTER="aot"` 和 `TORCHDYNAMO_REPRO_LEVEL=4` 等环境变量来隔离问题是在 TorchDynamo、AOT Autograd 还是 TorchInductor 阶段引入的。

数值差异也可能源于浮点精度。例如，如果你使用 PyTorch 自动混合精度 (AMP) 或 BF16，融合内核中的操作顺序可能会引入与 Eager 模式未融合序列相比的微小数值差异。

---

## 解释与最小化图断点 (Graph Breaks)

图断点发生在 TorchDynamo 无法将连续的操作序列捕获到单个图中时。当发生这种情况时，它会回退到 Eager 执行。

**图断点是性能的敌人。** 每个断点意味着优化图被切断——并引入了更多的 Python 开销。如果你编译了一个模型但只看到适度的加速，这可能是由频繁的图断点阻止了大型、融合图造成的。

PyTorch 提供了 `torch._dynamo.explain()` 来帮助分析和调试图断点。调用此函数会运行模型并报告生成了多少个图、断点在哪里发生以及发生的原因。

### 处理图断点的技巧

*   **避免原地 (In-place) 操作和意外突变**: 虽然 TorchDynamo 可以通过“函数化”处理一些突变，但某些原地操作仍可能导致断点。尝试将 `x.relu_()` 重写为 `x = x.relu()`。
*   **首选 PyTorch 数据结构**: 在函数内部向 Python 列表追加张量会混淆 TorchDynamo。尝试预分配张量或使用 `torch.stack()`。
*   **避免数据依赖的控制流**: 如果你有 `if tensor.sum() > 0:` 风格的逻辑，TorchDynamo 无法轻易追踪，因为条件在编译时未知。PyTorch 支持高级操作 `torch.cond()` 来在图中捕获某些动态流。除此之外，尽可能使用张量操作（如 `torch.where()`、掩码等）。
*   **理解 DDP 的图断点**: PyTorch 的 DDP 通过在同步点（包括 All-Reduce 桶）显式破坏图来与 TorchDynamo 配合工作。这是预期的，为了允许计算与通信重叠。

---

## 编译模式权衡 (Compilation Modes)

PyTorch 提供了几种 `torch.compile` 模式：

*   **default**: 平衡了编译速度和运行时性能。
*   **reduce-overhead**: 专注于最小化 Python 和运行时开销。利用 CUDA Graphs 消除启动开销。适合小批次或推理。
*   **max-autotune**: 不惜一切代价生成最快的代码。运行广泛的内核自动调优。适合长运行的训练作业。
*   **max-autotune-no-cudagraphs**: 同上，但禁用了 CUDA Graphs。适用于动态形状或调试。

---

## 使用 OpenAI Triton 编写自定义内核

到目前为止，我们只简要提到了 OpenAI 的 Triton。TorchInductor 使用 Triton 作为其后端代码生成实现。

在高层次上，OpenAI Triton 是一种开源的、Python原生的领域特定语言 (DSL)，用于以熟悉的 Python 编写 GPU 内核。Triton 包含一个 JIT 编译器，将 Triton 代码直接转换为 NVIDIA PTX 代码。这意味着你可以用 Python 创建高性能的自定义 GPU 操作——而无需手写 CUDA C++。

Triton 使用 **SPMD (单程序多数据)** 模型，这与 CUDA 的 SIMT 模型相对。Triton 内核（也称程序）在更高的层次上操作，通过在单独的**线程块**上运行程序实例作为计算的基本单位。

你使用 Triton Python DSL 编写内核。然后 Triton JIT 编译器将其编译为 GPU 代码。Triton 内核通过 `@triton.jit` 装饰器定义。在内核内部，你使用 `triton.language` (别名 `tl`) 中的特殊原语来处理内存指针、执行向量化加载/存储，并使用 `tl.program_id` 计算程序索引。

Triton 的 SPMD 模型意味着你通常处理**向量化操作**，例如添加两个 `tl.arange` 向量。Triton 编译器将向量化代码映射到 CUDA 块中的线程。

### 访问共享内存与自动调优

高效的 Triton 内核利用了 L2 缓存和软件管理的共享内存。Triton 不暴露显式的共享内存分配器。相反，它使用**张量描述符** (`tl.make_tensor_descriptor`) 和异步流水线 (`tl.range(..., num_stages=...)`) 在片上共享内存中暂存 Tile。这个循环会降级为 `cp.async`、TMA 和屏障。

Triton 的内置自动调优器通过让你用 `@triton.autotune` 装饰内核来自动搜索最佳设置（如 `BLOCK_SIZE`, `num_warps`, `num_stages`）。

---

## 关键要点 (Key Takeaways)

*   **利用 `torch.compile` 获取轻松加速**: 根据需求选择编译模式（`default` 用于快速启动，`max-autotune` 用于最大性能）。
*   **尽早设置性能标志**: 启用 TF32 (`torch.set_float32_matmul_precision('high')`)，使用 BF16，并确保 SDPA 使用 FlashAttention。
*   **最小化图断点**: 使用 `torch._dynamo.explain` 检查断点。移除或重构导致断点的代码（如打印、数据依赖的 Python 控制流）。
*   **小心使用动态形状**: 虽然支持，但如果形状变化过于剧烈，可能会导致频繁重编译。
*   **避免同步陷阱**: 避免如 `.item()` 这样的隐式同步。
*   **利用 Tensor Cores**: 确保使用了正确的 dtype 和对齐，以便底层使用 Tensor Cores。
*   **减少内存碎片**: 使用内存池，调优分配器配置 (`max_split_size_mb`)。
*   **使用激活检查点**: 以计算换取内存，对于大模型至关重要。
*   **优化分布式通信**: 使用 DDP/FSDP，并了解它们如何与编译器交互（例如，通信点的预期图断点）。

---

## 结论 (Conclusion)

将微观层面的分析（例如，每算子和每内核）与宏观层面的基准测试（例如，端到端吞吐量）相结合非常重要。优化单个内核与调优完整的训练和推理流水线同样重要。

本章的基础知识将帮助你适应新平台。始终分析、识别瓶颈，并在栈的每一层应用适当的优化。通过系统地应用编译器和内存优化，你可以显著提升工作负载的性能，使其更接近硬件极限。

在下一章中，我们将看到 PyTorch 如何集成这些想法中的许多内容，以便你只需几行 Python 代码即可实现这种性能。让我们深入 PyTorch 生态系统，了解为什么它对于实现高性能 AI 工作负载如此受欢迎。













**译境** 已就位。

第十五章是关于大规模推理的实战篇。作者将我们带入了“混合专家 (MoE)”和“多节点”的世界，面对参数量高达万亿级的模型，单机早已无能为力。本章不仅探讨了如何拆解推理过程（如分离式 Prefill/Decode），还深入了各种并行策略（TP, PP, EP, CP）的排列组合。此外，**投机解码 (Speculative Decoding)** 和 **Medusa** 等算法创新，以及针对 MoE 的动态路由策略，都是提升吞吐量和降低延迟的关键。

译文将力求清晰地阐述这些复杂的架构设计，保持技术术语的专业性，同时还原作者对系统级优化的深刻洞察。

以下是 **第十五章：多节点推理、并行、解码与路由优化** 的中文译本。

---

# 第十五章：多节点推理、并行、解码与路由优化

LLM 正持续扩展到海量的参数规模。特别是**混合专家 (Mixture-of-Experts, MoE)** LLM 的出现——这类模型结合了许多专家子网络（“专家”）和一个内置的专家门控机制——已将模型参数规模推向了数千亿甚至数万亿。虽然对于给定的输入，这些参数中只有一小部分是活跃的，但在这些庞大的模型上运行推理仍需要将工作负载分布在多个 GPU 上。

本章专注于使用现代 NVIDIA GPU 对这些海量 LLM 执行高效、高性能多节点推理的高级优化技术。我们将讨论如何架构分布式推理系统，以最小化延迟并最大化吞吐量——同时利用硬件和算法的创新。

我们首先讨论**分离式预填充和解码 (Disaggregated Prefill and Decode, PD)** 架构，该架构将推理工作负载拆分为不同的阶段，以便独立调优。接下来，我们将探索核心的推理并行策略，如数据并行、张量并行、流水线并行、专家并行和上下文并行——以及如何组合使用它们在许多 GPU 上服务大模型。

然后，我们涵盖**投机解码 (Speculative Decoding)** 方法，包括像 **Medusa**、**EAGLE** 和草稿-验证 (Draft-and-Verify) 方案等技术。这些技术允许在推理期间生成和评估多个 Token，而不是传统自回归 LLM 的标准单 Token 生成。这有助于克服顺序解码瓶颈。我们还将讨论用于强制输出格式（例如自定义 JSON 模式）的**受限解码 (Constrained Decoding)**，以及用于 MoE 模型的动态路由策略，以提高系统的专家门控和负载均衡效率。

---

## 分离式预填充和解码架构 (Disaggregated Prefill and Decode Architecture)

如前所述，现代 LLM 的推理工作流由两个不同的阶段组成：**预填充 (Prefill)** 和 **解码 (Decode)**。我们可以实施**分离式预填充和解码**来分离这些阶段。这让我们能够独立扩展预填充和解码集群——甚至在不同的硬件平台上——并显著提高大规模 LLM 服务的性能，这将在本章后面详细介绍。

> 跨供应商或跨架构部署要求两侧的 KV 缓存布局和 dtype 匹配。在实践中，生产系统应将预填充和解码保持在兼容的 GPU 家族上。这样，它们使用相同的数值格式，便于启用 KV 缓存传输和数据重用。

在预填充阶段，模型处理整个输入 Prompt——通常是数千、数万甚至数百万个 Token——在单次前向传播中产生由 LLM 计算的初始隐藏状态。然后它为输入 Prompt 中的所有 Token 填充注意力键值 (KV) 缓存。图 15-1 展示了分离式预填充和解码如何共享 KV 缓存并将 KV 传输与计算重叠。

*(图 15-1: 分离式预填充和解码共享 KV 缓存并将 KV 传输与计算重叠)*

在解码阶段，模型执行自回归生成以预测序列中的每个新 Token。它通过消耗所有先前生成 Token 的缓存注意力 KV 表示来做到这一点。

> 投机解码通过在单批次中预生成多个 Token 来加速解码过程。并行地，它验证这些 Token 是否正确。这减少了标准逐 Token 自回归解码的顺序性质。我们稍后会介绍投机解码。

### 预填充-解码干扰 (Prefill-Decode Interference)

传统上，LLM 推理系统将这两个阶段共置在相同的节点上，并简单地将所有计算批处理在一起。然而，这种朴素的方法导致了通常所说的**预填充-解码干扰**。例如，一个长 Prompt 的预填充可能会占用 GPU 并延迟其他请求的时间敏感解码工作——反之亦然。

在同一节点上共置预填充和解码会强制对这就两个具有非常不同特征的阶段使用单一的调度和资源分配策略。预填充由大型并行计算组成。相比之下，解码需要许多小的顺序计算。结果是，系统要么不得不优先考虑一个阶段的性能而牺牲另一个，要么过度配置硬件以满足两者的需求。

使用分离式预填充和解码架构，预填充和解码阶段被分配给不同的 GPU 池。这消除了两个工作负载之间的直接干扰。**DistServe 系统**通过分离预填充和解码，据报道在 TTFT (首字延迟) 和 TPOT (每输出 Token 时间) 约束下服务的 Goodput 请求增加了高达 **7.4 倍**（延迟 SLO 收紧高达 12.6 倍）。

### 独立扩展预填充和解码节点

如果我们能消除跨阶段干扰，就可以减少资源“死区时间”，即解码任务因长预填充计算而停顿——反之亦然。这样，GPU 花更多时间做更有用的工作，减少空闲时间。这提高了在给定延迟目标下的利用率和有用吞吐量（即 **Goodput**）。

我们可以通过专用一组节点处理预填充，另一组节点处理解码来分别扩展预填充和解码。这两个集群仅在将编码后的 Prompt 状态或注意力 KV 缓存从预填充 worker 传输到解码 worker 时进行通信，如图 15-2 所示。

这里，你看到分离的 GPU worker 处理预填充阶段以处理输入 Prompt——以及解码阶段以迭代生成输出 Token。预填充阶段的输出包括 Prompt 的 KV 缓存。它被传输到解码 worker 以生成后续 Token。

*(图 15-2: 分离式推理: 预填充池 [隐藏状态 + KV] → 使用 NVLink/NVSwitch [节点内] 或 GPUDirect RDMA [节点间] 进行 KV 移交 → 解码池)*

通过专用分离的 GPU 池，系统保持预填充和解码流水线并行忙碌。在实践中，分离已被证明可以在严格的延迟约束下显著提高吞吐量。一些研究表明可能获得巨大的收益，但一旦预填充和解码阶段分离，结果从适度改进到数倍更高的 Goodput 不等。结果很大程度上取决于工作负载和网络结构。

这种分离允许每个阶段针对吞吐量或延迟进行独立优化和扩展。可以在预填充 GPU 上积极地批处理预填充以最大化吞吐量，而不会通过增加解码延迟来拖累解码性能。此外，有了分离的集群，我们可以针对每个阶段调整并行设置、实例数量和调度策略。

### 对延迟 (TTFT) 和吞吐量 (TPOT) 的影响

解码 GPU 可以在较低的批次大小下运行——或使用专门的调度——以最小化流式生成的**每输出 Token 时间 (TPOT)**。例如，你可以使用优先考虑紧急解码任务的调度器以避免排队延迟。

这种分离之所以有效，是因为每个阶段有不同的性能预期。预填充阶段的**首字时间 (TTFT)** 针对低延迟进行了优化，而解码阶段优先考虑低 TPOT 和稳定的流式延迟。端到端吞吐量很大程度上由并发性和调度决定。在传统设置中，人们必须在 TTFT 和 TPOT 每 Token 延迟之间妥协。分离允许同时满足两个 SLO 目标。

> 在 KV 传输和 All-to-All 阶段监控 NVLink 和 NIC 利用率。目标是使用单独的流和事件将通信与计算重叠。

KV 移交产生的额外延迟极小，因为通信使用了用于多 GPU 和多节点传输的高带宽互连。这些互连包括 NVLink, NVSwitch, InfiniBand, 和以太网 (RoCE) 使用 GPUDirect RDMA。例如，配备 **ConnectX-8 SuperNICs** (800 GbE 级) 的多节点集群每端口提供高达 800 Gb/s 的 GPUDirect RDMA 带宽。这与主机中介的通信路径相比大大减少了 KV 传输时间。此外，建议**每 GPU 部署 1 个 NIC** 以优化预填充-解码分离并提高 MoE All-to-All 性能。

### KV 缓存数据传输与 NIXL

分离式系统使用连接器或调度器，一旦 Prompt 处理完成，就将 Prompt 的中间结果（最终隐藏状态和 KV 缓存）从预填充 worker 传输到解码 worker。这种移交会产生一些通信开销，但如果集群的互连是高带宽的（例如，NVLink 或 InfiniBand），这种开销相比于消除资源争用带来的收益是很小的。

在实践中，NVIDIA 的 **NIXL** 库通过基于拓扑和策略自动选择 NVLink/NVSwitch, RDMA 或主机暂存路径来最小化传输开销。例如，第 4 章介绍的 NVIDIA NIXL 库会自动选择最快的可用路径传输 KV 缓存。NIXL 与框架和推理引擎集成，包括 Dynamo 和 vLLM。NIXL-vLLM 集成如图 15-3 所示。

具体来说，**LMCache** 和 NIXL 集成在 vLLM 的分离式预填充中作为支持路径。NIXL 也被 NVIDIA Dynamo 和 TensorRT-LLM 用于使用点对点 GPU 互连和 RDMA 传输 KV 缓存数据。

在节点内，NIXL 通过 NVLink 和 NVSwitch 执行设备到设备传输，无需主机暂存。跨节点，NIXL 使用 InfiniBand 或 RoCEv2 上的 GPUDirect RDMA 来避免主机拷贝。这些路径即使对于多 GB 的负载也能保持极低的 KV 缓存移交延迟。

*(图 15-3: vLLM 推理引擎中使用 NIXL 的 KV 缓存数据传输)*

预填充和解码 worker 的放置应遵循网络结构。同节点放置通过 CUDA 对等访问将 KV 传输保持在 NVLink 和 NVSwitch 上，而跨节点放置应使用 InfiniBand 或 RoCEv2 上的 GPUDirect RDMA。NVIDIA Dynamo 与 NIXL 集成以跨节点在 GPU、CPU 内存和存储之间移动 KV 缓存，vLLM 通过 LMCache 和 NIXL 集成以实现分离式预填充。

> 当结构或虚拟化层阻止直接对等访问时，NIXL 可能会回退到主机暂存路径，这是非最优的。始终验证部署上的端到端 KV 传输时间。

### 使用 Kubernetes 部署分离式预填充和解码

在高级部署中，像 Kubernetes 这样的集群编排系统可以基于负载和输入特征动态调整 GPU 池分配——或分别扩展池。例如，如果许多用户带着超长 Prompt 和相对较小的输出（例如，大文档摘要用例）到达，Kubernetes 可以临时将分配转移到预填充池中使用更多 GPU。这将减少分配给解码阶段的 GPU 数量。

相反，如果许多用户请求超长输出（例如，长推理链，“think step-by-step”等），可以将更多 GPU 转移到解码池。在这两种情况下，可以为每种 worker 类型扩展新实例。

图 15-4 展示了一个基于 Kubernetes 的分布式 vLLM 集群，使用开源的 **llm-d** 项目分离预填充和解码 worker。vLLM 通过运行两个实例并使用 LMCache 和 NIXL 移交 KV 来实现分离式预填充，但 llm-d 将其扩展为用于分离式服务和 KV 感知路由的 Kubernetes 原生编排。此图展示了一个名为**变体自动缩放器 (Variant Autoscaler)** 的组件，它负责更新池中预填充和解码 worker 的副本数量。

*(图 15-4: 基于 Kubernetes 的 vLLM 集群，使用开源 llm-d 项目分离预填充和解码 worker)*

在现代推理部署中，所有节点都可以执行预填充和解码功能，因为它们共享相同的运行时和代码库（例如，vLLM, SGLang, NVIDIA Dynamo 等）。这取决于集群编排器在启动时静态地——以及在 worker 生命周期内动态地——为其分配特定角色（预填充或解码）。

总的来说，分离式预填充/解码架构为高吞吐量、低延迟的 LLM 服务奠定了基础。它确实引入了复杂性，因为必须传输和管理中间数据——对于长 Prompt 的 KV 缓存来说是数 GB 量级。调度也更复杂，但在超大规模下高效利用硬件的好处是显著的。

> 第 17 和 18 章将更深入地探讨分离式 PD 的其他技术，包括高级调度、路由和部署优化。

---

## 服务海量 MoE 模型的并行策略 (Parallelism Strategies for Serving Massive MoE Models)

高效服务海量 MoE 模型需要多种形式的并行，因为 GPU 内存有限。我们分解了关键的并行策略，包括张量、流水线、专家、数据和上下文并行——并讨论如何组合使用它们在许多 GPU 上分布 LLM。表 15-1 提供了这些策略的高级摘要、典型用途及详细描述。

*(表 15-1: LLM 推理的并行策略)*

| 并行策略            | 分区依据                            | 用例                                      | 优点                                                       | 缺点                                                         |
| :------------------ | :---------------------------------- | :---------------------------------------- | :--------------------------------------------------------- | :----------------------------------------------------------- |
| **张量并行 (TP)**   | 层内（跨 GPU 拆分神经网络权重矩阵） | 单个模型太大或需要加速层内繁重计算        | 计算受限层的近线性加速；重叠通信与计算减少开销             | 每层频繁通信 (All-Reduce)；需要高带宽互连 (NVLink)；跨节点效率低 |
| **流水线并行 (PP)** | 不同层在不同 GPU 上                 | 极深模型不适合单个 GPU；跨层内存扩展      | 允许分布模型状态；利用微批处理并发处理多 Token             | 增加流水线填充/排空延迟（气泡）；实现复杂；激活内存占用更高  |
| **专家并行 (EP)**   | 不同专家在不同 GPU 上               | 具有许多专家的大型 MoE 模型               | 几乎无限的模型规模；每个 GPU 仅计算一小部分 Token          | 运行时通信开销高 (All-to-All)；潜在负载不平衡                |
| **数据并行 (DP)**   | 在多 GPU 上复制整个模型             | 扩展吞吐量；多实例服务                    | 近线性吞吐量扩展；实现简单                                 | 无单查询延迟改进；成倍增加内存使用                           |
| **上下文并行 (CP)** | 跨 GPU 分区输入序列 Token           | 超长序列 (100k+) 以减少 Prompt 延迟和内存 | 长上下文预填充的近线性加速；通过拆分 KV 缓存启用超大上下文 | 需要自定义注意力算法；增加边界 Token 的通信                  |

*(图 15-5: 模型权重和输入数据在 GPU 上的拆分方式)*

### 张量并行 (Tensor Parallelism, TP)

**张量并行 (TP)** 将神经网络每一层内的计算拆分到多个 GPU 上。例如，Transformer 层中的大矩阵乘法可以按列或行分区——并在两个或更多 GPU 上并行计算。这些 GPU 随后通过执行 All-Reduce 来交换结果以聚合部分输出。

TP 通常用于模型的层（称为隐藏层）太大而无法放入单个 GPU 内存的情况——或者当我们想通过并行利用多个 GPU 来加速单个模型实例时。它使所有 GPU 在每层计算中保持步调一致，这需要极高的 GPU 间带宽才能高效。

理想情况下，TP 运行在通过高带宽 NVLink 和 NVSwitch 互连的 GPU 上。在现代多 GPU 机架（如 NVIDIA 的 GB200/GB300 NVL72）上，TP 可以在不使互连带宽饱和的情况下更大规模地使用。NVLink Switch 将 NVLink 域扩展到整个机架的 72 个 GPU。NVLink Switch Systems 可以在多达 8 个机架（576 个 GPU）之间扩展全对全连接的 NVLink 结构。这实现了大规模并行策略，不仅在单个 NVL72 内部，而且跨越多个机架。

> 虽然可以将 TP 扩展到跨机架，但建议在选择 TP 组大小时考虑拓扑感知，并尽可能在一个 NVLink/NVSwitch 岛内，以避免不必要的机架间交换机延迟并提高整体系统效率。

### 流水线并行 (Pipeline Parallelism, PP)

**流水线并行 (PP)** 将模型**按层**分区到不同的 GPU 上。例如，在一个 60 层的 Transformer 模型中，GPU 0 可能持有层 1–20，GPU 1 持有层 21–40，GPU 2 持有层 41–60。

在推理中，PP 可以通过跨多个批次分区模型来提高吞吐量——类似于装配线。在**预填充**阶段，PP 通过以交错方式将序列的不同部分流式传输到各层来实现高 GPU 利用率。

相比之下，在**解码**阶段，纯 PP 提供的益处较少，因为每个新 Token 仍必须按顺序通过流水线阶段。这产生了**流水线气泡**，即空闲期。为了减少气泡，实现使用微批处理 (Microbatching)。PP 主要通过启用非常大的模型在 GPU 间拆分层来帮助内存扩展。

PP 往往会因流水线阶段之间的传输开销而增加单个项目的端到端延迟。因此，PP 通常出于模型容量原因——而非延迟原因——被选择。

### 专家并行 (Expert Parallelism, EP)

**专家并行 (EP)** 专用于 MoE 架构。在 MoE 层中，有许多专家网络。对于每个输入 Token，只有少数专家被激活。这自然适合将不同的专家分布在不同的 GPU 上。

例如，如果一个 MoE 层有 16 个专家，我们有 4 个 GPU，每个 GPU 可以托管 4 个专家。在推理期间，当 Token 到达该层时，门控网络选择前两个专家。Token 数据随后被发送到拥有这两个专家的 GPU 进行处理。结果再合并回来生成下一个 Token，如图 15-6 所示。

*(图 15-6: 混合专家 (MoE) 通信)*

实现这一点需要 **All-to-All** 通信模式，以便 Token 动态地在 GPU 之间洗牌。这种动态路由在每个 MoE 层引入了通信密集的步骤。如果优化不当，这可能成为性能瓶颈。好处是 EP 允许总模型容量随 GPU 数量几乎线性扩展。

### 混合并行 (Hybrid Parallelism)

在实践中，服务海量 MoE LLM 使用上述并行策略的组合。图 15-7 展示了一个使用四个 GPU 的混合并行配置。

*(图 15-7: 4 × 2 × EP 混合并行组合的高级图示)*

这里，我们使用四个流水线阶段（每 GPU 一个）和两路张量并行。Token 通过专家并行在两个专家之间路由。这被称为 **4 × 2 EP 混合并行策略**。

指导原则是使用 TP 直到收益递减点——通常在一个节点或 NVL72 机箱内。然后最小限度地使用 PP——刚好够将模型放入内存。接着，最大化 EP 以跨 GPU 分布 MoE 参数。最后，添加数据并行副本以随着用户和请求的增加而提高吞吐量（如果预期有极长输入，可以叠加 CP）。

---

## 投机解码与并行 Token 生成技术 (Speculative Decoding and Parallel Token Generation Techniques)

LLM 推理的一个基本性能挑战是解码的顺序性。即使有最快的 GPU，生成数百个 Token 也可能需要数秒。本节讨论几种加速解码阶段的技术。

### 双模型、基于草稿的投机解码与 EAGLE

**投机解码 (Speculative Decoding)** 是一种以小模型上的额外工作换取大模型时间的技术。思路是运行一个轻量级的“草稿” LLM 与主 LLM 并行。草稿模型更快，并推测性地生成一批 $k$ 个 Token。

大模型（称为**目标模型**）随后通过在发送到 GPU 的单个批次中预测整个 $k$ Token 序列的下一个 Token 概率来验证草稿 Token。通过一次处理所有 $k$ 个 Token，目标模型增加了算术强度。如果目标模型的输出与草稿模型一致，我们就有效地在生成一个 Token 的时间内生成了 $k$ 个 Token。

图 15-8 展示了一个小草稿模型预测多个 Token，然后目标（大）模型并行验证这些 Token。

*(图 15-8: 带有草稿 [小] 和目标 [大] 模型的投机解码)*

**EAGLE** 算法通过在特征级别而非 Token 级别操作来重新思考投机解码。EAGLE 使用大模型自身中间表示的一步外推来预测下一个 Token 的特征。EAGLE 报告称对于 4 Token 草稿实现了高达 **3.5 倍**的加速，同时保留了分布。

**EAGLE-2** 通过引入上下文感知的动态**草稿树 (Draft Tree)** 扩展了 EAGLE，进一步提高了并行性，如图 15-9 所示。

*(图 15-9: 使用 EAGLE-2 的投机解码)*

### Medusa 的多头多 Token 解码

**Medusa** 框架采用更激进的方法。它修改模型架构本身，为每个解码步骤并行预测多个新 Token。

与双模型投机解码不同，Medusa 的架构真正地从单个模型每次迭代生成多个 Token。Medusa 通过向 Transformer LLM 添加额外的解码头（Medusa 头）来实现这一点。这让模型同时提出几个后续 Token。

Medusa 内部使用专门的基于树的注意力模式。在推理期间，Medusa 可以将顺序解码迭代次数减少并行预测的倍数。在实践中，Medusa 通常实现 **2–3 倍**的加速。

*(图 15-10: 使用 Medusa 的多 Token 解码)*

### 交叉多请求的解码步骤

另一种并行解码技术是交叉来自多个并发请求的解码步骤。这更多是一种推理引擎能力。框架如 vLLM 通过请求路由、连续批处理 (Continuous Batching) 和 Token 调度来实现这一点。想法是通过填充顺序步骤之间的间隙来保持 GPU 忙碌。

---

## 受限解码的性能影响 (Constrained Decoding Performance Implications)

LLM 解码的一个重要方面是在特定约束下生成文本，例如强制输出匹配 JSON 格式。**受限解码**通常会增加额外延迟，因为模型可能需要反复回溯。

为了减轻这些成本，许多框架编译 JSON 语法并预计算每个状态的有效 Token。这允许推理引擎在运行时掩盖无效的 Softmax 输出。这种方法减少了回溯，提高了缓存和接受率。

对于像简单 JSON 模式这样的适度约束，使用编译语法和重叠掩码计算的现代引擎可以将开销推低到个位数百分比。

---

## MoE 推理的动态路由策略

服务 MoE 模型需要仔细的专家分区和动态路由。

### 专家通信优化

在专家间 Token 激活的 **All-to-All** 交换期间，MoE 在 GPU 之间洗牌一批 Token。这发生在 MoE 的每一层，如果处理不当，可能会主导推理时间。

图 15-11 展示了每个 GPU 如何只接收它托管的专家所需的 Token。

*(图 15-11: 每个 GPU 仅接收它托管的专家所需的 Token)*

减少通信开销的一种策略是使用**分层路由**：首先使用 NVSwitch/NVLink 在同节点 GPU 间路由，仅对需要非本地专家的 Token 进行跨节点路由。此外，使用双缓冲通信以重叠传输与计算。

### 负载均衡、容量因子与专家复制

建议每个专家和 GPU 获得相等份额的工作以避免“热点”。否则，过载的 GPU 将成为瓶颈。

为了防止这种情况，高性能 MoE 服务系统暴露一个**容量因子 (Capacity Factor)** 参数（通常设置为 1.2–1.5 倍平均负载），限制每个专家每批次处理的 Token 数。溢出的 Token 被路由到次优专家或排队。

另一种策略是**专家复制 (Expert Replication)**。如果某个专家持续成为热点，系统可以将该专家克隆到另一个 GPU 上。

### 自适应专家路由与实时监控

与训练后固定的传统 MoE 专家门控不同，**自适应路由**可以在推理期间根据当前条件和专家负载实时调整门控决策。

图 15-13 展示了一种使用偏置门控分数方法的自适应 MoE 路由策略。

*(图 15-13: 自适应 MoE 路由实战)*

---

## 关键要点 (Key Takeaways)

**分离以同时优化延迟和吞吐量**
> 将 Prompt 预填充和解码阶段拆分到不同的 GPU 池中消除了干扰。这让你能同时实现低 TTFT 和高 TPOT。

**对海量模型使用混合并行**
> 没有单一的并行策略足以应对多万亿参数模型。按需结合张量、流水线、专家和数据并行。

**缓解顺序解码瓶颈**
> 高级解码方法如投机解码和 Medusa 可以通过增加 Token 级并行性和算术强度来大幅加速生成。

**通过约束保持输出质量和格式**
> 受限解码技术在生成期间强制执行规则。使用编译语法和优化掩码路径，开销可以很低。

**有效平衡 MoE 工作负载**
> 使用高带宽互连和分层 All-to-All 通信。应用容量限制和 Top-2 门控以避免掉队专家。复制热点专家。

---

## 结论 (Conclusion)

通过结合分离式预填充/解码流水线、多 Token 投机解码、动态专家路由和自适应编排，可以在最小资源争用和超低延迟下实时服务 LLM。

现代推理服务平台如 vLLM, SGLang, 和 NVIDIA Dynamo 拥抱了许多这些优化。关键是一个端到端的、适应性强的架构，将算法创新与高性能硬件能力相匹配。

在接下来的章节中，我们将更深入地探讨模型推理性能优化，包括动态、自适应和多节点服务策略。我们将涵盖从应用级前缀缓存、延迟感知请求路由到系统级自适应内存分配、精度切换和拥塞感知资源调度的所有内容。







# 第16章 大规模推理的性能分析、调试与调优

运维一个大型LLM推理集群，离不开能够确保系统按预期运行的监控与调试工具。当性能偏离既定目标时，这些工具也能助你迅速定位瓶颈所在。

在本章中，我们将演示如何利用 NVIDIA Nsight Systems 等工具进行性能分析（Profiling），以及利用 Prometheus/Grafana 进行集群范围的遥测，从而对这些复杂的系统进行监控和调试。我们还将展示如何收集和解读关键指标，如 GPU 利用率、内存压力、尾部延迟百分位、缓存命中率、每词元（per-token）计时等。这些指标将为我们的推理引擎性能优化提供指引。

接下来，我们将讨论运维层面的性能调优，包括那些在生产环境中久经考验的方法，用于优化 GPU 利用率、降低推理延迟并提高大型集群的吞吐量。这涵盖了计算与通信重叠（Overlap）、请求调度与批处理（Batching），以及高效利用 NVLink、NVSwitch 和 InfiniBand 等高速互连技术的技巧。

我们还将比较用于推理的实时量化技术，包括使用广义训练后量化（GPTQ）和激活感知权重量化（AWQ）等实现方法将模型压缩至 8-bit 和 4-bit 精度。在此过程中，我们将探讨仅权重量化与权重激活同时量化之间的权衡。我们就如何在服务流水线中应用量化以减少内存使用并提高吞吐量——同时保持模型精度——提供了实用的指导。

最后，我们会考量应用层面的优化，这些优化是对底层性能调优的补充。这些策略包括提示词压缩（Prompt compression）、前缀缓存（Prefix caching）、去重、查询路由（例如回退模型）以及部分输出流式传输。

## 推理性能的分析、调试与调优

现代 LLM 推理引擎包含大量动态组件——尤其是在预填充（Prefill）和解码（Decode）分离的架构下。正如**图 16-1** 所示，一个典型请求的生命周期涉及众多组件。

*(图 16-1：预填充与解码分离的 LLM 推理系统中典型请求的生命周期)*

鉴于这种复杂性，调整推理性能的工作流是高度迭代的。它需要细致的调优和持续的验证。

**首先**，你需要观察指标并识别当前的瓶颈，比如 GPU 未被充分利用或延迟高于预期。**接着**，形成一个改进假设，例如“增加批处理大小（Batch size）”或“增加操作 X 的通信与计算重叠”。**然后**，实施修复并验证该假设。

**随后**，理想情况下，你应该在预发环境（Staging environment）中使用具有代表性的工作负载和性能分析工具来测试该修复，以验证变更是否如预期般运作。例如，你可以验证某个操作是否展现出了适当的内存与计算重叠。

**最后**，将修复部署到生产环境，并监控 Grafana 和日志，以验证该修复在真实工作负载下是否提高了吞吐量和延迟。随着新瓶颈的出现，重复此工作流。

这种“观察-假设-调优”的循环应当是持续不断的。现代部署通常会将这些步骤自动化。例如，你可以利用定期的负载测试——以及随后的关键指标异常检测——来触发调优工作流。

> **提示**：建议在将优化（包括更新推理运行时和模型变体）部署到生产环境时执行金丝雀发布（Canary rollouts）。通过将优化部署到在一小部分服务器上运行的一小部分流量中，你可以在向所有最终用户全面部署生产之前验证优化效果。这种增量方法有助于尽早捕捉意外的副作用，从而减小其“爆炸半径”，避免影响所有用户。

考虑这样一个场景：主机侧 CPU 利用率飙升至 100%，原因是过度的分词（Tokenization）或推理数据预处理。这将限制推理引擎能够处理的并发流数量。一种修复方法可能是利用 GPU 加速的分词器库，或者使用 CUDA 或 OpenAI 的 Triton 语言编写的自定义 GPU 内核，将预处理工作移至 GPU。

部署新库或内核后，应监控变更前后的 CPU 利用率。如果你看到 CPU 利用率下降且整体吞吐量增加，那么系统就不再受限于基于 CPU 的输入预处理瓶颈了。

你还应关注任何类型缓存的命中率，包括前缀缓存、提示词嵌入缓存和 KV 缓存。你应该拥有“缓存命中”与“缓存未命中”的指标。高缓存命中率意味着系统正在有效地重用数据。相反，如果看到高缓存未命中率，你可能需要调整缓存大小、驱逐策略或缓存策略以最大化缓存命中。

vLLM 的 LMCache 组件允许调整 GPU 与 CPU 缓存的比例。如果因为 GPU 显存限制导致未命中率过高，你可以启用其分页缓存卸载（Offload）功能，让 CPU 协助处理。务必确保你的缓存驱逐策略（最近最少使用 [LRU]、最不常用 [LFU] 等）与访问模式相匹配。

另一个场景是使用 KV 缓存来重用批处理请求中相同输入序列前缀的数据，以避免重新计算前缀的 KV。在这种情况下，你需要测量请求共享前缀的频率。这会产生一个*前缀合并（Prefix merge）*事件，并增加 vLLM 中的前缀缓存命中指标，包括 `vllm:gpu_prefix_cache_queries` 和 `vllm:gpu_prefix_cache_hits`。这些指标让你能够计算命中率，例如“每次查询的命中数”。

测量前缀合并率有助于你将其与实际缓存命中率相关联，以此衡量缓存层的真实收益。通过这种方式，你可以调整批处理和调度策略以最大化共享前缀——并预测不同工作负载下的端到端吞吐量和延迟改进。

你可以在推理引擎上运行合成生成的数据，测试包含许多重复前缀的提示词。理想情况下，你会看到由于前缀合并而导致的预填充计算量的减少。

像 vLLM 和 SGLang 这样的现代 LLM 推理引擎原生暴露前缀合并指标。但如果前缀合并不是你的推理引擎导出的一等指标，你应该植入一个自定义计数器“前缀去重 Token 数”来监控其有效性。

> 如果你发现前缀合并的表现未达预期，应检查前缀匹配逻辑是否失败。从检查是否存在分词器差异开始调试过程。这是大多数前缀匹配问题的可能原因。

除了性能之外，监控还有助于容量规划。通过跟踪利用率和延迟随负载增加的变化情况，你可以预测系统将在何处触及特定限制，例如 p95（第 95 百分位）延迟开始呈指数级上升。在这种情况下，动态批处理大小可能增加到了边际收益递减点。

> 如果你使用分层缓存策略，包括基于 NVMe 的 KV 缓存扩展，请务必监控设备的 I/O 延迟。高 I/O 延迟将显著降低缓存性能。

当单卡并发达到极限，且进一步增加批处理大小不再提高吞吐量时，你可能需要通过添加更多 GPU、部署额外的模型副本或增加专家数量（Expert count）来横向扩展，将工作分配给更多的计算单元。

你还应考虑模型压缩——或切换到更低精度（FP8/FP4）——以便在横向扩展之前获得更高的单卡有效吞吐量。然而，一旦硬件饱和（例如 SM 达到 100% 且显存带宽接近峰值利用率），添加更多 GPU 或使用张量/流水线并行可能是提高吞吐量的唯一途径。

记住，始终要权衡新硬件的成本与效率增益。有时，升级到拥有更大显存和更高 FLOPS 的新型 GPU 比横向扩展旧 GPU 机群更具成本效益。

增加专家数量可以提高吞吐量上限——但前提是你也改进了专家路由和调度，以管理额外的全对全（All-to-all）通信。否则，天真的扩展可能只是将瓶颈转移到了网络上。接下来，我们讨论监控以及如何验证我们的优化工作是否真正得到了回报。

## 监控系统指标与计数器

与传统的微服务调用不同（后者执行时间相对统一且可预测），LLM 请求是非均匀的，其延迟可能差异巨大。这种差异如**图 16-2** 所示。

*(图 16-2：传统微服务调用与 LLM 调用的差异)*

对于生产环境中的持续监控，通常使用 Prometheus 从每个 GPU 计算节点收集指标，并使用 Grafana 仪表板进行可视化。需要跟踪的关键 GPU 指标包括 GPU 利用率（SM 忙碌的时间百分比）、显存使用量、复制引擎利用率、PCIe 和 NVLink 吞吐量，以及 GPU 温度和功率（例如，是否发生降频）。注意：诸如 L1 和 L2 活动、占用率（Occupancy）和指令吞吐量等底层计数器，最好使用 Nsight Compute 或 CUPTI 收集，而不是 DCGM 和 Prometheus。

> `cudaMemPool` 指标和异步分配器统计数据在监控内存碎片时非常有帮助。这些应集成到你的监控中，因为这将极大地促进在生产环境中调试系统性能问题。

监控互连利用率（包括 NVLink、NVSwitch 带宽和 NIC 吞吐量）同样重要。这样，你就能捕捉到多 GPU 和多节点集群配置中的通信瓶颈。

NVIDIA 的数据中心 GPU 管理器（DCGM）暴露了许多 GPU 指标，Prometheus 可以抓取并收集这些指标。例如，DCGM 提供用于 SM 利用率百分比的 `DCGM_FI_DEV_GPU_UTIL`，用于内存复制引擎利用率的 `DCGM_FI_DEV_MEM_COPY_UTIL`，以及用于帧缓冲区内存使用量的 `DCGM_FI_DEV_FB_USED` 等。

DCGM 暴露 NVLink 错误计数器，并且在某些平台和驱动程序版本上可以暴露吞吐量计数器。对于持续的链路利用率，也可以使用 `nvidia-smi nvlink` 和 Nsight 工具。你应该将这些指标集成到仪表板中，并设置警报以帮助识别网络何时因跨 GPU 和跨节点通信流量而饱和。DCGM 还会跟踪 Xid 计数器以及关键 GPU 错误。

> 虽然 DCGM 暴露了 NVLink 计数器，但在撰写本文时，`dcgm-exporter` 默认并未在所有平台上暴露每条链路的带宽。因此，如果你需要链路级吞吐量，可能需要直接查询 DCGM 或扩展 exporter。

建议同时收集高层应用指标，如每秒查询/请求数（QPS/RPS）、平均延迟和 p95/p99 延迟、活跃上下文数量以及吞吐量（Tokens/sec）。关于 KV 缓存利用率和大小（整体及每节点）的指标同样至关重要，需要进行监控。

你可以设置 Prometheus 节点导出器（Node exporters）从每个节点收集所有这些指标，将数据汇聚一处，甚至为关键阈值设置警报。Grafana 随后可以将这些指标绘制成实时仪表板，与团队共享。**图 16-3** 展示了如何从 Kubernetes 集群中的每个 GPU 收集指标并导出到 Prometheus 以便用 Grafana 可视化。

*(图 16-3：DCGM 从 Kubernetes GPU 节点收集指标并发送至 Prometheus)*

这样，当你部署一个新的优化（例如增加批处理）时，Grafana 会立即显示每个 GPU 的利用率是否增加。你还可以监控以确保 p95/p99 延迟保持在目标范围内。

计数器（Counters）在测量中也极其实用——尤其是对于动态和自适应系统。例如，如果你的推理引擎根据当前条件动态调整批处理大小，你可能希望增加一个“批处理大小变更”计数器。

另一种选择是将变更记录在日志文件中，但这需要使用 Apache Spark 等工具进行离线分析（缓慢、基于文本的搜索/聚合）。之后还需要手动将日志文件分析结果与 Prometheus 指标相关联。

通过为有趣的应用级事件（包括错误）增加一个简单的计数器，数据被推送到 Prometheus 并在 Grafana 仪表板中与其他所有指标一起实时可见。此外，对于关键事件，考虑使用结构化日志和分布式追踪。

现代应用性能管理（APM）工具——以及 OpenTelemetry——可以摄取这些日志/追踪并将它们与指标关联。这提供了整个系统中事件的一致时间线视图。洞察这一时间线将有助于加快调试性能问题的速度。

如果持续监控这些指标，你就能深入了解下一步该在何处进行调优。例如，如果 GPU 利用率低于预期，你可以检查显存是否已满。如果没有用满，可以尝试增加批处理大小或最大并发请求数。但也务必关注延迟服务等级目标（SLOs），你不希望超出这些目标。

> 现代推理服务器为动态批处理暴露了一个“最大延迟”设置。调整此设置以满足你的 SLOs。增加它会提高批处理大小（吞吐量）。增加太多则会损害 p99 延迟。请根据你的延迟目标持续调整此设置。

相反，如果 GPU 显存接近上限，推理引擎可能会开始将不活跃的 KV 缓存数据交换（Swap）到主机 CPU 内存或 NVMe 存储。这将降低 GPU 利用率，因为 GPU 需要等待来自较慢的 CPU 内存或磁盘的额外数据传输。

> 如果你看到 GPU 内存复制引擎利用率激增——或异常的 NVLink 利用率——同时伴随着较低的 SM 利用率，你的推理引擎很可能正在将 KV 缓存数据换入换出 GPU 显存。由于过高的数据传输延迟，这将成为系统的瓶颈。

如果发生交换（Swapping），你可以调整推理引擎的分页参数以减少抖动（Thrashing），应用 FP8 或 FP4 量化，增加用于缓存的 GPU 显存分配，并可能更改交换策略。这应该会降低复制利用率并提高计算利用率——这正是你想要看到的。

Grafana 也用于延迟跟踪。你可以绘制端到端请求延迟的分布图——通常同时测量预填充延迟和每 Token 延迟。如果 p99 延迟在特定时间激增，你应该将其与 GPU 指标和其他日志相关联。

例如，p99 延迟激增可能与 GPU 利用率下降的时期相关。也许延迟激增与触发更大动态批处理大小的流量激增有关。这可能会导致该时段的延迟更高。为了验证这一点，你可以在 Grafana 仪表板的延迟图表上叠加 RPS（每秒请求数），查看两张图表是否相关。

如果是根据我们的假设，激增是由于动态增加的批处理大小引起的，请确保它没有超过服务等级协议（SLA）。如果超过了，你可以尝试减少最大请求批处理队列延迟或减小最大批处理大小，以此限制延迟。

日志在诊断问题时也是无价之宝。你应该在代码中植入监测点，记录关键事件，如批次形成的时间、通信开始/结束的时间等。最好使用 DEBUG 级别，以便按需启用/禁用它——且不影响请求响应延迟。

当启用调试日志时，你会看到文本格式的逐步时间线。在一次调试会话中，你很可能会结合使用日志时间线和 Prometheus/Grafana 指标。例如，你可以看到全对全通信耗时超过 5 毫秒的频率。

结合基于日志的时间线和指标，你可以看到异常值，例如可能导致全对全通信交换中某次迭代变慢的网络问题。如果这种情况持续发生，你可以提高专家容量因子（Capacity factor），以便任何多余的 Token 自动溢出到二级专家副本——理想情况下托管在具有更稳定网络路径的 GPU 上。这将平衡负载并最小化延迟。

> 在实践中，将容量因子设置为 1.2–1.5 是常见的，因为这允许当主专家过载时，20%–50% 的额外 Token 被重新分配。这可以显著平滑 MoE 推理中的尾部延迟。溢出到第二个专家比在互连降级的 GPU 上排队等待略微停滞的专家要好。如果你的网络因任何原因持续出现问题，这将降低对异常值的敏感度。

### 使用 Nsight Systems 和 Nsight Compute 进行性能分析

在开发和调优推理代码时，你可以使用 Nsight Systems 捕获 CPU 和 GPU 上的工作负载追踪（Trace）。Nsight Systems 提供了一个时间线视图，以微秒级分辨率显示 CPU 线程、GPU 内核、CUDA 事件、NCCL 通信等。

通过使用 NVTX 注解对代码进行插桩，为了清晰起见，我们可以在时间线上标记诸如“预填充阶段”、“解码步骤”或“全对全通信”等区域。以下代码展示了使用具有显式 push 和 pop 范围的 NVTX v3 C API 围绕示例预填充和解码步骤的 NVTX 范围标记：

```cpp
// 使用 C API 进行 NVTX 注解的 C++ 示例片段。
#include <nvtx3/nvToolsExt.h> // 或 <nvToolsExt.h>
#include "my_model.hpp" // 你的模型的 C++ 接口
#include <vector>

// 保持调用点整洁的小助手。
#define NVTX_PUSH(name, argb) \
do { \
    nvtxEventAttributes_t a{}; \
    a.version = NVTX_VERSION; \
    a.size = NVTX_EVENT_ATTRIB_STRUCT_SIZE; \
    a.colorType = NVTX_COLOR_ARGB; \
    a.color = (unsigned int)(argb); \
    a.messageType = NVTX_MESSAGE_TYPE_ASCII; \
    a.message.ascii = (name); \
    nvtxRangePushEx(&a); \
} while (0)

#define NVTX_POP() do { nvtxRangePop(); } while (0)

struct Token { int id; };

void run_inference(
    const std::vector<Token>& prompt_tokens,
    Model& model,
    int num_generate_steps) {
    
    // 预填充 (Prefill)
    NVTX_PUSH("Prefill", 0xFF4F86F7);
    model.encode(prompt_tokens);
    NVTX_POP();
    
    // 逐个解码 Token (Decode)
    for (int t = 0; t < num_generate_steps; ++t) {
        NVTX_PUSH("Decode", 0xFFFF8C00);
        Token next_token = model.decode_next();
        // ... (采样 / 流式传输给客户端)
        NVTX_POP();
    }
}
```

在这里，我们使用 `nvtxRangePushEx`/`nvtxRangePop` 显式标记区域。我们在 `model.encode(...)` 之前立即推入一个 "Prefill" 范围，并在之后立即弹出。在解码循环内部，我们在每次迭代的顶部推入 "Decode"，并在 `model.decode_next()` 之后弹出。小巧的 `NVTX_PUSH`/`NVTX_POP` 助手还附加了颜色（十六进制值）和文本。这有助于减少时间线可视化中的不匹配——同时保持调用点简洁。显式的 push/pop 配对在代码中清晰可见，便于审计。

彩色块状注解将出现在标记为 "Prefill" 和 "Decode" 的 Nsight Systems GPU 活动时间线上。这使得查看每个阶段耗时多久——以及这些阶段如何与通信操作重叠——变得容易。这有助于识别诸如 GPU 空闲间隙和意外同步等问题。

> 注意，我们直接使用 NVTX C API (`nvToolsExt`) 而不是 PyTorch 的 `record_function()`。这让我们能够在纯 C++ 运行时中注解热点路径，并在从 Python 或其他语言启动工作时保持标记一致。

通过将范围缩小到围绕 `model.encode(prompt_tokens)` 的最小必要区域，性能分析标记精确覆盖了预填充工作，而不包含其他代码。这提高了追踪的清晰度和性能诊断能力。

当在多个 CUDA 流上排队工作时（例如，用于 H2D/D2H 拷贝的专用“传输”流和用于内核的“计算”流），你应该使用每流（Per-stream）范围。为此，你可以用不同的 NVTX 范围包裹每个流的主机代码。

例如，你可以使用 `nvtxNameCudaStreamA(transfer_stream, "transfer_stream")` 和 `nvtxNameCudaStreamA(compute_stream, "compute_stream")` 来命名流。然后，你会在内存拷贝/传输周围使用 `nvtxRangePushA("transfer_stream")` 和 `nvtxRangePop()`，在内核启动周围使用 `nvtxRangePushA("compute_stream")` 和 `nvtxRangePop()`。

在 Nsight Systems 时间线中，使用 NVTX 命名的流使得重叠（或缺乏重叠）显而易见。以下代码演示了这些是如何组合在一起的：

```cpp
// 创建流后的一次性操作
nvtxNameCudaStreamA(transfer_stream, "transfer_stream");
nvtxNameCudaStreamA(compute_stream, "compute_stream");

// H2D/D2H 拷贝周围 (传输流)
nvtxRangePushA("transfer_stream");
cudaMemcpyAsync(h_logits, d_logits, bytes, cudaMemcpyDeviceToHost, transfer_stream);
nvtxRangePop();

// 内核入队周围 (计算流)
nvtxRangePushA("compute_stream");
my_kernel<<<grid, block, 0, compute_stream>>>(...);
nvtxRangePop();
```

在这里，我们要命名流并将入队站点包裹在每流范围中，以便 Nsight 时间线保持可读性。重要的是要注意 NVTX 范围注解的是主机线程时间线。GPU 泳道（Lanes）按流显示内核/内存拷贝。命名流有助于在分析期间将主机范围关联到正确的 GPU 泳道。

Nsight Compute 让我们能够分析单个内核以查明效率低下的原因。我们可以利用 Nsight Compute 的分段分析功能（Section-based profiling）来关注内核的特定部分，例如内存事务。

另一个鲜为人知但超级有用的工具是 Nsight Compute 的 **CUDA 程序计数器 (PC) 采样**功能。这可以采样程序计数器并识别热点，而无需完整的、重量级的插桩，如**图 16-4** 所示。

*(图 16-4：Nsight Compute 的 CUDA 程序计数器 (PC) 采样功能有助于以低开销方式识别热点)*

具体来说，我们可以利用它来分析实时推理服务器，并精确定位哪些内核指令耗时最多。而且我们可以以低开销的方式做到这一点。既然我们已经介绍了使用 Nsight Systems 和 Nsight Compute 进行性能分析，下面让我们讨论一些推理的常见故障排除秘籍。

> 在对实时服务进行生产调查时，首选程序计数器采样（PC Sampling），以便以最小的开销定位热点。只有当采样指向特定内核或阶段时，才切换到完整追踪。

### 推理故障排除秘籍

在生产环境中，持续运行重型性能分析器是不切实际的。因此，你需要依靠轻量级的、基于指标的监控，如 GPU SM 利用率、KV 缓存警告、尾部延迟百分位、缓存命中率和 OOM（内存溢出）警报来检测异常并指导针对性的修复。当某个指标越过特定阈值时，你可以对其根本原因形成假设，例如批次太小、KV 缓存不足、路由热点、分片不平衡、内存超售等。

然后你应用修复措施，如调整批处理大小、提高内存利用率上限（如果可能）、调整路由阈值或启用 CPU 卸载。一旦修复推送，你应该验证影响并确认指标已回落至阈值以下。**表 16-1** 展示了一些常见生产问题的关键指标、症状、可能原因和建议修复。

**表 16-1. 常见故障排除症状、原因及建议措施**

| 指标/症状                             | 可能原因                                       | 建议措施                                                     |
| :------------------------------------ | :--------------------------------------------- | :----------------------------------------------------------- |
| SM 利用率 < 50%                       | 批次太小或缺乏融合算子                         | 增加批处理大小；启用融合算子（FlashAttention 或 PyTorch 中 cuDNN 融合的 `scaled_dot_product_attention` (SDPA) 后端）；或添加自定义融合算子（例如使用 Triton）；然后使用 `nsys --trace=cuda` 进行分析。 |
| KV 缓存抢占警告 (Preemption warnings) | KV 缓存空间不足 (vLLM)                         | 提高 GPU 显存利用率阈值；减少最大批处理 Token 数；考虑使用 PagedAttention 进行动态 KV 分配。 |
| 高尾部延迟 (p95 > 200 ms)             | 解码节点热点或队头阻塞 (Head-of-line blocking) | 检查路由器日志中的路由模式。调整预取阈值。启用推测解码（Speculative decoding）路径。 |
| 负载下缓存命中率 < 60%                | 分片放置不平衡或缺失前缀缓存                   | 验证前缀缓存连接器配置（例如 vLLM 的 LMCache NIXL 和 NVIDIA Dynamo 的 NIXL 连接器），并在需要时增加前缀缓存 TTL 或副本数量。 |
| 多租户 GPU 上发生意外 OOM             | GPU 显存超售 (Overcommitted)                   | 降低每个实例的 GPU 显存利用率；启用 CPU/NVMe 卸载；将进程绑定到 CPU 插槽以减少跨插槽流量。 |
| 不规则的性能异常值                    | 时钟不匹配或热节流 (Thermal throttling)        | 确保所有时钟同步，并监控热节流和功率节流。                   |

> **注意**：所有指标表中的数值仅用于说明概念。有关不同 GPU 架构上的实际基准测试结果，请参阅 GitHub 仓库。

你也可能在日志文件中发现此类信息。像 AWS 这样的云提供商支持对日志文件进行正则表达式（RegEx）过滤，以从日志行中提取数值并直接将其导出为指标。例如，AWS CloudWatch 就支持这一实用功能。下一个代码块中有一些有用的示例日志行可供监控。

这里是一个 vLLM 日志片段，表明由于 KV 缓存空间不足导致了 KV 缓存抢占。因此，触发了 KV 重新计算，这使用了更多 GPU 计算资源并增加了延迟：

```
WARNING 2025-05-03 14:22:07 scheduler.py:1057 Sequence group 0 is preempted by
PreemptionMode.RECOMPUTE because not enough KV cache space.
total_cumulative_preemption_cnt=1
```

接下来是一个 NVIDIA Dynamo 路由日志示例。这里，第一行显示本地解码工作者（Worker）上有 90% 的前缀缓存命中，从而使预填充在本地运行。下一行显示本地缓存未命中。路由器随即将预填充调度到远程 GPU-node-03 工作者节点：

```
[Router] 2025-05-03T14:23:11Z INFO KVRouter: prefix-cache hit (90%) for
model=DeepSeek-R1; routing to local vLLM worker
[Router] 2025-05-03T14:23:12Z INFO KVRouter: cache miss; dispatching remote
prefill to GPU-node-03
```

## 全栈推理优化

高性能 LLM 推理需要在堆栈的每一层进行协调优化。这包括从模型架构和内核实现到运行时引擎、系统编排和部署基础设施的所有内容。

**模型级**技术如剪枝（Pruning）、蒸馏（Distillation）、稀疏性（MoE 路由）、高效注意力（如 FlashAttention）和量化感知训练可以减少计算和内存需求。在**内核级**，融合操作、自定义注意力引擎（如 FlashInfer）、Tensor Core 利用、分块平铺（Block tiling）和异步内存传输有助于最大化 GPU 吞吐量。

**运行时**策略如动态批处理、分页 KV 缓存、CUDA Graphs 以及计算与通信重叠将使 GPU 在可变负载下保持饱和。**系统编排**层可以使用预填充/解码分离、智能路由、多租户隔离和自动伸缩（带有热备用）来平衡延迟并提高成本效率。

> 许多生产系统使用基于 Kubernetes 的编排来运行分离的预填充与解码部署。它们使用入口控制器（Ingress controllers）根据负载或用户优先级路由请求。并且它们保留热备用（Warm standby）GPU Pods，准备在流量激增时启动。

最后，你应该探索**部署模式**，如地理分布式边缘服务、智能 API 网关批处理、用于模型变体的 CI/CD，以及实时性能分析。这将提供生产环境中的最高可靠性和适应性。**表 16-2** 描述了堆栈中每一层的一些常见优化方法。

**表 16-2. 堆栈各层的常见优化方法**

| 堆栈层级                                     | 关键技术                                                     |
| :------------------------------------------- | :----------------------------------------------------------- |
| 模型 (Model)                                 | 剪枝和知识蒸馏以在最小精度损失下缩小模型尺寸；稀疏性 (MoE) 以跳过计算；高效注意力 (FlashAttention) 以减少内存占用和中间缓冲区；FP16/BF16 或 INT4/FP8 的量化感知训练以增强低精度下的鲁棒性。 |
| 内核 (Kernel)                                | 融合算子内核 (例如 Linear + GELU + LayerNorm) 以减少启动开销和内存流量；自定义注意力内核 (FlashInfer) 用于块稀疏 KV 和 JIT 编译内核；利用 Tensor Core 和专用指令 (cp.async, TMA) 进行矩阵运算。 |
| 运行时 (Runtime)                             | 如 vLLM、SGLang 和 NVIDIA Dynamo 中实现的带有延迟控制的动态批处理 (例如，连续批处理) 以合并请求；分页 KV 缓存管理以灵活分配内存并合并批次 (vLLM 的 PagedAttention)；CUDA Graphs 和缓冲池以减少每次推理的开销；使用多个 CUDA 流 (一个流用于数据传输，另一个用于计算) 以重叠计算与通信；使用基于事件的同步——且仅在必要时使用。 |
| 系统编排 (System orchestration)              | 预填充-解码分离以消除队头阻塞；智能路由和缓存亲和性 (Cache affinity) 以平衡负载和缓存命中；多租户隔离和每用户配额以防止“吵闹的邻居”；使用热备用实例进行自动伸缩以隐藏模型加载时间，并接受轻微的成本增加以换取流量激增期间显著更好的延迟。 |
| 部署与基础设施 (Deployment & infrastructure) | 地理分布式和边缘部署以减少网络往返时间 (RTT)；智能 API 网关在服务器池之间进行请求级批处理；CI/CD 流水线以金丝雀模式推出新的量化或内核优化模型变体；高带宽互连 (NVLink/NVSwitch 和 InfiniBand) 以及 GPU 和 CPU 之间的 NUMA 亲和性以获得最佳内存访问。 |
| QoS 与伸缩 (QoS & scaling)                   | 具有 SLA 感知的动态批处理和尾部延迟控制；使用 MIG 或流优先级的 GPU 隔离以强制执行 QoS；用于 TTFT、TPOT、利用率和显存带宽利用率的实时性能分析仪表板；基于工作负载特征的动态并行切换 (TP, PP, DP)。 |

在进行优化时，考虑跨层协同效应非常重要。例如，量化（模型层）减少了内存占用，允许更大的批处理大小（运行时层）而不发生 OOM 错误，这反过来允许编排组件合并每个 GPU 周期的更多请求。

你也应该有一个以性能分析为驱动的关注点。持续的性能分析应指导下一步优化哪一层。例如，在融合和量化之后，如果 CPU 成为预处理和后处理的瓶颈，请投资于更快的 Tokenizer 或将某些任务卸载到 GPU。

应用优化时总会有权衡。像层级 CPU 卸载和高级解码方法这样的技术增加了复杂性。例如，推测解码增加了一个草稿模型，Medusa 增加了多头并行解码。这些通常保留用于极端情况，如超长上下文或不稳定的延迟方差。更轻量级的方法，包括稀疏性、批处理和分离架构，在生产中提供了大部分收益。

建议采用全栈优化方法，协调模型架构、内核设计、运行时行为、系统编排和部署策略。这意味着保持你的软件堆栈更新，包括 CUDA、cuDNN、NCCL 等。较新的版本通常包含最新的优化和错误修复。

全栈方法降低了每一层成为瓶颈的可能性。这样，团队可以系统地消除瓶颈，实现一致的低延迟，并在大规模 LLM 推理中最大化硬件利用率。

## 调试正确性问题

监控也有助于捕捉由错误（Bugs）引起的异常。例如，如果内存使用量随时间持续攀升，可能是你的 CUDA 内核中存在内存泄漏。建议在测试期间使用 Compute Sanitizer (`compute-sanitizer`) 来捕捉设备内存错误、竞争条件和越界访问。示例如下：

```bash
compute-sanitizer --tool memcheck your_binary
```

如果某个 GPU 显示的利用率远低于其他 GPU，它可能由于无声的 NCCL 故障或未捕获的错误而退出了 NCCL 通信组。你可以通过在日志中查找 `WARN NCCL_COMM_FAILURE` 来检查 NCCL 错误代码。它提供非常详细的错误日志。

> 通过设置环境变量 `NCCL_DEBUG=WARN` 启用 NCCL 调试。这将有助于浮现原本可能无声的错误。不过请注意，NCCL 日志非常冗长！

使用 NCCL 测试套件调试 All-reduce 和 All-to-all 的性能与正确性问题。你也可以使用 `ncclCommGetAsyncError` 配合 `ncclCommAbort` 来检测和处理异步通信错误。考虑启用 NCCL 的 IB GID 追踪，并使用 NVSwitch 系统遥测来检测互连层面的问题。

你应该在 Prometheus 的 Alertmanager 中设置警报，以检测异常模式，如“GPU 利用率 < 10% 持续至少 60 秒”、“内存使用率超过 W% 阈值”、“NVLink 错误率 > X”、“PCIe 重放（Replays）超过 Y 阈值”、“温度超过 Z 度”等。

在实践中，你可以配置 Prometheus Alertmanager 规则，如**表 16-3** 所示。这样，你可以主动调查问题。

**表 16-3. 基于 GPU 系统的常见 Prometheus 警报示例**

| 指标                 | 条件             | 严重程度          | 说明                    |
| :------------------- | :--------------- | :---------------- | :---------------------- |
| GPU 利用率           | < 10% 持续 > 60s | Idle (空闲)       | 利用率不足              |
| GPU 利用率           | > 90%            | Bottleneck (瓶颈) | 可能饱和                |
| 内存使用率           | > 80%            | Warning (警告)    | 接近 OOM                |
| 内存使用率           | > 95%            | Critical (严重)   | 内存溢出错误的高风险    |
| 温度                 | > 85 °C          | Warning (警告)    | 接近热节流              |
| 温度                 | > 95 °C          | Critical (严重)   | 关机或硬件损坏风险      |
| NVLink 重放/恢复错误 | ≥ 1              | Critical (严重)   | 指示链路重试或恢复      |
| NVLink CRC 错误      | > 100 errors/sec | Critical (严重)   | 链路上的 CRC 故障率高   |
| PCIe 重放错误        | ≥ 1              | Critical (严重)   | PCIe 总线上的数据包重试 |
| 不可纠正的 ECC 错误  | ≥ 1              | Critical (严重)   | 数据损坏，需要重置      |

你还需要设置硬件错误计数器和警报。例如，如果报告了 ECC 错误或 NVLink 重试，应立即报警，因为这些会迅速降低性能或导致掉线（Drop-outs）。掉线是指 GPU——或其互连——无声地断开连接。例如，NVLink 可能会掉线——或者 GPU 在发生致命错误后可能“从总线掉落”。

> 使用 DCGM 获取每条链路的 NVLink 吞吐量和总计，包括 `DCGM_FI_DEV_NVLINK_TX_BANDWIDTH_L*`、`DCGM_FI_DEV_NVLINK_RX_BANDWIDTH_L*` 和 `*_TOTAL`。如果需要，回退到 `nvidia-smi nvlink`、Nsight Systems/Compute 或 NVSwitch 计数器。

考虑一个 NCCL 故障案例。你可能会收到一个警报，显示一个节点的 GPU 利用率接近 0%，而其他节点为 90%。你可以通过检查节点日志并找出哪个节点正在生成 NCCL 错误来开始调试该问题。

这种类型的主动监控和警报让你能更快地捕捉这些问题，找到故障节点，并开始将其恢复正常。在这种情况下，你可能想要重新初始化 NCCL 通信器或执行全节点重启（只需确保节点在重启后重新加入 NCCL 组）。

你可以通过为 NCCL 错误增加一个“NCCL Error”计数器来更快地捕捉这些问题。此外，你的推理服务器可以记录 NCCL 错误，这些日志将被 Fluentd 或 AWS CloudWatch 自动抓取并转换为计数器。

然后你可以在 Grafana 中的每节点 GPU 利用率图表之上叠加错误计数器图表。这将使 NCCL 错误与 GPU 利用率的下降相关联，从而使你能够更快地识别和修复故障节点。

> 应用级计数器在生产中极其有用——尤其是与系统指标结合使用时。

在通过实际指标验证（包括增加的吞吐量、降低的延迟、提高的利用率等）之前，优化不应被视为成功。鉴于现代 AI 推理系统的复杂性，严格的测量驱动的系统性能调优方法是必不可少的。

简而言之，你应该结合应用级计数器、从 Fluentd 或 AWS CloudWatch 自动抓取的日志计数器以及底层系统指标。这种全栈遥测提供了在生产工作负载上以峰值性能运行——并优化——多节点 LLM 推理系统所需的可视性。你应该将指标、计数器和日志视为系统行为的“基本真理（Ground truth）”。

> 我们的直觉和感觉常常会把我们引向错误的调试路径。但指标不会撒谎。提前正确地对代码进行插桩——并在出现问题时信任指标。

## 动态批处理、调度与路由

即使模型已经在集群中进行了最佳的分区和并行化，在多节点推理集群部署中仍有更多的应用级优化机会。在本节中，我们重点介绍通过动态批处理请求以及使用优化的调度和路由策略来最大化 GPU 利用率、最小化延迟并提升吞吐量的技术。

### 动态批处理 (Dynamic Batching)

推理服务系统中最强大的性能技术之一就是批处理（Batching）。通过将多个传入输入合并为单个批次供模型一起处理，批处理通过在多个输入上分摊固定成本（如内核启动和内存加载）来提高吞吐量。然而，这是以单个请求的延迟为代价的。一些单个请求可能需要等待一段时间（例如 2 毫秒）才能加入批次并被处理。

动态批处理是请求批处理的一种特化形式，它即时组装动态大小的传入推理请求批次。它可以配置为缓冲请求一段时间，或者直到达到给定的批次大小。所有现代 LLM 推理引擎都支持动态批处理，包括 vLLM、SGLang 和 NVIDIA Dynamo。

动态批处理与*静态批处理（Static batching）*形成对比，后者锁定固定的批次大小（或将所有序列填充到最长的一个），然后等待该批次中的每个请求完成才返回结果。这可能会导致早到达的请求产生无限制的排队延迟——并在填充（Padding）上浪费 GPU 周期。

使用动态批处理，系统积累传入的请求，一旦达到目标批次大小或经过短暂的超时（例如 2 毫秒），就分发“已到达的所有内容”。这将最大延迟限制在你指定的超时值内。

凭借其即时调整大小的能力，动态批处理让你可以在多个序列之间分摊内核启动开销——同时避免静态批处理的最坏情况延迟。这提高了 GPU 利用率，并在可变负载下提供了可预测的延迟。**图 16-5** 展示了静态批处理与动态批处理的区别。

*(图 16-5：静态批处理与动态批处理的区别)*

动态批处理让系统根据实际请求到达模式和延迟目标（例如最大延迟）自动增大或缩小批处理大小。关键在于智能地进行批处理，在保持延迟处于可接受范围内的同时提高整体吞吐量。现代推理引擎实现了微批处理（Microbatching），即在将批次分发到 GPU 之前仅积累几毫秒的请求。通常使用 2–10 毫秒的延迟，但这应根据延迟 SLO 进行调整。

例如，你可以配置 2 毫秒的批处理延迟，以确定服务器在分发批次之前等待额外请求的时间。如果在该间隔内到达了三个请求，批处理器会立即将它们分组并发送到 GPU。任何在 2 毫秒计时器到期后到达的请求将被收集到下一个批次中。这种超时驱动的触发器限制了每个请求的排队延迟（不超过 2 毫秒），同时通过将多个序列分组在一起来提高吞吐量。

这些微批次减少了增加的延迟，并允许 GPU 同时处理多个请求——而不是一次处理一个。大型 LLM 模型在小批处理大小时受限于内存带宽，因此增加批处理大小将提高算术强度（Arithmetic intensity）——以及整体硬件利用率。

在实践中，LLM 服务系统会选择一个平衡的批处理大小，既能提供良好的吞吐量，又不会产生太多延迟。在高负载下，动态批处理可以同时改善吞吐量和延迟，因为它防止了 GPU 在请求之间空闲。事实上，如果请求到达率很高，批处理可以减少整体队列等待时间，因为在相同的时间内处理了更多的工作。这有利于高负载流量模式下的端到端延迟和尾部延迟。

在低 RPS（每秒请求数）下，动态批处理会增加一点延迟，因为它在等待其他请求加入批次。这会稍微增加低 RPS 时的延迟。然而，在中等到高 RPS 下，延迟被分摊，并且与如果我们逐个运行所有内容会发生的排队延迟相比变得微不足道。因此，批处理降低了整体延迟，包括尾部延迟。

> 你应该使用像 Grafana 这样的工具绘制延迟百分位与负载的关系图来验证改进。使用批处理，随着吞吐量的增加，你通常会看到整体 p50 延迟保持平稳——甚至下降。这会一直持续到一个拐点。记录下这个拐点，并保持在这个值以下。

根据延迟 SLO 配置批处理非常重要。例如，如果你承诺对特定长度的请求实现 2 秒的 p99 延迟，你就不能让一个请求在批处理队列中等待 500 毫秒。默认情况下，你的动态批处理延迟初始值应设置得远低于 p99 延迟要求（在 1–2 毫秒的数量级），以避免过度的批处理延迟。

使用自适应批处理延迟，该值可以在低 RPS 时动态降至接近 0 毫秒，并在需要时在峰值负载下增加到 5–10 毫秒。vLLM 等引擎采用了这种自适应方法，以在不同流量模式下维持 SLO 合规性。

批处理主要利好高流量场景。如果流量低，系统只会运行单个请求，延迟将非常低。然而，在高负载下，系统可以应用激进的批处理来实现高吞吐量，并将提供更好的整体延迟，因为它避免了队列堆积并在许多请求之间分摊了延迟。

### 连续批处理 (Continuous Batching)

连续批处理，也称为*飞行批处理（In-flight batching）*或*迭代级调度（Iteration-level scheduling）*，通过在每次 Token 生成迭代时重新填充批次，而不是等待完整序列完成，来维持高 GPU 利用率。它会驱逐已完成的请求，并完全基于 GPU 的就绪状态立即拉入新请求。这项技术对于聊天助手等低延迟用例尤为重要。

与动态批处理等超时驱动的方法相比，事件驱动的连续批处理策略消除了空闲计算槽位和其他这些方法的填充开销。通过从不依赖固定的“最大延迟”计时器，连续批处理允许新请求在生成过程中途加入正在进行的批次——而无需阻塞在最长的序列上，如**图 16-6** 所示。

*(图 16-6：连续批处理允许新序列（请求）在生成过程中途加入批次)*

在这里，连续批处理最小化了推理计算流水线中浪费的槽位。它消除了每批次因等待最长响应完成而造成的空闲时间。连续批处理不是等待批次中的所有序列完成（由于输出长度可变，这很低效且导致 GPU 利用率不足），而是在每次迭代时用新序列替换已完成的序列。这种方法允许新请求立即填充 GPU 槽位——从而实现更高的吞吐量、降低的延迟和更高效的 GPU 利用率。

在大型模型上，批处理 4–8 个请求通常可以将吞吐量比 1–2 个请求的批次提高一倍或三倍。这是因为 GPU 的数学单元和内存流水线得到了更好的利用。而且每个请求的额外延迟仅在几毫秒的数量级——使该策略成为低延迟用例的理想选择。**表 16-4** 总结了迄今为止讨论的不同批处理策略，包括天真的静态批处理策略。

**表 16-4. 静态、动态和连续批处理的比较**

| 方面         | 静态批处理 (Static)        | 动态批处理 (Dynamic)                                  | 连续批处理 (Continuous)             |
| :----------- | :------------------------- | :---------------------------------------------------- | :---------------------------------- |
| 触发器       | 固定批次大小或序列长度     | 批次大小目标或超时                                    | Token 生成完成事件                  |
| 延迟界限     | 无界；等待满批次           | 受限于 `max_batch_delay_ms`                           | 最小；中途驱逐并重新填充            |
| 填充开销     | 高开销：填充至最长序列     | 中等开销：在每批次内填充                              | 低：槽位无需等待填充即可重新填充    |
| GPU 空闲缓解 | 差：尤其是在可变大小负载下 | 较好：但如果计时器在小批次上触发，可能会导致 GPU 空闲 | 优秀：让 GPU 在每次迭代都保持饱和   |
| 实现         | 简单                       | 中等：需要计时器和队列管理                            | 复杂：需要逐 Token 的协调和状态跟踪 |

### 连续调度 (Continuous Scheduling)

请求调度的另一个维度涉及并发模型执行。vLLM 社区称之为*连续调度*。其理念是，如果模型很小——或者批次大小受到延迟目标的限制——单个模型实例可能无法完全让 GPU 饱和。

连续调度像操作系统调度器对待 CPU 一样对待 GPU。它在单独的 CUDA 流上启动独立的小内核。这让硬件 Warp 调度器能够跨任务交错 Warp，而无需显式上下文切换。

例如，如果我们的推理引擎没有完全让 GPU 饱和，它可以在同一 GPU 上并发运行多个模型推理流。这样，当一个实例正在等待数据传输或非 GPU 操作时，另一个实例可以执行。

具体来说，在解码阶段，一次生成一个 Token 通常会让 GPU 利用率不足——尤其是在单个文本序列上执行时。这是因为相对于计算量，工作负载相对较小且需要大量的内存移动。

为了解决这种低效问题，推理引擎可以交错来自不同用户请求的多个解码任务。例如，如果 10 个用户同时在解码不同的序列，你不能简单地将它们批处理成一个大的矩阵乘法。这是因为每个序列的长度可能不同。这将阻止 GPU 执行高效的矩阵运算，除非进行大量填充。

相反，连续调度器可以在单独的 CUDA 流上为每个序列的下一个 Token 快速连续启动 10 个小解码内核。这通过利用 GPU 的细粒度 Warp 调度器实现了跨序列的真正并发。因此，内核在 SM 之间交错执行。这通过近似轮询（Round-robin）的处理模式防止了空闲周期。

通过在自己的流上排队每个解码任务，GPU 可以重叠跨序列的内存传输和计算。这减少了每 Token 延迟并提高了整体吞吐量。

GPU 在 Warp 级别在内核之间切换。当一个解码内核因 CPU 上的低效控制流或网络 I/O 等待而停滞时，其他活跃内核可以立即继续。这保持了 GPU 的充分利用。

连续调度通过维护一个准备运行的 Token 任务队列，甚至在 Token 级粒度上也实现了高利用率。最终结果类似于批处理。GPU 始终在进行 Token 生成工作，但它不需要将它们组合成一个巨大的矩阵乘法。这只是意味着我们不让 GPU 在 Token 计算之间闲置。这种 Token 级粒度的调度对于最大化数百万用户的吞吐量至关重要。

在固定间隔或每当 GPU 释放时，连续调度器将所有待处理的下一 Token 请求收集到一个可配置大小的批次中，然后将该批次作为单个 GPU 调用分发。这有助于平衡吞吐量和延迟。

> 如果你正在设计自定义调度器，请考虑混合方法：使用短计时器和最大批次大小进行 Token 调度。

像 vLLM 和 SGLang 这样的最先进系统合并了动态批处理和连续调度。它们的自定义连续调度器是围绕挤出 GPU 时间“气泡”的想法构建的。

vLLM 的 **PagedAttention** 是这种混合方法的一个具体例子。PagedAttention 将注意力键值（KV）缓存分解为切片或页面（Pages），并动态地对活跃序列的页面特定计算进行分组。这通过交错大型预填充 GEMM 与快速、小型解码内核，在高负载下维持接近 100% 的 GPU 利用率。

vLLM 高效地使用块级 KV 缓存结构来单独跟踪每个请求的状态。这种细粒度的簿记通过实时连续打包和多路复用工作负载，提供了快速的流式响应和最佳的硬件利用率。

另一个例子是 SGLang 的 **RadixAttention**，它使用基于树的 KV 缓存分组。这实现了类似的接近 100% 的 GPU 利用率，并为未使用的缓存页面引入了惰性驱逐。我们稍后会更详细地介绍 vLLM 的 PagedAttention 和 SGLang 的 RadixAttention。这两种方法都是开源的，因此你可以直接在代码中查看它们的调度实现。

### 无停顿调度（分块预填充）

当提示词极长时，你可以将它们分成块（Chunks）并交错预填充和解码工作。这被称为*无停顿调度（Stall-free scheduling）*或*分块预填充（Chunked prefill）*。

考虑一个 20K Token 的提示词。通过将提示词分成 5K Token 的块，你减少了每次迭代的最大停顿，并将每次迭代的工作限制在固定数量的 Token 上。这提供了可预测的每迭代延迟。不同块大小的分块预填充成本如**表 16-5** 所示。

**表 16-5. 使用不同块大小的分块预填充处理 20K Token 提示词的成本**

| 块大小 (Chunk size) | 块数量 | 每块注意力操作数 (Ops)               | 总 MLP Tokens |
| :------------------ | :----- | :----------------------------------- | :------------ |
| 20K                 | 1      | 20K × (20K + 1) ÷ 2 = 200M           | 1 × 20K = 20K |
| 10K                 | 2      | 50M + 150M = 200M                    | 2 × 10K = 20K |
| 5K                  | 4      | 12.5M + 37.5M + 62.5M + 87.5M = 200M | 4 × 5K = 20K  |

在这里，由于 KV 缓存，每块的注意力成本随完整上下文增长。每个新块针对其自己的 Token 与所有先前块的 Token 计算注意力。

在自注意力中，长度为 $N$ 的序列的 QK 点积数量是三角形的，大约为 $N(N + 1) \div 2$。这是每层每头 $Q \times K$ 点积运算的数量。这个成本是 $N$ 的二次方 ($O(N^2)$)。所以对于 $N = 20,000$，大约是 2 亿次运算。分块并不减少这个总数。分块只改变了解码器可获得工作的时间点。分块预填充的好处是延迟平滑和改进的重叠，而不是减少总的注意力点积运算。

> 预填充自注意力每层每头执行 $N(N + 1) \div 2$ 次 $Q \times K$ 点积。这个成本是 $N$ 的二次方 $O(N^2)$。注意，分块和流水线预填充仅改变重叠和延迟；它们不改变注意力工作的总量。减少总注意力成本的唯一方法是减少有效上下文或使用局部或稀疏注意力。例如，你可以使用固定注意力窗口 $W$ 将成本降低到 $O(NW)$。

简而言之，分块预填充技术以紧密交错的方式调度提示词预填充处理和 Token 生成。它统一了大批量预填充计算的高效率与流式解码的响应性。这可以降低尾部延迟，同时在许多工作负载上保持高吞吐量。

### 延迟感知调度与动态路由

延迟感知调度可以分析传入的请求，创建动态批次，并路由批次以最小化延迟。考虑六个提示词同时到达一个双 GPU 推理系统。提示词长度按顺序为 6K, 2K, 6K, 2K, 2K, 和 2K。让我们比较天真的先进先出（FIFO）调度器与延迟感知调度器。

首先，FIFO 调度器根据到达顺序创建两个批次。批次 1 是 [6K, 2K, 6K] 并被发送到 GPU 1。批次 2 是 [2K, 2K, 2K] 并被发送到 GPU 2。结果总结在**表 16-6** 中。

**表 16-6. FIFO 与延迟感知调度对一系列传入提示词的处理 [6K, 2K, 6K, 2K, 2K, 2K]**

| GPU   | 提示词批次 | 自注意力 QK Ops T(N) = N(N + 1) ÷ 2                          | Tokens N |
| :---- | :--------- | :----------------------------------------------------------- | :------- |
| GPU 1 | 6K, 2K, 6K | T(6K) + T(2K) + T(6K) = 18,003,000 + 2,001,000 + 18,003,000 = 38,007,000 | 14K      |
| GPU 2 | 2K, 2K, 2K | 3 × T(2K) = 3 × 2,001,000 = 6,003,000                        | 6K       |

在这里你看到，FIFO 策略将前三个提示词 [6K, 2K, 6K] 发送到 GPU 1，大约进行 38,007,000 次自注意力 $Q \times K$ 点积和 14,000 个 Token 的 MLP 计算，而 GPU 2 大约进行 6,003,000 次自注意力 $Q \times K$ 计算和 6,000 个 Token 的 MLP。关键路径由 GPU 1 决定，大约为 38,007,000 次自注意力 $Q \times K$ 点积。

相反，延迟感知调度器分析六个请求的预期延迟，并将它们重新排列为两个批次 [2K, 2K, 6K]。在这种策略中，每个 GPU 的自注意力成本为 22,005,000 次点积，MLP Token 为 10,000 个，如**表 16-7** 所示。

**表 16-7. 延迟感知调度对提示词的处理 [6K, 2K, 6K, 2K, 2K, 2K]**

| GPU   | 提示词批次 | 自注意力 QK Ops T(N) = N(N + 1) ÷ 2                          | Tokens N |
| :---- | :--------- | :----------------------------------------------------------- | :------- |
| GPU 1 | 2K, 2K, 6K | T(2K) + T(2K) + T(6K) = 2,001,000 + 2,001,000 + 18,003,000 = 22,005,000 | 10K      |
| GPU 2 | 2K, 2K, 6K | T(2K) + T(2K) + T(6K) = 2,001,000 + 2,001,000 + 18,003,000 = 22,005,000 | 10K      |

这将关键路径的自注意力从 38,007,000 减少了约 42% 至 22,005,000。因此，延迟感知调度器可以显著降低 TTFT（首 Token 时间）。因为延迟感知调度器知道提示词长度、自注意力中的三角形 $N(N + 1) \div 2$ 成本以及 MLP 中的 $O(N)$ 成本，它可以平衡计算权重并最小化整体延迟。例如，在这个例子中，延迟感知调度器将 2K Token 请求前移。这允许这些较轻的请求更快地完成预填充并开始解码，而无需等待较重的 6K Token 预填充完成。

> 这些计数假设使用可变长度融合注意力内核，仅对有效 Token 进行计算——例如，FlashAttention 或带有 cuDNN 后端的 PyTorch `scaled_dot_product_attention` (SDPA)。如果你的内核填充到批次中的最大序列长度，注意力算术成本将接近批次大小乘以该批次中最长序列的 T 个 Token。在这种情况下，FIFO 和延迟感知策略之间的差异将会缩小。

简而言之，如果到达顺序在批次内不是全局理想的，天真的 FIFO 调度器可能会使其中一个 GPU 过载。延迟感知调度器可以分析传入的请求并将它们重新排列成更平衡的批次，以最小化延迟。

> 一些推理服务框架已整合强化学习来在线调整调度策略。这建立在上述延迟感知策略的基础上，通过自动针对不断变化的流量模式进行调优。

## 系统级优化

系统级优化技术包括重叠通信与计算、最大化 GPU 利用率、管理功率和热量问题、处理错误以及优化内存访问。总的来说，我们要确保 GPU 硬件在近乎 100% 的时间里做有用功，即有效吞吐量（Goodput）。我们还将讨论吞吐量与延迟之间的权衡，以及如何找到适合生产 SLO 的平衡点。

### 重叠通信与计算 (Overlapping Communication and Computation)

正如我们在本书中反复看到的，重叠通信与计算对于在许多 GPU 上分布的大型模型的推理性能至关重要。像 GB200/GB300 NVL72 这样的系统拥有极高的带宽（机架内高达 ~130 TB/s 的聚合 GPU-to-GPU NVLink 带宽），这意味着重叠甚至更加有效，因为数据传输足够快，计算不太可能因为数据而挨饿。然而，即使有了 NVL72，使用多个 CUDA 流和非阻塞集合通信（Nonblocking collectives）也是至关重要的。这将允许你执行通信-计算重叠以隐藏延迟并保持所有 72 个 GPU 忙碌——即使在这样的速度下。

> 在实践中，你要启用 NCCL 的 GPUDirect RDMA 支持——并使用 NCCL 的组调用（Group calls）来重叠多个小的 All-reduce 操作。此外，考虑使用 SHARP（在带有合适交换机的 InfiniBand 上可用）将规约操作卸载到网络结构上。此优化可以提高某些架构和拓扑上的吞吐量。在某些测试中，它显示出对于大型全对全通信大约 10%–20% 的吞吐量提升。

考虑一个需要在 GPU 之间传输数据的推理步骤（例如，MoE 的全对全通信）——或者仅仅是将提示词数据从 CPU 发送到 GPU，以及将响应从 GPU 发送到 CPU 以返回给最终用户。在所有这些场景中，只要可能，我们都希望在 GPU 做其他工作的同时异步执行这些传输。

一个基本的例子是使用单独的 CUDA 流来重叠计算和数据传输。例如，在专用传输流（使用锁页主机内存）上启动 `cudaMemcpyAsync`，同时计算内核在默认流上运行。

> 记住对通过 PCIe 或 NIC 进行的传输使用页锁定（Pinned/Page-locked）主机缓冲区。这样，CUDA 驱动程序可以直接 DMA 并与计算重叠。

然后，你应该仅在需要数据时才与 CUDA 事件同步。这可以防止 GPU 因 I/O 而空闲。这样，数据传输将使用非阻塞 CUDA 调用（例如，Pinned memory）和 CUDA 流，以便 GPU 不会停滞等待这些操作。在具有全双工 NVLink 带宽的现代 GPU 上，这种重叠可以在很大程度上隐藏通信延迟。但是，你应该在你特定的工作负载上通过性能分析来验证这一点。

这种上下文中的数据传输可以包括将新的提示词嵌入从 CPU 发送到 GPU——或将最后一个 Token 的 Logits 从 GPU 移动到 CPU。例如，就在 GPU 计算出一批新 Token 的 Logits 之后，我们可以启动这些 Logits 到 CPU 的异步拷贝以进行后处理（例如，采样）——或作为流式响应发送回客户端——并立即为仍留在 GPU 上的另一批 Token 启动下一个计算内核。

在多节点场景中，像 All-to-all 这样的通信集合可以通过将工作分成块来重叠。一些 MoE 运行时使用的一种技术是将 Token 批次分成两半，当第一半的 Token 正在被专家处理时，可以开始发送第二半的 Token。

当专家完成第一半时，第二半的数据已经到达目标 GPU——它们可以无需等待地继续处理。这需要仔细调度 NCCL 调用相对于计算内核的时间。

例如，你可以使用 CUDA 事件来实现这种重叠，以信号通知半个批次何时完成。此时，你可以为该部分数据启动 NCCL All-to-all——同时下一部分完成计算。你可以使用 CUDA Graphs 捕获这些异步模式并减少启动开销。结果是提高了 GPU 利用率，因为 GPU 等待通信的空闲时间更少。

另一个重叠区域是在 CPU 和 GPU 之间。例如，当 GPU 忙于生成下一个 Token 时，CPU 可以并发准备下一个输入或对先前生成的 Token 执行结果处理。高度优化的推理引擎将任何 CPU 侧预处理（例如，输入文本分词）与来自其他请求的 GPU 计算并行重叠。例如，引擎可以在重叠当前批次的 GPU 计算的同时准备下一个批次。

这听起来很明显，但它需要多线程架构，使得一个线程处理网络和排队，而另一个线程启动 GPU 操作。还有另一个线程可能处理响应后处理等。Python 或 C++ 推理循环绝不应因为等待原本可以并发完成的操作而阻止 GPU 开始新工作。

在流水线并行场景中，系统应重叠（流水线阶段之间的）通信与每个阶段内部的计算。例如，GPU 0 在完成 Token t 的部分后，可以在开始处理另一个序列的 Token t+1 的同时，开始向 GPU 1 发送激活值。

现代互连和框架允许计算内核和数据传输重叠，只要它们使用不同的资源。你应该在推理流水线中使用多个 CUDA 流，以便每个流水线阶段都有一个独立的流。这样，激活值的发送/接收可以与其他正在为不同微批次执行计算的流并行发生。其效果是减少了流水线气泡。

在网络侧，像 NVIDIA GPUDirect RDMA 这样的技术允许网络适配器（如 InfiniBand NICs）直接读写 GPU 显存，无需涉及 CPU——也无需通过主机内存中转。通过利用 GPUDirect 进行 KV 缓存或专家激活值的跨节点传输，推理引擎减少了延迟和 CPU 开销。

GPUDirect RDMA 移除了通过主机内存的中转，允许 NIC DMA 引擎直接访问 GPU 显存。CPU 发布工作请求，但数据路径绕过 CPU。这释放了 CPU 核心用于其他任务。

考虑一个实际例子，使用 InfiniBand 在两个 NVL72 机架之间的一个 MoE 层进行全对全通信。如果你通过先执行全对全然后计算来同步地做这件事，没有重叠的话 GPU 利用率会保持在低位。然而，通过重叠批次的全对全通信与计算，你可以显著提高 GPU 利用率。

在 MoE 层重叠全对全通信涉及将 Token 批次分成两半，并在第一半的专家仍在计算时开始第二半的交换。这通过与计算交错来隐藏通信延迟。

本质上，每个 GPU 开始计算其本地专家对它已有的 Token 的输出——同时从其他节点接收剩余的 Token。当它完成第一批次时，第二批次已经到达并可以立即计算。

这种优化相当底层，涉及 NCCL 组和 CUDA 流之间的 CUDA 事件同步，但它产生了值得的吞吐量改进和更平滑的延迟。在实践中，你首先作为 NCCL 组的一部分启动 NCCL All-to-all 而不等待完成。然后你立即启动下一个计算内核。务必使用 CUDA 事件检查完成情况。

我们还可以将 I/O 与计算重叠以用于流式输出。例如，一旦生成了几个 Token，我们就通过套接字将它们发送给最终用户——此时模型已经在处理接下来的 Token。这将网络延迟（例如，将响应发送回最终用户）隐藏在计算（例如，后续 Token）之后。因此，最终用户看到的是稳定的流，没有停顿。

这种向客户端非阻塞流式传输的模式被所有现代 LLM 推理引擎采用。它们使用单独的线程通过 SSE/WebSockets 向客户端发送 Token 更新。

> 如果你自己实现流式输出，请确保使用线程安全的队列或锁将生成的 Token 移交给网络线程。你不想在这个移交过程中引入同步问题。这可能非常难以识别和调试。

如果我们不重叠，我们将生成少量 Token，然后让 GPU 空闲，同时我们将 Token 发送回最终用户。相反，我们的网络发送由异步线程处理，该线程从响应缓冲区获取输出并将其流式回传给用户。这允许主推理线程（例如，CUDA 流）立即继续生成更多 Token。这有效地将 Token 生成（计算）与输出传输（I/O）流水线化。

简而言之，重叠通信与计算需要以流水线思维思考，并尽可能使用异步操作。现代 GPU 和网络硬件提供了实现这一点的原语，包括 CUDA 流、非阻塞集合通信和 RDMA。CUDA 设备侧原语，如与 `cuda::barrier`（来自 CUDA pipeline 头文件）一起使用的 `cuda::memcpy_async`，有助于在内核内部重叠全局到共享内存的移动与计算。（注意：主机到设备和设备到主机的传输仍然需要显式 CUDA 流和页锁定主机内存来与计算重叠。）高效的 LLM 推理系统充分利用了这些特性。

总之，只要可能，始终重叠通信与计算。否则，扩展到许多 GPU——即使有 NVLink/NVSwitch——也将达不到峰值硬件性能。这些优化对最终用户来说大多是不可见的，因为模型输出没有变化。然而，这对于从推理集群中榨取每一滴性能并让用户持续回访你的应用至关重要。

### 最大化 GPU 利用率与吞吐量 vs. 延迟的权衡

性能调优的终极目标是让 GPU 尽可能忙于做有用功，如矩阵乘法——同时最小化任何空闲和未充分利用的 GPU 资源。前面描述的许多技术，包括批处理、并行和重叠，都是为了实现接近 100% 的 GPU 利用率而设计的。

GPU 利用率百分比——特别是有效利用率或有效吞吐量（Goodput）——是一个需要持续监控的重要性能指标。虽然你想追求 100% 的有效利用率，但你需要确保没有违反你的 SLO，例如延迟。在某一点上，你可能会达到收益递减点。

考虑一个推理服务器，目前的朴素实现显示 60% 的 GPU 利用率。调查发现，解码阶段是瓶颈，因为它在等待单个线程按顺序处理所有序列。让我们引入使用多流的并发解码。通过交错解码，如前所述，我们将 GPU 利用率提高到 95% 并使吞吐量翻倍。这证实了并发解码是有效的。

我们可以尝试通过将更多请求放入一个巨大的批次来达到 100%，但这会减慢单个查询的速度。绘制吞吐量与延迟曲线通常很有帮助，方法是测试各种批处理大小。在这个图表中，通常有一个急剧的“膝部（Knee）”，在那里吞吐量增益开始付出过高的延迟代价。

要在吞吐量-延迟曲线中找到最佳点（Sweet spot），首先尽可能开启全并发和重叠，这样你知道硬件可以保持忙碌。然后逐渐增加批处理大小，直到达到最大资源利用率。此时，测量单个查询的延迟并稍微回调（例如，从 16 回调到 8）以满足你的响应时间目标。

建议监控 p95 和 p99 尾部延迟以及 p50 中位数延迟。这是因为小的抖动和不均匀的批次填充可能会产生在 p95/p99 处明显的长尾异常值。在一个服务许多请求的大型集群中，即使是 0.1% 的异常值也会频繁发生。因此，p99——甚至 p99.9——对于测量用户体验可能比 p50 更重要。此外，长尾延迟通常迫使过度配置以满足激进的 SLO。因此，减少尾部延迟具有直接的成本效益。

> 一个好的经验法则是将你的批处理大小上限设定在略低于提供峰值吞吐量的值。团队通常将目标定为最大峰值吞吐量的 90%，称为净空缓冲区（Headroom buffer）。这是因为全速运行可能会导致不可预测的延迟激增。例如，如果将批处理大小增加到 16 个请求开始给少数请求增加少量延迟，你应该将批次限制减少到 8。这将提供更一致的延迟，且仅略微降低吞吐量。

一些推理系统可以根据这些指标即时动态调整批处理大小。我们将在第 17–19 章涵盖此技术——以及更多自适应推理策略。

重要的是要记住，让 GPU 持续以 100% 运行可能会因触及功率和热量限制而导致节流（Throttling）。有时，以 90% 运行高效内核可以胜过以 100% 运行但遭遇节流的情况。接下来，让我们讨论 GPU 功率和热量约束——这是基于 CPU 的应用程序开发人员和系统工程师经常不考虑的特性。

### 功率和热量约束

另一个需要考虑的维度是功率和热量约束。如果冷却不足，持续以 100% 运行 GPU 将导致热节流。现代 GPU 系统采用液冷以减少热节流并维持性能。如果你运行的是较旧的风冷系统，请当心降频（Downclocks）。

当 GPU 利用率高时，你可以用 `nvidia-smi` 或 DCGM 监控降频。具体来说，DCGM 通过 `DCGM_FI_DEV_CLOCK_THROTTLE_REASONS` 暴露 XID 节流原因。而 `nvidia-smi` 会在 GPU 被功率节流时显示“Pwr Throttle”标志。

你也可能触及板卡上的功率限制——尤其是在加速时钟（Boost clocks）下。现代 GPU 在加速状态下消耗的功率显著更多，因此你应该监控 GPU 是否因触及功率限制而降频。如果发生这种情况，你将体验到低于理想的 GPU 性能。始终监控来自 DCGM 的 `DCGM_FI_DEV_POWER_USAGE` 指标，并在其超出正常范围时发出警报。

> 如果你的 GPU 持续触及功率限制，考虑启用动态加速模式（Dynamic Boost mode）——或稍微降频——以避免可能导致延迟激增的热节流。

要在软件中绕过这些约束，你可以通过添加微小的批次间延迟稍微放松利用率。你也可以限制并发。现代 GPU 也支持时钟封顶。因此，与其增加延迟，不如使用 `nvidia-smi -lgc` 或类似命令稍微限制 GPU 时钟，以防止触及热限制。这只会略微降低吞吐量——并提供更一致的性能。

从硬件角度看，如果冷却已经足够，你可以尝试改善冷却或提高功率限制。要增加功率上限，请使用 `nvidia-smi -pl` 或调整你的 GPU 加速设置。

这些都在提醒我们，将现实世界的系统推向极限可能会导致极大地影响性能的意外副作用。建议在推理引擎中包含一个“关闭加速（Boost-off）”标志，以便在系统检测到因功率或热约束导致的节流引起性能下降时，即时应用软件变通方案。这将使系统以稍微未充分利用且更冷的状态运行，直到完全恢复稳定性和性能。

### 错误处理

虽然通常不与性能关联，但高效地处理错误很重要。一个充分利用的推理系统处理过多错误激增的余地较小——尤其是因为错误处理很可能不是系统中最优化的代码路径。

在故障场景中，快速失败（Failing fast）是关键，因为如果请求会出错，最好立即返回错误，而不是用缓慢的失败浪费 GPU 时间。务必在推理调用周围实现适当的异常和超时，以捕获挂起或崩溃。

此外，建议实现背压（Backpressure）。这意味着如果错误激增，你可以开始拒绝新请求——或减少批处理大小——给系统一些恢复的余地。

在规模化时，建议构建一些净空（Headroom），添加一些主要处于空闲状态的额外副本。这些可以作为缓冲区，应对错误激增或其他意外情况。虽然自动伸缩可能是你想到的降低成本的首选，但请记住自动伸缩需要时间来配置新资源。

虽然空闲容量耗费资金，但失去客户或违反 SLA 的成本通常更高。在稳态期间建议至少保留 10%–15% 的缓冲容量。对于不能承受停机的更关键服务，建议配置 100% 的缓冲容量——或全集群副本。

预热的、空闲的节点简直无法替代，它们可以按需立即处理额外负载。失去最终用户的成本很可能高于保持少量副本空闲并准备好处理任何额外、意外负载的成本。

### 内存

优化资源利用率也延伸到了内存。虽然 GPU 显存和显存带宽增长有些渐进，但模型大小和上下文长度正在呈指数级增长。因此，内存仍然是一个需要优化的宝贵资源——并且在不久的将来仍将保持宝贵。

因此，你要有效地利用 GPU 显存和显存带宽。内存通常被模型权重和 KV 缓存填满。在推理期间，最好始终将模型权重保留在 GPU HBM 中。如果你将内存换入换出 CPU DRAM 或 NVMe 存储，你将招致额外的页面错误和传输延迟。

即使对于像 Grace Blackwell 和 Vera Rubin 这样的现代 CPU-GPU 超级芯片，这种额外的传输延迟也是存在的。因此，高性能 LLM 推理引擎显式管理内存，而不是依赖按需分页。

压缩是减少推理系统内存使用的有效技术。特别是对于 KV 缓存，它是预填充阶段为每个进入系统的查询生成的。

由于 KV 缓存的大小随查询数量和输入大小扩展，你应该考虑 KV 缓存压缩和量化。KV 缓存压缩/量化意味着如果模型可以容忍精度损失，则存储降低精度的键和值。而且，虽然不理想，但 KV 缓存卸载是针对很少使用的 KV 缓存数据的一个选项。

### KV 缓存卸载与内存池分配

通过将很少使用的 KV 缓存条目卸载（分页移出）到 CPU 内存或磁盘，推理引擎为更活跃的数据腾出空间。这类似于处理虚拟内存。

例如，vLLM 的 PagedAttention 使用托管内存池将 KV 缓存卸载到 CPU 内存和 NVMe 存储。同样，SGLang 的 RadixAttention 使用树状结构缓存，可以懒惰地驱逐最少使用的前缀。NVIDIA Dynamo 也有类似的 KV 缓存卸载和内存管理机制。

如果没有好的 KV 缓存分配器，当不同长度的请求流经系统时，你会产生过多的内存碎片（例如，~20%–30%）。这将限制你在遇到 OOM 错误之前可以处理的请求数量。记住使用大池大小的分配器，如前一章所讨论的。

采用适当的内存分页策略将把碎片减少到可以容忍的几个百分点。这意味着你可以将更多上下文打包进 GPU 并保持高利用率而不让系统崩溃。

通过适当的 KV 缓存内存管理，现代推理引擎可以在不耗尽 GPU 显存的情况下服务更多的并发用户。它们通过管理自己的 KV 缓存内存池并将数据卸载到 CPU 和 NVMe 存储来做到这一点。因此，它们以最小的碎片实现了接近完全的内存利用率。

权衡是如果上下文再次变得活跃并需要被分页调入时的一点额外数据传输延迟。然而，与总请求延迟相比，这种开销通常很小。

内存利用率和计算利用率是相辅相成的。如果内存管理不善，你会浪费内存，你就无法用更多并发任务充分利用 GPU。通过在 GPU 上保留尽可能多的相关数据，系统可以服务大量的并发请求。

糟糕的内存管理会导致峰值负载下反复出现 OOM 崩溃。这将使 GPU 退出资源池并导致级联延迟问题。使用适当的内存管理避免 OOM。这将最大化利用率并维持集群稳定性。

简而言之，高效的内存管理可以提高有效吞吐量，减少内存碎片，避免意外的 OOM 错误，并允许更多并发的飞行中请求——尤其是在高吞吐量场景下。

## 实时推理的量化方法

提高推理性能最有效的方法之一是降低精度。这将立即减少内存使用和内存带宽利用率——并提高计算速度。

量化用更少的位表示模型的权重——有时也包括激活值。现代 NVIDIA GPU 使用用于 FP16、FP8 和 FP4 格式的降精度 Tensor Cores 原生支持低精度算术。

在本节中，我们讨论专门用于推理的量化技术。这包括仅权重量化方法，如 GPTQ、AWQ、SpQR 和其他结构化稀疏感知方法——以及针对权重和激活量化的全精度降低。具体来说，GPTQ 和 AWQ 在实践中已被证明非常有效。

对于许多大型模型，使用 GPTQ 的 4-bit 仅权重量化可以保留 FP16 模型 99%+ 的精度。并且它提供了 ~2× 的推理加速和 ~4× 更小的模型占用。AWQ 进一步提高了 3-4 bit 量化的精度。这些技术已集成到许多 AI 框架中，包括 Hugging Face Transformers、PyTorch、vLLM 等。它们支持直接加载 GPTQ 和 AWQ 量化模型。接下来，我们将介绍使用降精度格式在精度和性能方面的权衡——以及如何将量化有效地、安全地集成到服务工作流中。

### 将精度从 FP16 降低到 FP8 和 FP4

最初，LLM 推理通过从 FP32 迁移到 TF32、FP16 或 BF16 等降精度格式看到了巨大的收益。例如，NVIDIA Tensor Cores 使用 FP16 对比 FP32 时执行 2× 的吞吐量。它通过将半精度乘加操作融合到专门的 Tensor Core 硬件流水线中来实现数学性能翻倍——且没有明显的精度损失。

FP8 将精度进一步降低到 8-bit 浮点。与 FP16/BF16 相比，这减少了一半的内存占用。并且它再次使 Tensor Core 数学吞吐量翻倍，因为 GPU 每个周期执行的 8-bit 乘加运算是 16-bit 运算的两倍。

虽然你可以通过简单地启用 TF32 数学运算 `torch.set_float32_matmul_precision("high")` 在 PyTorch 中获得适度的加速，但你想要充分利用 NVIDIA Transformer Engine (TE) 提供的 8-bit 和 4-bit 精度支持。TE 作为一个库提供 FP8 和 FP4 内核，这允许现有代码以最小的更改使用这些降低的精度。

NVIDIA 的 TE 在这些降低的精度下自动管理每张量（Per-tensor）的缩放因子。在推理时，你的推理服务器可以加载 FP16 模型但使用 FP8 矩阵乘法。

TE 对每个张量应用缩放以维持数值稳定性，使用的缩放因子通常通过两种方式之一选择：在训练期间使用代表性数据进行的固定的、提前校准步骤，称为*静态校准（Static calibration）*——或者一个动态计算的值，跟踪张量的最大绝对值，称为*基于 amax 的动态缩放（Amax-based dynamic scaling）*。**图 16-7** 展示了 TE 使用范围分析、缩放因子和目标格式进行精度转换。

*(图 16-7：NVIDIA Transformer Engine (TE) 在 Transformer 层上使用范围分析、缩放因子和目标格式进行精度转换)*

为了更高的压缩率，FP4 格式比 FP8 更好地减少了模型权重存储和流量。考虑到缩放元数据和打包，与 FP8 相比，有效减少通常约为 1.8×，与 FP16 相比约为 3.5×。然而，由于 FP4 的动态范围非常有限，可靠的 FP4 推理需要每通道（Per-channel）缩放——或其他校准，如 GPU TE 支持的每块*微缩放（Microscaling）*。这些技术是在最小化精度损失的情况下使 FP4 可用于大型网络所必需的。

### 仅权重量化 (GPTQ, AWQ)

现代 LLM 服务堆栈中的仅权重量化通常使用 GPTQ 或 AWQ 等方法将权重压缩为 4-bit 整数，同时保持激活值为较高精度，如 FP8、FP16 或 INT8。这使得权重内存占用比 FP16 减少约四倍，并将权重带宽减半或更好，且经适当校准后通常精度损失极小。

NVIDIA 的 FP4 实现（正式称为 *NVFP4*，但在本文中简称 *FP4*）在硬件中使用块状微缩放。NVIDIA TE 提供了以块粒度进行 NVFP4 微缩放的硬件支持。

> 根据内核使用每张量（Per-tensor）或每通道（Per-channel）缩放。建议探索激活值的每张量缩放和权重的每通道缩放。例如，当使用 FP8 E4M3 进行 KV 缓存量化时，通常使用每张量缩放。

每块微缩放意味着不是对整个张量使用单个缩放因子，而是为张量内的每个固定大小块（通常为 32 个元素）维护一个单独的缩放。这些单独的缩放适应局部值分布，以比单缩放量化更好地保留范围并减少量化误差。

> 始终使用反映你推理工作负载的校准数据进行量化。在训练数据子集上的一次性校准可能无法捕捉运行时使用模式。

在实践中，GPTQ（训练后量化）和 AWQ 常用于 LLM 上的 4-bit 权重——通常精度损失可忽略不计。来自 Hugging Face 等的开源工具可以自动应用这些技术。

MoE 专家结构使得权重量化更具吸引力，因为我们可以使用降精度权重在内存中装入更多专家。因此，我们可以减少交换到/来自 CPU 内存的专家数量。此外，如果需要，我们可以使用拥有更多活跃专家的更大模型——以及更多专家副本。

训练后量化（PTQ）工具如 GPTQ 应用近似二阶算法，逐层将权重量化至 3–4 bits，只需几个 GPU 小时，且几乎没有精度下降。较新的 GPTQ 变体通过非对称校准和并行计算进一步改进了该算法。这减少了量化误差并将高效的低位支持扩展到了更大的模型。

AWQ 识别权重的“显著”（Salient）通道的一小部分。显著通道产生具有大激活幅度的输出，这对模型输出有不成比例的大影响。

AWQ 在将所有权重转换为 4-bit 精度（例如，INT4）之前，使用通道特定缩放来保留这些通道。NVIDIA 的 TE 知道如何在保留的通道上使用通道特定缩放因子以维持模型保真度。

> 大多数 AI 框架如 PyTorch 和推理引擎如 vLLM 和 TensorRT-LLM 原生支持加载 GPTQ 量化和 AWQ 量化的模型检查点。

### 激活量化

将激活值与权重一起量化可以通过对 GEMM 输入——以及潜在的累加器——使用降精度值来提高性能。这将减少基于注意力的 KV 缓存和 MLP 层中中间激活值的内存流量。

然而，激活分布可能随输入变化很大。因此，如果没有适当的微调或校准，激活量化有时可能具有挑战性。一个中间地带是带有校准的 INT8 激活。这来自 NVIDIA 的 INT8 模式，该模式使用每张量校准，并在 TensorRT 中用于从一组代表性校准数据生成的激活直方图中选择缩放因子。

SmoothQuant，一种用于 8-bit 激活量化的免训练/免校准 PTQ 方法，可用于使用简单的行/列缩放算法将部分量化误差从激活转移到权重。这让我们能够对权重和激活都使用 INT8，只需极少的微调——从而实现低（例如 < 1%）精度损失的全 INT8 推理。

> 已证明在对权重应用 GPTQ/AWQ 之前使用 SmoothQuant 激活量化可以在低精度下更好地保留精度。

### 训练后量化工作流

前面描述的量化技术是在训练后应用的——相对于量化感知训练（QAT），后者是在模型训练（即模型预训练）期间完成的。典型的工作流是用 FP16/FP32 训练或微调模型，然后在代表性数据集上运行 GPTQ 或 AWQ 等训练后校准脚本以确定量化参数。然后将量化后的模型权重加载到推理引擎中进行服务。

如果需要，你也可以运行一个小的微调任务来恢复任何丢失的精度；然而，使用 GPTQ 和 AWQ 技术通常不需要这样做。如果你需要完整的 QAT，你可以只运行几个 epoch 的训练，在模型图中加入“伪量化（Fake quant）”操作以模拟评估期间的低精度数学运算。这将帮助你评估在此精度下的预期准确性。

> 在实践中，LLM 的量化感知微调计算成本昂贵且并不总是可行。然而，可以使用较小的校准数据集（例如 1,000 个提示词）配合百分位截断（Percentile clipping）或 LMS（损失感知量化）等技术来微调量化比例，而无需完全重新训练。

值得注意的是，你应该格外小心地平衡压缩与模型鲁棒性之间的权衡。量化可能会放大某些错误。例如，如果模型勉强处于某些事实知识的阈值，量化可能会将其推向错误。

始终在下游任务上进行验证，因为 PTQ 做的假设可能会遗漏微妙的分布偏移。因此，当使用降精度量化模型时，你应该对响应进行广泛的 A/B 测试，以确保你的评估显示质量或安全性没有退化。如果你看到退化，应该将模型保留在较高精度——或尝试在量化形式下执行轻微的微调以恢复性能。

### 结合权重和激活量化

将激活量化到 4-bit 仍然具有挑战性。将低精度权重量化与较高精度激活量化相结合通常会在内存节省、计算效率和准确性之间产生最佳权衡。因此，许多生产系统使用仅权重 4-bit（例如 GPTQ/AWQ）量化结合 8-bit 激活量化。

具体来说，在一种 W4A8（8-bit 激活）变体中，运行时解包 INT4 权重，使用学习或校准比例反量化为 FP8，并在 FP8 Tensor Cores 上执行矩阵乘法。这种混合路径由 TensorRT-LLM 等推理引擎提供，经适当校准后可实现接近无损的精度。这保留了激活的完整动态范围，同时与 FP16 相比将权重存储减少了 4×。

相比之下，传统的 INT4 和 INT8 W4A8 方案将 4-bit 整数（INT4）权重与 8-bit 整数（INT8）激活配对，并在 INT8 Tensor Cores 上运行计算。这种方法依赖基于直方图的校准将激活范围映射到 INT8 而无质量损失。

虽然 INT4/INT8 内核在现代整数 Tensor Core 流水线上可以提供略高的原始吞吐量，但它们需要仔细的激活校准，并且没有 FP8 那样的动态范围。

混合 INT4 和 FP8 方法结合了两者的优点。在混合方法中，4-bit 整数（INT4）权重被解包并重新解释为 FP8 输入。然后计算在 FP8 Tensor Cores 上执行。这种 INT4 和 FP8 混合 W4A8 变体提供了接近无损的精度、巨大的显存带宽减少，以及在现代 GPU 上的出色吞吐量。

对于现代 GPU，两种不同的低精度路径在生产中很常见。首先，NVFP4 工作流使用由 Transformer Engine 或 TensorRT 管理的带有微缩放的 FP4 块。其次，W4A8 工作流使用 INT4 权重配合 FP8 或 INT8 激活，在 TensorRT-LLM 中使用融合反量化执行。根据你的模型校准和精度目标选择路径。

总之，量化是降低推理成本的最佳方法之一。通过将模型大小削减 2× 或更多，你有效地使每 GPU 吞吐量翻倍。前面提到的技术，包括 GPTQ、AWQ 和 SmoothQuant，帮助你以最小的精度损失（例如，通常 < 1% 的下降）实现这些增益。

> 建议你从 8-bit 权重开始，然后评估 4-bit 仅权重量化以获得额外增益。只有当你需要最大化优化并且可以花时间进行校准时，才转移到 W4A8。

下一步是消除任何转换开销。通过将量化-反量化操作直接融合到计算内核中，你保留了使用量化的收益。

### 将量化-反量化步骤融合到执行图中

现代推理引擎使用 TE 执行权重打包并提供高效的混合精度数学计算，而没有使用 INT8 的显式校准开销。这些推理引擎通常实现 CUDA/Triton 内核，以便在需要时手动将“量化-反量化”步骤融合到执行图中。

只要单独的量化/反量化内核会引入太多额外的启动并抵消降精度数学运算的延迟和带宽优势，就应将这些步骤融合到图中。这对于缺乏原生融合 INT8 支持的推理后端尤为有用。

将量化-反量化融合到主计算内核中对于高吞吐量推理流水线特别有价值，可以通过消除由转换引起的瓶颈来恢复性能。接下来，让我们探索现代 AI 推理服务引擎支持的各种应用级优化。

## 应用级优化

除了核心模型和系统优化外，还有几种更高层级的技术可以显著提高 LLM 服务的性能和用户体验。这些方法在应用或推理服务层运作，改进提示词的构建和缓存方式、对话历史的保留方式、请求路由到不同模型的方式等。

这些优化不涉及修改模型权重或部署新硬件。它们是应用层的算法和系统级改进。它们本质上可以“免费”产生显著的效率和易用性增益，因为它们几乎不产生任何成本。

在本节中，我们将讨论几种此类优化，包括提示词压缩、前缀缓存和去重、回退模型路由以及流式输出。这些策略通过减少输入大小、避免冗余计算、提供不同请求类型的优雅处理以及改善最终用户的感知延迟来提高性能。

### 提示词压缩 (Prompt Compression)

用户经常向 LLM 发送非常长的提示词或对话历史。然而，并非所有这些上下文对于产生适当的响应都是必要的。提示词压缩是指一组技术，用于在不丢失相关信息的情况下缩短或简化输入提示词。

一些系统提示词或系统注入的指令非常冗长（“你是一个 ChatGPT，一个旨在帮助用户的友好助手……”）。大型系统提示词将在输入上下文中占据大量空间——而且是针对每个请求。

提示词压缩减少了模型需要做的工作量。它直接转化为成本节约，因为更短的输入意味着更少的 GPU 计算。

> 记住，基于 Transformer 的 LLM 模型内的注意力机制是 $O(n^2)$ 时间复杂度，其中 $n$ 是以 Token 数衡量的输入大小。

一种简单的提示词压缩形式是移除冗余或无关文本。考虑一个包含大量与其查询无关的文本块的用户提示词。这可能包括复制粘贴的文章，而问题仅与文本的一段有关。在这种情况下，上游组件可以在将其馈送给 LLM 之前总结或提取相关部分。

另一种提示词压缩形式是对话摘要或截断。例如，对于长聊天记录，对话的早期部分可能不再相关。系统可以通过将旧部分总结为精简形式来智能地修剪对话。

> 许多像 ChatGPT 这样的生产聊天机器人会自动进行提示词压缩，以保持在其上下文长度限制内——并加快整体处理速度。这对于长时间运行的对话是必备的。

为此，你可以运行一个小 LLM——或传统的基于规则的系统——将对话的最旧部分总结为简短摘要。通常策略是这样的：当对话超过最大上下文长度的 75% 时，总结最旧的 25% 的消息。然后将此摘要置于对话的较新部分之前。这也成为了未来的新提示词。

实施提示词压缩时，确保没有丢失关键事实。一种方法是让模型压缩提示词（例如，生成摘要），然后通过向模型询问有关它的问题来验证压缩后的提示词。通过这种方式，你的算法检查关键信息是否在压缩版提示词中得以保留。

这可以防止超长聊天的过度延迟，并使模型专注于对话的最新部分。通过截断早期轮次，你减少了有效提示词长度 $N$，这将预填充自注意力工作从 $N(N + 1) \div 2$（大约 $O(N^2)$）减少到较短窗口的较小三角形成本。所以，尽管成本随窗口大小呈二次方增长，但较小的 $N$ 对于非常长的对话来说有显著差异。

好的摘要可以通过过滤掉无关细节来改善响应。然而，糟糕的摘要可能会省略用户真正关心的输入重要部分。当系统有信心时——或者如果对话明显偏离主题时——摘要效果最好。

### 提示词清洗 (Prompt Cleansing)

另一种技术是提示词清洗。这用于改进输入格式和分词。它有助于减少发送给模型的不必要空白或标记。分词器处理每个字符，包括空格和换行符；我们发送的不必要 Token 越少越好。

虽然像 OpenAI 的 `tiktoken` 这样的分词器对空白的处理非常高效，但带有大量 markdown 和 HTML 的大型提示词可能会使 Token 计数膨胀。简单的预处理，如移除 HTML 标签和将花哨的引号转换为纯文本，可以避免奇怪的分词并减少 Token 数量。

例如，我们可以通过不发送不影响输入含义的空行和重复标点符号来潜在地减少推理引擎的计算量。这可能只在各处节省几个 Token，但在数千个请求中累积起来就很客观——尤其是对于长输入提示词。

在某些情况下，我们可以通过使用引用而不是完整内容来压缩提示词。例如，如果用户的提示词包含我们系统以前见过的一大段文本——也许是因为他们指的是我们要么先前存储的文档——我们可以用类似“file0”的引用来替换它。

然后可以对模型进行微调以检索该引用的实际内容——或者我们可以使用检索系统来处理它。这进入了检索增强生成（RAG）领域，我们要么在此不再深入，但关键点是如果有替代方法，我们并不总是需要通过模型馈送原始内容。

> 一些推理引擎支持每个会话设置一次系统提示词，而不是每次都发送。这是一个比为每个请求重新压缩系统提示词更好的解决方案。

我们将在下一节讨论如何利用前缀缓存提高大型系统提示词的效率。但现在，让我们使用提示词压缩来创建一个更短且功能等效的指令集。例如，可以训练模型使用特殊 Token 或元数据来代表长系统提示词。这样，它就不必总是处理冗长的自然语言版本。

考虑一个充满基于文本规则的 200 Token 系统提示词，可以用 10 个特殊 Token 替换，这些 Token 触发用 200 个自然语言 Token 表达的相同规则。这要求模型经过训练以解析元数据，推导规则并遵循它们。

目前有关于“配置（config）”Token 的研究，这些 Token 告诉模型为给定的配置 Token 加载一组特定的预配置指令。把这想象成给给定的提示词前缀（例如，系统提示词）分配一个唯一 ID。微调模型以识别像 `<POLICY_A>` 这样的 Token 作为例如长的、500 字策略的替身是很常见的。

> Hugging Face Transformers 库实现了流行的 CTRL 方法。这是开始使用配置 Token 进行提示词修剪的好地方。

使用特殊 Token 和元数据替换长系统提示词更多是训练方面的考虑。但如果它减少了提示词的大小并降低了预填充开销，它可以提供巨大的推理加速。在规模化时，这直接转化为每个查询所需的计算量更少——以及更多的成本节约。

### 前缀缓存 (Prefix Caching)

通常，对 LLM 的多个请求在其输入中共享一个公共*前缀*，因为许多查询可能以相同的系统提示词开始。与其每次都重新计算相同前缀的模型输出，不如计算一次并为后续请求重用相同的 KV 缓存。

这种技术被称为*前缀缓存*，有时称为前缀记忆化（Prefix memoization）。使用前缀缓存，Transformer 的状态，包括注意力层中的键和值，会在带有相同前缀的请求再次出现时被存储和重用。

> 前缀缓存可以通过重用重复部分的缓存前缀计算，将原本总工作量为 $O(N \times L)$（$N$ 个长度为 $L$ 的请求）转变为 $O(N + L)$ 的工作量。

vLLM 实现了前缀缓存（`enable_prefix_caching=True`）以避免重新计算。它首先识别传入提示词的前 $N$ 个 Token 是否与来自先前请求——或同一会话中较早时候——已在缓存中的前缀匹配。如果是，vLLM 避免对这 $N$ 个 Token 进行昂贵的注意力重新计算——只是将数据从缓存的 KV 复制到新上下文中。确保你已启用前缀缓存——并分配了足够的内存。

像 vLLM 这样的推理引擎可以自动将传入查询分组为微批次，以实现高 GPU 利用率，在许多请求中分摊开销，并使用连续批处理等保持低延迟。确保如果你不是直接使用 vLLM 这样的推理引擎，你要在堆栈中寻找使用前缀共享的方法。

前缀缓存可以加速具有重复前缀的工作负载。考虑一个场景，我们要根据一份长文档问 10 个独立的问题。每个问题都将在提示词中包含该文档，后面跟着一个问题，如“[长文档文本] 问题 1: …”，“ [长文档文本] 问题 2: …”，等。

所有 10 个提示词的文档文本都是相同的。通常，模型需要为每个问题重新处理整个文档的 KV 条目。这将导致 10× 的冗余工作，因为模型将重新编码长文档 10 次。使用前缀缓存，它只编码一次，每个后续问题仅产生较小的问题后缀的计算。

具体来说，使用前缀缓存，第一个查询将计算文档部分的 Transformer 状态，对于后续查询，模型可以直接跳到处理“问题 1: …”，“问题 2: …”部分，因为文档的内容与 KV 缓存中发现的前缀匹配。

使用前缀缓存，你的推理引擎可以产生近线性的加速。例如，如果你在一份文档上问 10 个独立的问题，使用前缀缓存处理这些问题的速度大约比不使用快 10×，因为长文档的注意力只计算了一次而不是 10 次。

前缀缓存的另一个应用是聊天会话，因为对话历史是“多轮”聊天会话中每个新“轮次”的公共前缀。当最终用户发送新消息时，早期的消息形成了一个不断增长的前缀。

优化的推理系统保留上一轮的 KV 缓存，只为新的用户消息计算 KV。这就是如果你不在轮次之间重置对话，像 ChatGPT 这样的大多数聊天模型所做的事情。

> 前缀缓存的好处在交互式设置中最为明显——特别是如果用户在同一上下文/会话中快速跟进。在这种情况下，第二个回答会比第一个快得多，因为前缀（对话历史）已经被缓存了。

除了在其 ChatGPT 消费产品中支持状态外，OpenAI 在其 API 中也支持有状态对话，这在幕后使用了某种形式的前缀缓存——以及许多其他优化。对于无状态 API 调用，如果客户端传递对话历史，前缀缓存可以实现类似的效果。在这种情况下，服务器可以从前缀缓存中检索预计算的隐藏状态——直到上一轮。然后它将从那里继续处理新的用户消息。

要自己实现这一点，你可以对对话历史进行哈希处理。这将给你一个一致的键来在缓存中查找。但是，你必须小心管理内存，因为为许多对话存储整个 KV 缓存可能会非常快地消耗 GPU 显存。vLLM 的分页通过将不活跃的缓存分页到 CPU 内存——或磁盘——并在缓存命中时将它们分页回 GPU 显存来提供帮助。

你还可以跨多个请求——甚至在单个请求内——去重提示词的部分内容。在这里，去重是指合并相同的子提示词以仅计算一次它们的 Transformer 状态。例如，如果两个用户几乎同时发送完全相同的提示词前缀，系统可以合并这两个提示词并仅处理它们一次。

如果输入有重复序列，这也可以在单个请求内发生。这在训练中更常见，训练使用记忆化解码等技术来去重重复文本。在推理中，在同一请求中获得长的、重复的输入序列很少见，但是可能的。

> 如果你的工作负载在相同前缀上有许多重复查询，你应该为缓存分配更多内存以最大化命中率。相反，如果前缀很少重用，你应该使用较小的缓存——甚至完全禁用前缀缓存。始终在生产中测量前缀命中率并据此进行调整。

前缀缓存通常实现为 Token 序列 Trie 树（发音为“try”）。Trie 树，通常称为*前缀树*，是一种基于树的数据结构，其中每条边代表单个 Token，每个节点编码从根到该点的 Token 序列。

在 Token 序列 Trie 树中，每个观察到的提示词前缀都存储为它自己的 Token 路径。这实现了共享前缀的快速查找。当新请求到达时，推理引擎遍历 Trie 树——逐个 Token——从根开始，直到它无法再匹配下一个 Token。匹配 Token 的序列落在完成最长缓存前缀的节点上，如**图 16-8** 所示，示例系统提示词为“You are ChatGPT, a friendly assistant designed to help users…”。

*(图 16-8：实现为 Trie 数据结构的前缀缓存)*

此时，系统重用共享的 KV 缓存数据（克隆指针）并恢复解码，但仅针对序列中的剩余 Token。这避免了对已缓存前缀的冗余自注意力计算，因为此前缀的注意力分数已经被计算过了。

SGLang 的 RadixAttention 使用压缩 Trie 树，或基数树（Radix tree），覆盖整个 Token 序列。这种类型的前缀缓存将 Token 序列折叠成树中的单条边以节省空间。树中的每个节点指向一个存储为连续 GPU 页面的 KV 缓存张量，该页面持有前缀的 KV 缓存。

RadixTree 数据结构空间效率高，并提供快速的前缀搜索、高效的插入和 LRU 风格的驱逐。以下伪代码展示了在 Token 生成期间使用基数树进行 KV 缓存查找：

```python
radix: RadixTree = RadixTree() # 保存边标签 + node.cache 指针

def generate_with_radix(prompt_tokens: List[int]):
    # 1) 查找最长缓存前缀
    node, prefix_len = radix.longest_prefix(prompt_tokens)
    # 浅拷贝该前缀的 KV 缓存
    model_state = ModelState.from_cache(node.cache) # 引用计数增加

    # 2) 处理剩余的提示词后缀
    for token in prompt_tokens[prefix_len:]:
        model_state = model.forward(token, state=model_state)

    # 3) 随着进行，在基数树中插入或分割边
    matched = prompt_tokens[:prefix_len + 1]
    # insert 返回此完整前缀的节点
    node = radix.insert(matched, cache=model_state.kv_cache)

    prefix_len += 1

    # 4) 现在自回归生成新 Token
    output_tokens = []
    while not model_state.is_finished():
        token, model_state = model.generate_next(model_state)
        output_tokens.append(token)

        # 缓存每个生成的前缀
        matched = prompt_tokens + output_tokens
        node = radix.insert(matched, cache=model_state.kv_cache)

    return output_tokens
```

当生成开始时，引擎调用 `radix.longest_prefix(prompt_tokens)` 沿基数树向下走多 Token 边，直到到达匹配提示词最长缓存前缀的最深节点。然后它使用 `ModelState.from_cache(node.cache)` 对该节点的 KV 缓存页面进行轻量级克隆。这为 `model_state` 提供了种子，而无需为缓存的前缀重新计算任何自注意力。

接下来，它只处理提示词中未见过的后缀——逐个 Token——并动态更新基数树。对于每个新 Token，它调用 `radix.insert(...)`，根据需要分割边或创建新边。然后它在每个新节点存储中间的 `model_state.kv_cache`。

一旦整个提示词被消耗，循环切换到自回归解码阶段，该阶段使用 `model.generate_next(model_state)` 生成新 Token。同样，这会将每个生成的前缀插入基数树。这种方法最小化了冗余计算，使用空间高效的 KV 页面存储，并执行快速前缀查找——所有这些都支持增量缓存更新。

SGLang 的 KV 缓存设计自动捕获所有常见的重用模式，包括多轮聊天、少样本示例（Few-shot examples）和分支逻辑。同时，它确保共享前缀作为大的、合并的内存块被获取，以实现高效的 GPU 访问。

知道何时使缓存失效总是一个挑战。如果其他事情需要缓存内存，你可能需要驱逐一些前缀。缓存系统支持不同的策略，如“最近最少使用”（LRU），其中最近最少使用的前缀首先被驱逐。例如，SGLang 的 RadixAttention 会在 GPU 显存稀缺时懒惰地驱逐最近最少使用的基数树叶子节点，如**图 16-9** 所示。

这是一个前缀缓存树结构——以及 LRU 驱逐策略——被 SGLang 用于多个传入请求。这个例子基于 LMSys 一篇很棒的 SGLang 博客文章。

*(图 16-9：多个请求的前缀缓存演变)*

在这里，有两个聊天会话和跨这些聊天会话的多个查询。每个的标签是一个 Token 序列（例如，子字符串）。绿色节点代表树中的新节点。蓝色节点是当前正在访问的缓存节点。红色节点已被驱逐。以下是每一步的分解：

1.  初始空的基数树是空的。
2.  服务器处理传入提示词“Hello!”并响应 LLM 生成的“Hi!”。通过这个简单的响应，许多 Token 作为单条边添加到树中。这条边链接到一个新的绿色节点。具体来说，系统提示词“You are a helpful assistant”；用户消息“Hello!”；和 LLM 响应“Hi!”被合并。
3.  服务器收到一个新提示词。这是多轮对话的第一轮。服务器成功在树中查找到提示词前缀并重用其 KV 缓存数据。新的一轮作为一个新的绿色节点添加到树中。
4.  一个新的聊天会话开始，步骤 3 中的节点 *b* 被分割成两个单独的节点。这让两个聊天会话共享系统提示词。
5.  来自步骤 4 的第二个聊天会话继续。然而，内存有限，因此必须驱逐步骤 4 中的节点 *c*，它显示为红色。新的一轮追加在步骤 4 的节点 *d* 之后。
6.  服务器收到一个新提示词（查询）。处理后，服务器将提示词插入树中。这要求根节点分裂，因为这个新提示词不与现有提示词/节点共享任何前缀。
7.  服务器收到一批更多提示词（查询）。这些提示词与步骤 6 的提示词共享前缀。因此，系统从步骤 6 分裂节点 *e* 并共享前缀。
8.  服务器收到来自步骤 3 中对话（第一个聊天会话）的新消息。在这种情况下，它驱逐步骤 5 中第二个聊天会话的所有节点（例如，节点 *g* 和 *h*）。这是因为它们是那一刻最近最少使用的（LRU）。
9.  服务器收到一条消息，请求对步骤 8 中节点 *j* 的查询提供更多答案。由于内存限制，系统需要驱逐步骤 8 中的节点 *i*、*k* 和 *l*。

这个例子演示了前缀如何在多个请求之间共享。此外，它展示了 LRU 缓存驱逐策略在前缀缓存上下文中的工作原理。接下来，让我们转向使用级联模型部署模式以更好地利用 GPU 资源。

### 模型级联与分层模型部署

并非所有查询都需要最大、最先进模型的能力（和成本）。一些用户请求足够简单，可以由更小、更快、更便宜的模型回答。选择正确的模型被称为*模型级联（Model cascading）*或*回退模型路由（Fallback model routing）*。

要实施模型级联，你可以使用分层模型部署。这是一种维护多个不同大小和能力的模型的方法。对于每个传入请求，我们根据查询复杂性、所需精度或当前系统负载动态选择使用哪个模型。

考虑一个问答（QA）推理部署，既有一个巨大的 7000 亿参数的最先进模型，也有一个较小的 700 亿参数模型，后者由于需要较少的 GPU 资源而托管起来更快、更便宜。如果用户问一个非常直截了当的事实性问题（由分类器或启发式方法确定），700 亿参数模型可能会处理得很好。

对于直截了当的问题，你可以将查询路由到较小的模型，它将在 50 毫秒内响应，而不是 7000 亿参数模型的 500 毫秒。如果由于低置信度（由响应中返回的 Logits 确定），小模型的答案被认为不令人满意，系统可以将问题路由到更大的模型。

这种两阶段方法可以降低整体平均延迟和计算使用量——尽管如果由于对小模型的初始响应缺乏信心而被路由到两个模型，单个请求可能会经历更长的延迟。人们普遍认为，许多商业 AI 服务，如 ChatGPT，使用这种将较简单查询路由到更小、更快模型的方法来降低成本和延迟。

在实践中，将比如 60% 的查询路由到 10× 更小的模型可以显著削减你的推理计算成本——如果设计和调整得当，可能会使总体成本降低 5×。这是因为昂贵的模型仅在需要时才使用。

> 维护多个模型和路由系统是一项工程开销，因为你不仅要监控一个模型的性能，还要监控第二个模型、两个模型之间的相互作用以及路由系统。只有当你的流量规模和成本结构使其值得时才追求这一点。

实施模型级联需要查询分类器或启发式机制来辅助模型路由决策。这实际上是一个相对较难正确解决的问题，因为该机制通常需要持续适应、全面的启发式规则和不断的路由器调优。因此，许多组织仍然依赖使用离线分析来更新决策标准的启发式和基于规则的路由器。

一种方法是将其视为一个推测性问题，类似于前面描述的推测解码。你可以先在草稿模式下运行较小的模型，并让它生成带有置信度分数的答案。如果置信度高，直接返回响应。如果低，调用更大的模型。

确保如果需要大模型，小模型的额外延迟不会让用户等待太久。在实践中，当你怀疑问题很难时，你可以并行运行小模型和大模型。这将小模型的延迟与大模型重叠。这看起来很浪费且反直觉，但对于某些延迟敏感的用例，为了保持低延迟这可能是有益的——以额外的计算为代价。

另一种方法是提前为众所周知的用户查询训练一个单独的分类器。例如，你可以分类复杂性并问，“这个问题需要大模型的广泛知识或推理吗？”如果不需要，使用小模型。

重要的是要注意，随着用户查询随时间演变，这个分类器本身将需要定期重新训练。此外，它引入了另一个可能降级和失败的组件——因此需要被监控。确保这个模型是轻量级、快速且健壮的——否则它将成为你推理系统中的新瓶颈。

最初使用简单的启发式方法很常见。例如，如果用户的查询简短且基于事实，如“今天天气如何？”或“谁是 X 的总裁？”，你可以直接使用较小的模型。对于较长的提示词（例如 > 50 个 Token）或包含 *explain（解释）*、*analyze（分析）* 或 *elaborate（详述）* 等词汇的更具创造性的查询，你可以直接将其发送到大 LLM 并绕过较小的模型。

你应该记录并标记每个请求是由哪个模型处理的（例如，“small”与“large”）——以及是否发生了回退。这让你能够按模型分解延迟、准确性和回退计数。

这种标记数据也让你能够监控调度器的决策。通过计算小模型的答案被拒绝的次数，你可以识别可能导致重新训练小模型或改进路由器启发式规则的模式。

相反，如果小模型处理许多查询都很好，你可以利用这些信息提高其置信度阈值。这将增加小模型的使用率，降低响应延迟，并改善最终用户体验。

你也可以基于容量进行路由。如果大模型集群处于最大负载，与其排队请求并增加延迟，我们可以暂时将不太关键的请求路由到较小的模型，后者可能没那么好但仍能快速给出一些答案。这是一种重负载下的优雅降级形式。用户可能会注意到质量下降，但这比超时或等待过久要好。

许多 AI 服务故意这样做。例如，在峰值负载期间，免费层用户可能会被默默路由到较小的模型，以释放较大的模型给高级层（付费）用户。当资源受限时，这是一种现实的权衡。

> 我们将在第 17–19 章涵盖更高级的动态和自适应推理服务器能力。这些功能在很大程度上取决于当前系统负载，包括可用 GPU 显存、显存带宽利用率、KV 缓存利用率等。

另一种形式的模型级联是基于内容本身的。例如，如果用户问了一些 LLM 由于安全原因拒绝回答的问题，一些推理系统将回退到一个专门微调过以处理不安全提示词的特殊模型——甚至回退到一个检索系统。

这更多是关于内容策略而非性能，但这仍然是模型路由逻辑的一个重要考虑因素。而且通常回退模型要小得多，所以这实际上最终是一个性能胜利，因为它快速处理了不安全请求——并释放了主模型来处理安全查询。

从部署角度来看，多模型设置需要仔细的扩展，因为每个模型需要足够的实例来处理其流量份额。有时一个模型可能会成为瓶颈，如果许多简单的查询进来，例如，小 LLM 饱和而大模型闲置。

你可以根据需要使用容器编排在一天中的某些时间自动伸缩并运行更多或更少的小模型实例——甚至动态地将 GPU 从大模型池转移到小模型池。

### 流式响应 (Streaming Responses)

为了提高推理系统的“感知”性能，你应该在输出 Token 生成时流式传输它们。这比强迫最终用户等待完整完成才看到响应要好得多。这样，用户可以在信息到达时开始阅读和处理。这可以使有效交互快得多——即使总时间是相同的。

人类阅读速度在每分钟 200 到 300 字之间，这转化为大约每秒 4–7 个 Token——对于更快的阅读者高达每秒 13 个 Token。维持这种流式响应速度是理想的。鉴于每个性能决策都归结为权衡，这是在做优化选择、规划容量等时要考虑的一个重要指标。

> 重要的是要监控系统的 Token 吞吐量与这种人类阅读速率的对比。例如，如果你发现由于模型延迟，你的系统仅以 2 tokens/sec 进行流式传输，那就是进一步优化的信号。

大多数现代推理引擎都支持流式传输，包括 vLLM、SGLang 和 NVIDIA Dynamo。当你启用流式传输时，服务器使用 WebSockets、Server-Sent Events (SSEs)、HTTP Streaming 协议等将 Token 刷新给客户端。并且它应该在 Token 生成时立即这样做，以满足前面提到的每秒 4–13 个 Token 的人类阅读指标。

模型实际上一次生成一个 Token——除非使用推测性或多 Token 解码技术，如 EAGLE 或 Medusa。因此，系统需要将那些生成的 Token 分组成 2–5 个 Token 的批次，刷新流，并将批处理的 Token 发送回最终用户。

重要的是不要在刷新之前在批次中积累太多 Token，否则你就破坏了流式传输的目的。另一方面，每包发送一个 Token 可能因开销而效率低下。框架通常在每个换行符或句末 Token 处刷新。

例如，如果一个答案是 100 个 Token 并且需要 5 秒完全生成，使用流式传输，如果批次各为 5 个 Token（5 tokens ÷ 20 tokens per second = 0.25 seconds），第一批 Token 将在 0.25 秒后到达。这将伴随着稳定的 Token 批次流直到响应完成。这样，用户可以在仅仅 0.25 秒后开始阅读。

如果没有流式传输，用户将盯着空白屏幕 5 秒钟，然后突然看到整个答案一次性显示，这并不理想。这可能导致狂怒点击（Rage clicking）和其他形式的用户挫败感。

从性能角度来看，流式传输会因为发送额外的小网络数据包而不是一条大消息而产生轻微的开销。但与模型的计算时间相比——以及与改善的最终用户体验相比——这种开销通常可以忽略不计。使用 HTTP/2 和持久连接有助于通过作为连续流发送 Token 而无需重新建立连接来减少开销。

> 注意 Nagle 算法和延迟确认（Delayed acknowledgments）。这种相互作用可能会增加数十毫秒的延迟，在某些堆栈上，最坏情况下设置可达约 200 毫秒。使用 `TCP_NODELAY`，并且在可用的地方，使用快速确认（quick-ack）或减少延迟确认计时器以最小化 Token 刷新延迟。这些有助于减少 Token 发送延迟，这对于实时流式传输至关重要。缺点是更多的小数据包填满管道——带宽效率降低。但在为超低延迟调优时请记住这一点。

正确管理流控制很重要，例如，如果客户端因网络问题消耗流的速度很慢，你不想阻塞模型的生成。理想情况下，推理引擎将继续生成整个响应——即使客户端在消耗流方面落后。

系统应使用单独的线程和 CUDA 流将数据发送回最终用户。这样，主 Token 生成循环不会因发送响应时的问题而中断。

推理引擎维护一个有界缓冲区来管理流控制并防止无限制的内存增长。如果客户端停滞或断开连接，就会发生这种情况。处理这些类型的边缘情况很重要，因为它们在生产场景中相当常见。在实践中，你可能允许积累高达 50–100 个 Token。

超过一定限制，推理引擎可以暂停生成或完全关闭连接。这样，如果客户端在生成中途掉线——或仅仅因为连接慢而跟不上——引擎可以停止产生 Token 并释放这些资源来处理其他请求。

选择缓冲区限制涉及平衡内存使用和用户体验。对于短响应这很少是问题，但在非常大的响应中有些常见——特别是对于较慢的消费者连接。

另一个改善流控制的技巧是使用 Token 池化（Token pooling）。如果模型生成 Token 的速度快于流式传输所需，系统可以故意添加延迟以平滑生成速率。例如，如果模型在 0.5 秒内爆发性输出 20 个 Token，因为它是响应的一个简单部分，一次性发送它们可能会显示一大块然后暂停等待下一部分。

你可能更喜欢你的应用程序 UI 使用更稳定的打字机效果。在这种情况下，你可以引入像 50 毫秒这样的人工流延迟在 Token 发送之间。这不会太影响实际延迟，它通过避免突发的 Token 爆发来改善 UX。

你可以使 Token 池化延迟可配置，以适应不同类型用户（例如，付费用户、免费用户等）的不同 UX。付费用户可以流得更快，而免费用户会被限制一点。这有助于用有限资源管理负载。

流式响应还允许最终用户在响应完成之前开始评估中间结果并采取行动。例如，如果用户看到答案的第一部分并意识到它没有朝着他们想要的方向发展，他们可以使用停止或中断按钮停止生成。

这节省了不必要的计算并改善了 UX，因为最终用户不必等待糟糕的完成结束才提供反馈。你应该监控——或至少测量——提前停止。

过多的显式停止可能表明模型的相关性存在问题。如果你看到许多用户在某一点停止，可能表明模型经常跑题或超过该长度后过于冗长。这可以通知你需要额外的微调以使其更简洁。或者你可以调整生成并发送给最终用户的最大 Token 数。

或者提前停止可能是由频繁改变主意的受挫“狂怒点击”型用户引起的。无论哪种方式，最好允许用户中断 Token 生成过程。这让推理集群回收资源来处理其他请求。

简而言之，流式传输是响应式 LLM 服务的必备功能。它不增加原始吞吐量——如果有的话，它增加了一点轻微的开销，但它提高了用户感知的系统速度。应小心实施流式响应以免干扰生成。不同的线程和 CUDA 流应处理将响应发送回最终用户，以便主 Token 生成循环不会在不稳定的最终用户网络连接上停滞。

> 建议你在测试环境中持续分析启用与禁用流式传输的端到端延迟。确保 Token 发射是均匀间隔的——并且没有引入如锁争用或 I/O 等待等重大瓶颈。像 Locust 这样的工具对 Python 友好且可以模拟客户端。这让你能够在规模上测试你的低延迟流式工作负载。

### 防抖 (Debouncing) 和请求合并 (Request Coalescing)

许多生产系统还实施称为*防抖*和*请求合并*的 UX 功能。通过防抖，或在响应前暂停，系统可以识别用户是否快速连续发送多个请求——无论是意外还是狂怒点击，如**图 16-10** 所示。

*(图 16-10：防抖在执行动作前暂停片刻)*

在这种情况下，系统可以将多个查询合并为一个查询，或丢弃除最新查询之外的所有查询。这些类型的应用级护栏有助于减少后端过度、重复、浪费的负载。

例如，如果用户双击提交或在一秒钟内发送两个非常相似的查询，系统可以丢弃重复项。狂怒点击者在受挫时会急躁地重新提交多次——通常是因为应用程序的响应时间差。

具有讽刺意味的是，防抖和请求合并可能会产生更多延迟，让“狂怒”用户群更加受挫，并导致他们点击更多！在这种情况下，如果 UI 在请求飞行期间禁用输入是有帮助的。这可以在 UI 级别防止双击。但拥有服务器端保护也是好的。

> 现代推理负载均衡器支持防抖间隔。使用适度的间隔（例如，2–5 毫秒），并在批处理效率和增加的延迟之间找到良好的权衡。下次你在 ChatGPT 中提交东西时，注意一下防抖延迟。既然你知道了，是不是觉得有点烦人？！

### Token 输出限制和超时

因为响应 Token 计数直接影响延迟，你可以实施输出长度限制或超时。这种应用层约束可以防止模型持续生成超出合理 Token 数量的失控生成。

Token 输出限制和超时有助于维持一致的延迟。它们还可以防止恶意和意外的提示词导致极长的生成，从而占用 GPU 资源。许多公共 API 设置严格的输出限制正是出于这个原因。它既是一种滥用预防机制，也是一种性能保障。

始终设置服务器端超时。例如，如果生成超过 30 秒，返回部分结果或道歉。用户期望快速失败而不是挂起的响应。

> 同时监控你的模型是否倾向于啰嗦。应用适度的 Token 限制，也许对于聊天回答是 4,096 个 Token，实际上可以通过保持模型在正轨上并避免长篇大论来提高质量。

根据用户需求选择这些限制和超时很重要。这些限制保持延迟可预测，并且还限制了最坏情况下的计算场景。这有助于容量规划等。

总之，输入优化（例如，压缩和清洗）、缓存、智能路由和 UX 优化（例如，流式数据）的结合可以减少工作负载，减小推理集群的规模，节省成本，并提高用户满意度。这些应用级策略补充了前面章节的底层优化——完善了提高 LLM 服务性能的整体方法。

## 本章要点

本章的技术表明，高效的 LLM 服务是一项整体工程努力，结合了现代 GPU 硬件、新颖的软件优化和全面的监控。性能分析工具识别瓶颈。系统的调试技术修复问题。以下是本章的一些关键要点：

**全面的性能分析**
跨推理堆栈执行端到端的性能分析。通过使用分析器测量每个阶段的延迟和资源使用情况，工程师可以查明减速和低效之处。这种数据驱动的方法指导针对性的优化以消除瓶颈。

**监控与可观测性**
为部署的推理服务实施健壮的监控。实时跟踪关键指标，如延迟百分位、吞吐量、GPU 利用率和内存使用情况，以及早检测退化或资源饱和。使用日志和追踪获得对每请求处理的可视性，并在大规模工作负载中识别热点或异常。

**调试与迭代调优**
通过采用系统的调试工作流，你可以快速解决性能和正确性问题——即使跨越数万个节点。这样，当意外激增发生时（例如，吞吐量下降或延迟增加），你可以轻松地从高层症状深入到底层问题。

**用指标验证优化**
诸如 GPU 调试器、内存泄漏检测器和与性能相关的单元测试等工具有助于验证像量化和内核融合这样的优化不会引入无声的性能和正确性错误。这种迭代的调优-测试循环对于随每个发布到生产的新优化保持高性能和可靠性至关重要。

**效率与成本优化**
专注于使推理更具成本效益的改进。每一项提高吞吐量和利用率的优化都将直接改善每次查询的成本。通过分析和精炼系统，团队可以用更少的 GPU 服务更多的请求。这将导致基础设施成本和电源效率的显著节省。

## 结论

性能分析、调试和全栈系统调优对于维持大规模高效、可靠和具有成本效益的 LLM 推理至关重要。随着模型规模向数万亿参数增长，生产集群正在向每个部署数十万甚至数百万个 GPU 扩展。在这种规模下，软件和硬件的持续协同设计（Codesign）仍然至关重要。仅依靠硬件来提高性能已不再足够——特别是当功率成为限制因素时。软件和硬件都需要协同设计并持续一起调优——且在堆栈的每一层。

优化的推理基础设施为最终用户提供快速的响应时间、可预测且稳定的系统行为以及低运营成本。只有这样，你才能交付服务于世界上最大、最强大模型的推理系统。

在接下来的章节中，我们将深入了解并优化现代 LLM 推理系统中计算、内存和网络最密集的部分：计算（预填充）KV 缓存，在集群中的所有工作者之间共享它，并使用它生成（解码）新 Token。





# 第17章 扩展分离式预填充与解码以实现大规模推理

正如前几章所述，LLM 推理可分为两个截然不同的阶段：预填充（Prefill）阶段和解码（Decode）阶段。预填充阶段处理输入提示词（Prompt），生成该提示词的模型内部键值（KV）缓存；而解码阶段则利用这些缓存值，逐个（或在推测解码中一次几个）生成输出词元（Token）。

这两个阶段的性能特征有着根本性的差异。**预填充阶段**是计算受限（Compute-bound）的，它涉及对成千上万个词元并行进行繁重的矩阵乘法运算，消耗大量的 FLOPS（浮点运算次数）。相比之下，**解码阶段**是内存 I/O 受限（Memory I/O bound）的，它在生成每个词元时都需要读取庞大的 KV 缓存并写入新值，对显存带宽造成巨大压力。简而言之，预填充是高吞吐的并行工作负载，而解码则是串行的、对延迟敏感的工作负载。

早期的 LLM 服务系统将这两个阶段视为一个整体，运行在同一硬件上。因此，它们通常通过请求批处理（Batching）来优先考虑预填充阶段的吞吐量。然而，随着交互式应用的增长，首字延迟（TTFT，即所有词元的预填充延迟）和每输出词元耗时（TPOT，或每词元解码延迟）等延迟指标变得与原始吞吐量同等重要。对于单个基于 GPU 的推理引擎来说，要同时优化这两个阶段的 TTFT 和 TPOT 是非常困难的。

大量批处理请求虽然能提高吞吐量，但会恶化 TTFT，因为每个请求都要等待最慢的预填充完成。它还会影响 TPOT，因为解码步骤会被新提示词的预填充任务阻塞。

单体式（Monolithic）推理系统必须在两者之间做出选择：要么以牺牲后续词元生成速度为代价来改善（降低）首字延迟，要么在让新请求承受高初始延迟的同时提高（增加）每词元吞吐量。在极端情况下，一个超长提示词可能会完全占用 GPU，阻塞其他用户的所有预填充工作。而且，一旦开始解码，这种“一次一个词元”的处理方式会让 GPU 的计算核心在每次词元生成之间处于空闲状态。

为了解决这些问题，研究人员和工程师开始探索解耦这两个阶段的方法。核心洞察在于：**预填充和解码实际上并不需要在同一硬件上运行——甚至不需要在同一种类型的硬件上运行。**

**分离式预填充与解码（Disaggregated Prefill and Decode）**意味着将其分配给各自专门针对该阶段需求进行优化的不同资源上。这一理念由 DistServe 论文中的系统率先提出，该系统证明，通过消除阶段间的干扰，可以同时满足 TTFT 和 TPOT 的严格延迟要求。

DistServe 的评估显示，与未采用分离架构的最先进基线相比，在满足严格延迟服务等级目标（SLOs）的前提下，其服务的请求量可能提高 7.4 倍。因此，业界框架开始尝试分离预填充和解码服务器。

开源的 vLLM 库引入了分离式操作，并结合了 LMCache 等组件。NVIDIA 的 Dynamo 实现了具有动态路由和自动伸缩功能的分离式预填充与解码，并公开了操作细节。许多提供商和开源框架都在实施或评估这种分离架构。例如，据报道，为了满足严格的延迟 SLO，OpenAI、Meta 和 xAI 的工业级服务系统都采用了这种分离方法。可以说，分离式预填充与解码已成为大规模 LLM 推理的标准实践。

在超大规模（Ultrascale）场景下，大型推理部署可能涉及成百上千甚至数百万个 GPU，服务数十亿次请求。在这些环境中，分离架构带来的成本和性能收益是巨大的。

通过拆分工作负载，你可以隔离优化每个阶段，避免其中一个成为另一个的瓶颈。本章的其余部分将探讨如何在极大规模下设计和运维分离式预填充/解码推理系统。

我们将探索在预填充和解码工作节点（Worker）之间路由请求的调度算法、在高负载下维持服务质量（QoS）的技术，以及使这种分离高效运行的机制。我们将涵盖从高速互连到专用解码内核的各个方面，并讨论为每个阶段使用不同 GPU 类型的异构硬件策略。

## 为什么要分离预填充与解码？

现代交互式 LLM 服务通常将 p99（99% 的请求）的 TTFT 延迟目标定为 < 200–300 毫秒。如果不分离预填充，几乎不可能保证这一点，因为“一刀切”的 LLM 服务方式会造成巨大的性能浪费。

作为背景参考，MLPerf v5.0 (2025) 对 Llama2 70B（700 亿参数）的推理基准测试目标是 p99 TTFT 约为 450 毫秒，TPOT 延迟为 40 毫秒。对于 Llama 3.1 405B（4050 亿参数），基准测试目标是 TTFT 约 6 秒，TPOT 为 175 毫秒。具体而言，这些 SLO 反映了 Llama 2 Chat 70B 和 Llama 3.1 405B Instruct 的 p99 目标。

考虑这样一个场景：一个用户的请求包含极长的提示词（数千个 Token），而另一个用户的请求提示词非常短。如果没有分离预填充和解码，当这些请求大约同时到达时，长提示词的预填充计算将长时间占用 GPU。

如果不进行分离，那个短提示词的请求就需要等待不必要的漫长时间才能开始解码。这被称为**干扰（Interference）**，因为一个请求的预填充工作延迟了另一个请求的解码工作。**图 17-1** 展示了连续批处理（Continuous batching）上下文中的预填充与解码干扰。

![图 17-1：同一 GPU 上并置预填充和解码导致的干扰](https://oreil.ly/GRkHs)

在简单的先进先出（FIFO）调度策略下，长提示词会放大所有人的尾部延迟。通常，队列前面的长任务或计算密集型预填充会阻塞后面更短、更轻量的请求。这被称为**队头阻塞（Head-of-line blocking）**，它会导致利用率低下、延迟异常以及最终用户的不满。

在灵活的分离式架构中，可以将大型提示词的预填充发送到专门的计算优化型预填充工作节点池，而轻量级提示词的预填充可以直接发送到解码工作节点——绕过预填充工作节点。这种灵活性使得短任务免受队头阻塞的影响，从而最大化整体吞吐量并最小化延迟的尾部效应。

### 分离架构的优势

分离架构主要有两个优势：**减少干扰**和**阶段特定的优化**。下面我们逐一讨论。

#### 减少干扰

通过分离，预填充任务不再与同一设备上的解码任务争夺资源。一个忙于生成大量 Token 的解码工作节点，不会阻止另一个用户的提示词被处理，反之亦然。

为每个阶段提供专用资源意味着长提示词的计算不会阻塞另一个用户的 Token 生成。在实践中，这会产生更可预测的延迟。**图 17-2** 展示了并置（Colocated）与分离式预填充和解码之间的对比。该实验在 DistServe 论文及作者随后的博客文章中有更详细的描述。

![图 17-2：并置与分离式预填充和解码的对比](https://oreil.ly/GRkHs)

在这里，SLO 设置为 P90 TTFT 0.4 秒和 P90 TPOT 0.04 秒（即图 17-2 中的水平线）。并置系统在给定的 TTFT 延迟范围内只能支持约 3 RPS（每秒请求数）的有效吞吐量（Goodput）。而在 TPOT 延迟界限内，它只能维持 1.6 RPS。因此，由于必须同时满足 TTFT 和 TPOT 延迟 SLO，并置配置的有效吞吐量仅为 **1.6 RPS**。

在分离这两个阶段并分配两个预填充工作节点（2 GPU）和一个解码工作节点（1 GPU）后（称为 **2P1D** 配置），预填充和解码工作节点的整体 RPS 均优于单 GPU 的并置配置。具体来说，预填充工作节点达到了约 5.6 RPS，解码工作节点达到了约 10 RPS。因此，2P1D 配置的有效吞吐量分摊到三个 GPU 上为 **3.3 RPS/GPU**。

> 3.3 RPS/GPU 的计算方式是取预填充工作节点（5.6 RPS × 2 = 11.2 RPS）和解码工作节点（10 RPS）两者的最小值，即 10 RPS。这 10 RPS 是由总共三个 GPU 完成的。因此，我们将 RPS 除以 GPU 数量（3）。在配置的 SLO 下，该系统的有效吞吐量结果为 10 RPS ÷ 3 GPU = 3.3 RPS/GPU。

> 在此对比中，解码侧的改进主要影响每 Token 延迟。同时，预填充隔离主要改善首字时间。必须同时满足这两个 SLO 才能计入有效吞吐量。

这种隔离也能改善尾部延迟。经验表明，分离式系统显示出更紧凑的延迟分布——避免了单体式系统中常见的长尾现象。通过消除跨阶段干扰，每个阶段都能更可靠地、以更可预测的一致性满足其 SLO。

现在，你可能会问：“3 倍的成本换取 2 倍的提升值得吗？”问得好。虽然需要额外的调优来使该方案更具成本效益，但它指明了收紧延迟分布和提高有效吞吐量的正确方向。你需要根据你的工作负载来决定。分离架构是满足有效吞吐量 RPS 需求的热门选择。

#### 阶段特定的优化

阶段特定的优化让每个阶段都能使用最适合它的硬件和并行策略。例如，预填充阶段是计算受限的。因此，你通常会增加张量并行度（Tensor Parallelism），以在高性能 GPU 上驱动峰值 FLOPS。此外，现代 GPU 提供了更低精度的模式（FP8 和 FP4），可以增加计算密集型预填充阶段的吞吐量。

> 你应该优先考虑对权重使用 FP4，并在验证通过的情况下对激活值也使用 FP4。许多技术栈使用 FP4 权重和 FP8 激活。这些降低的精度有助于最大化吞吐量并最小化 HBM 占用——且精度损失极小。现代硬件和软件堆栈（包括 NVIDIA Tensor Cores 和 Transformer Engine）均支持这些精度。

相比之下，解码阶段受限于内存带宽，并受跨 GPU 同步开销的影响。因此，它在较少或没有张量并行（通常 TP=1）时效率最高，因为它更多地依赖融合内核来提高算术强度——以及高显存吞吐量的 GPU。

在单体架构中，你必须为两个阶段选择同一种 GPU 和同一种并行策略，这对至少其中一个阶段来说是次优的。而分离架构让你能够独立调整每个阶段以实现最大效率。

拆分阶段也开启了**异构集群**的大门，即为预填充和解码角色分配不同类型的 GPU，以实现最佳的性价比。例如，使用计算优化型 GPU 进行提示词预填充，使用内存优化型 GPU 进行 Token 生成，每美元产生的吞吐量可能比同构部署更高。

> 在实践中，最新的 GPU 通常既有更高的 FLOPS 也有更大的显存。因此，直接使用最新一代 GPU 同时进行预填充和解码是很诱人且更常见的做法。但要知道，异构性是降低成本的一个可行选项。

我们将在本章稍后探讨异构集群的概念。我们将展示在大规模场景下，如何使用高端 GPU 处理提示词、使用较便宜的 GPU 进行生成，从而显著节省成本。

总之，分离架构消除了交叉干扰，使得对每个阶段进行专门处理成为可能。预期的结果包括更紧凑的延迟分布（因为不再有由不匹配的提示词大小引起的长尾）、在延迟约束下提高的吞吐量（有效吞吐量）以及更好的整体资源利用率。

> 你应该使用性能分析工具（如 NVIDIA Nsight Systems）来识别预填充和解码阶段的瓶颈。这些工具可以追踪不同工作节点上的 GPU 内核和 RDMA 传输。这将有助于验证解码内核是否完全重叠了通信等。

接下来，我们将讨论如何实际实现一个分离式服务系统，包括系统架构、通信以及决定如何充分利用分离集群的调度策略。

## 分离式预填充与解码集群池

在分离式部署中，我们维护两个（或更多）工作节点池，其中一组 GPU 专门用于预填充提示词处理，另一组专门用于 Token 生成。这些工作节点可以位于数据中心的不同节点或机架上——如果互连速度够快，甚至可以位于不同的数据中心。（注意：将预填充和解码保持在同一个数据中心内是实现现实 SLO 的务实设计选择。）

工作节点池通过网络进行通信，将预填充产生的模型 KV 缓存移交给将执行解码的 GPU。调度器或路由器负责协调这种通信。

考虑这样一种配置：模型权重被加载到两组 GPU 服务器上。一组是**预填充工作节点（Prefill Workers）**，处理提示词并计算 KV 缓存。另一组是**解码工作节点（Decode Workers）**，使用预填充工作节点生成的 KV 缓存处理 Token 生成。

这两组工作节点通常使用高速互连（例如 NVLink/NVSwitch 和 InfiniBand）以及通过 RDMA 进行的零拷贝 GPU 到 GPU 传输进行通信。在实践中，这些传输使用 **GPUDirect RDMA** 或 UCX，并且可以在 Nsight Systems 中与 CUDA 内核、NVLink 活动、存储指标和 InfiniBand 交换机指标相关联，进行端到端验证。

> 对于基于超级芯片的 NVL 网络结构（例如 Grace-Blackwell, Vera-Rubin 等），请使用 NVIDIA Multi-Node NVLink (MNNVL)，为 TP 解码保持启用 NVLink-first 集合通信，并在可用时为 AllGather 和 ReduceScatter 集合通信启用 SHARP。

当系统收到新请求时，通常由**解码工作节点**接收。这被称为**以解码为中心的设计（Decode-centric design）**。之所以首选这种方式，是因为预填充工作节点已经在忙于 KV 计算，处于计算受限状态。

让解码工作节点处理客户端 I/O、路由和会话状态管理，可以避免推理系统使预填充工作节点过载。此外，将请求入口集中在解码节点上简化了网络管理、自动伸缩和策略执行。

> 这只是分离式预填充-解码的一种系统架构风格，其中解码工作节点是所有请求的集中入口。NVIDIA Dynamo 推理系统使用了这种架构。另一种常见的架构是使用专用的、中心化的 API 路由器将请求路由到预填充或解码工作节点。但这需要在系统中增加一个额外的动态组件，并需要在路由器与预填充/解码工作节点之间进行额外的协调——以及额外的扩展和延迟考量。

解码工作节点收到请求后，会决定是自己进行预填充，还是将其“卸载（Offload）”给预填充工作节点池。如果决定卸载给预填充工作节点池，解码工作节点稍后将接收回 KV 结果，然后继续解码并生成下一个 Token。

下面是一个简化的 NVIDIA Dynamo 集群配置片段，定义了两个角色，它们使用 NVIDIA 的 **Inference Xfer Library (NIXL)** 进行基于 GPUDirect RDMA 的 KV 缓存传输，允许一个 GPU 通过网络直接写入另一个 GPU 的内存：

```yaml
roles:
  - name: prefill_worker       # 预填充工作节点角色
    model_path: models/llm-70b
    instance_count: 4          # 4 个预填充工作节点
    gpu_type: B200             # B200 Blackwell 计算密集型预填充

  - name: decode_worker        # 解码工作节点角色
    model_path: models/llm-70b
    instance_count: 8          # 8 个解码工作节点
    gpu_type: B300             # B300 Blackwell Ultra 高显存解码
```

> NVIDIA 的 **Rubin CPX 加速器**是预填充工作节点的另一个选项。Rubin CPX（CP 代表“上下文处理”）专门为计算受限的工作负载（如预填充）设计。Rubin CPX 标志着 NVIDIA 从“通用加速计算”GPU 转向针对更广泛 AI 工作负载（如推理）中特定阶段（例如预填充）进行优化的专用芯片。

在此配置中，我们有 4 个使用 B200 GPU 的预填充工作节点（为计算密集型预填充提供充足算力）和 8 个使用 B300 GPU 的解码工作节点（为内存受限的解码提供高 HBM 容量）。混合使用 B200 和 B300 有助于匹配其 FLOPS 和 HBM 容量特性，同时最小化成本。这两种角色都将使用 NIXL 和 GPUDirect RDMA 来传输 KV 缓存块。NIXL 抽象了通过 NVLink 和 RDMA 网卡进行 GPU 到 GPU 数据移动的传输层。它还为 GPUDirect Storage 提供了连接器，以便可以从不同的存储层读取（或写入）KV 缓存页面。

在底层，当系统运行时，每个解码工作节点都会注册一块 GPU 显存区域，以便预填充工作节点可以使用 RDMA 直接写入。通常，诸如 NIXL 描述符之类的内存注册元数据会在启动时或首次接触时进行交换。这样，对于每个远程预填充任务，只需要发送一个小的标识符，而不是完整的内存地址结构。

例如，Dynamo 使用 **etcd** 进行工作节点发现和租约管理。工作节点向路由器或控制平面注册必要的内存句柄，以便对等节点在需要时获取描述符。预填充工作节点将在首次使用时检索它们。这样，预填充请求可以仅包含目标 KV 缓冲区的 ID，使得控制消息非常轻量。

此外，NVIDIA Dynamo 的 NIXL 实现为推理数据移动提供了高吞吐量的 RDMA 和存储抽象，并包含用于 NVLink、基于 UCX 的网络结构和 GPUDirect Storage 的插件。因此，预填充工作节点可以直接将 KV 块写入解码 GPU 的显存中。

> 在预填充和解码使用不同并行（例如张量并行）布局的混合并行部署中，你需要在 NIXL 读取之后（数据使用之前）在接收端插入一个布局转换内核。这样，KV 页面就能匹配解码内核预期的布局。与网络传输相比，这种转换的延迟微不足道，并且避免了重新预填充。

这种架构解耦了每个阶段的扩展。例如，如果你发现由于并发长提示词过多导致预填充成为吞吐量瓶颈，你可以增加更多的预填充工作节点来提高提示词处理能力。

如果由于许多用户生成长输出导致解码成为瓶颈，你可以扩展解码工作节点。因为解码和预填充是分离的，扩展其中一个不会直接干扰另一个。

像 NVIDIA Dynamo 这样的系统支持动态的、运行时可配置的分离，你可以在不停止集群的情况下动态添加或移除预填充工作节点。新的预填充工作节点只需注册并开始从队列中拉取任务。如果预填充工作节点因任何原因（崩溃、重启、自动伸缩事件、网络分区等）离开集群，解码工作节点将暂时承担更多本地预填充任务以进行补偿。

NVIDIA Dynamo 的分布式运行时使用 etcd 进行工作节点发现和租约管理。其 Planner 组件可以通过撤销租约或启动新的自动发现工作节点来扩展工作节点。这种动态灵活性在超大规模下至关重要，因为负载经常波动。当这种情况发生时，你需要按需在角色之间交换工作节点。

### 预填充工作节点设计

预填充工作节点，或称*提示词服务器（Prompt Servers）*，是专门用于执行请求初始提示词预填充阶段的计算节点。本节讨论预填充节点如何架构以高效处理繁重的计算——以及它们如何在负载下平衡 KV 缓存填充的延迟与吞吐量。

因为预填充工作负载是计算密集型的，预填充节点应使用高 FLOPS 的 GPU，并针对大型矩阵乘法进行优化。每个预填充任务将 $n$ 个输入词元馈送到所有模型层。

预填充工作节点将并行使用数千个 GPU 线程——如果可用，还将跨越多个 GPU 节点。它们使用熟悉的并行技术，包括张量并行和流水线并行，以降低 TTFT。

**内存管理**

在内存方面，预填充节点需要加载完整的模型权重，并为提示词分配 KV 缓存。这个 KV 缓存随后会被传输到解码工作节点，我们稍后会看到。

预填充用模型参数和带有提示词输入的模型前向传递的工作激活值填充 GPU 显存。一旦 KV 缓存创建完毕，它会立即发送到解码工作节点。KV 缓存不会在预填充节点的内存中保留很久。

如果模型极大或提示词极长，由于内存限制，预填充可能需要跨 GPU 进行张量或并行拆分。预填充服务器在并行策略（数据、张量、流水线、专家 (MoE) 和上下文）上应保持灵活，以满足延迟目标。

一些推理框架会预分配一大块 GPU 显存作为预填充的工作空间。这减少了整体内存碎片和缓冲区分配时间。

**优化延迟 vs. 吞吐量**

在调优分离式预填充集群时，你面临一个基本的权衡：是最小化每个单独提示词的 TTFT，还是在高负载下最大化整体每秒请求数 (RPS) 或降低 TPOT。

分离式系统通过支持针对“延迟优先”方法与“吞吐量优先”方法的不同调度策略来处理这种权衡。接下来描述这两种方法：

*   **延迟优先方法 (Latency-first approach)**
    为了降低 TTFT，预填充节点应在提示词到达时立即处理——几乎不进行批处理。在这种模式下，你避免了等待其他请求来填满一个批次。因此，每个提示词都立即开始执行并尽可能快地完成——假设集群中有可用的 GPU。
    这种延迟优先方法的缺点是 GPU 利用率较低，因为你使用的是小批次或不进行批处理。因此，GPU 经常处于空闲状态，对于给定的集群规模，你的系统将服务更少的并发请求。在这种情况下，你可以过度配置预填充集群容量，或者使用微小的批处理大小（如 1）来保证请求的严格延迟 SLO。

*   **吞吐量优先方法 (Throughput-first approach)**
    如果峰值吞吐量（RPS）和最小化 TPOT 是你的首要任务，你应该将提示词分组成更大的批次以充分加载每个 GPU。通过将 8–32 个提示词累积成单个批次，你可以提高算术强度并保持 GPU 计算单元忙碌。这将增加整体吞吐量。
    吞吐量优先方法的缺点是每个请求都会产生等于收集批次所需时间的批处理延迟。批次越大，延迟越长。

对于极端吞吐量的推理系统配置，你可以选择使用数据并行或流水线并行将多个 GPU 分配给每个请求。

在**数据并行**中，整个模型在每个 GPU 上都有副本。批次被拆分为微批次分配给各 GPU。每个 GPU 在其完整的模型副本上对其数据子集执行前向传递。然后聚合所有 GPU 的输出以获得最终结果。

数据并行聚合了所有 GPU 的内存带宽和算力以提高每批次的性能。然而，它将最大并行度降低为（GPU 总数 ÷ 每个请求的 GPU 数）。这降低了你的整体并发请求容量。如果系统为单个请求使用了过多的 GPU，可能会导致资源空闲。这会在吞吐量和并发性之间造成不平衡。

**流水线并行**将模型的层划分为不同 GPU 上的连续阶段，例如 GPU 0 和 GPU 1。一旦 GPU 0 完成微批次 0 的阶段，它就将激活值转发给 GPU 1 并开始微批次 1 的阶段 1。这种流水线模式使所有 GPU 都在忙于不同的工作块。

流水线并行增加了每批次的吞吐量，但如果微批次大小或阶段划分没有仔细平衡，它会增加 GPU 间通信开销和流水线“气泡”。

最终，你为每个请求分配的每个额外 GPU 都会增加吞吐量，但会减少你一次能处理的请求数量——假设集群规模固定。你总是可以横向扩展 GPU 集群，但假设集群规模固定，你应该根据延迟 SLO 或吞吐量 SLO 对你的用例哪个更重要来选择配置。

**延迟感知调度与批处理**

分离式系统结合了前面提到的延迟感知调度策略来平衡这些因素。例如，它们可能保证单请求执行——不进行批处理——除非负载高到合并少量请求不会违反 TTFT 目标。

许多集群设计在调度器中包含 SLO 约束。例如，如果 p90 TTFT 必须 ≤ X 毫秒，系统将选择仍能满足典型提示词大小 SLO 的最大批处理大小或并行策略。

另一种策略是**自适应批处理窗口**。例如，在低负载下，它可以立即运行请求，批处理大小为 1。而在较高负载下，系统可以允许到达时间在小窗口（如 2–10 毫秒）内的请求组成微批次。这样，轻微的延迟可以产生巨大的 GPU 利用率收益——但只在需要且可容忍时使用。

许多推理引擎优先考虑预填充工作节点的延迟。系统通常尽可能快地执行提示词任务，甚至容忍一些 GPU 利用率不足，因为快速的首个 Token 能显著改善用户体验。

通常会配置比平均负载所需的更多的预填充容量。这样，预填充集群可以在没有延迟峰值的情况下吸收提示词的突发流量。在下一章中，我们将讨论动态重新平衡资源的自适应机制，以便预填充和解码工作节点都不会随着时间的推移成为瓶颈。

像 Kubernetes 这样的现代编排器可以自动扩展每一层。例如，如果预填充 GPU 利用率居高不下而解码利用率较低，编排器可以触发自动伸缩事件以添加预填充 Pod（或节点）——甚至可能移除一些解码 Pod/节点。

> 这种自适应扩展通常使用预填充队列长度等指标来驱动决策。

另一种选择是实施**优先级队列**，将短提示词调度到批处理较少的独立快速通道上。长且可批处理的提示词进入吞吐量优化队列。NVIDIA Dynamo 在调度中支持延迟等级（Latency classes）。你可以通过标记请求并为每个等级设置不同的批处理窗口来模拟这一点。

关键要点是预填充工作节点优先考虑快速周转。分离架构让我们能够在不损害解码性能的情况下做到这一点，因为解码运行在不同的一组工作节点上。我们可能会在低流量期间“浪费”一些预填充 GPU 周期，但在高峰流量期间我们保持了低 TTFT。对于交互式服务来说，这是一个值得的权衡。

### 解码工作节点设计

解码工作节点，或称*生成服务器（Generation Servers）*，专门用于自回归解码阶段。一旦提示词的 KV 缓存准备就绪，它就会被发送到解码工作节点，后者使用 KV 缓存尽可能快地生成剩余的输出 Token，以维持低 TPOT 延迟。

如果请求最初路由到解码工作节点（如图 17-3 所示），它必须首先决定预填充是在本地完成还是使用分离式路由器远程完成。如果决定远程预填充，它会将预填充请求推入预填充队列，由预填充工作节点获取。

预填充工作节点持续从预填充队列中拉取任务，读取前缀缓存中缓存的任何 KV 块，并计算预填充操作。然后它将 KV 块写回解码工作节点，由后者完成解码。

![图 17-3：预填充工作节点设计：读取前缀缓存 → 计算预填充 → 写入用于解码的 KV 缓存](https://oreil.ly/GRkHs)

解码工作节点的设计重点是高效处理许多并发序列生成——以及管理 KV 缓存的内存占用。在本节中，我们将描述解码服务器如何使用连续批处理和巧妙的内存管理技巧等技术实现高吞吐量。这些有助于降低 TPOT 延迟并提高可扩展性——特别是对于长序列。让我们从两个阶段之间的 KV 缓存传输开始。

**预填充与解码之间的 KV 缓存传输**

高性能分离架构需要在预填充和解码工作节点之间尽可能高效地移动 KV 缓存数据。通过使用像 NIXL（第 4 章中描述）这样的库进行直接 GPU 到 GPU 传输，我们可以避免 CPU 参与并利用非阻塞操作。这样，当一个 GPU 正在传输 KV 数据时，它还可以服务其他前向传递请求，而无需等待传输完成。

考虑一个到达解码工作节点的请求。在这种情况下，解码工作节点的调度器分配必要的 KV 块，并将包含这些 KV 块标识符的远程预填充请求添加到预填充队列。此交互如**图 17-4** 所示。

![图 17-4：使用 NIXL 在预填充和解码工作节点之间传输 KV 缓存数据](https://oreil.ly/GRkHs)

预填充工作节点使用 NIXL 在选定的传输层上执行直接的远程 GPU 内存读写。这避免了 CPU 拷贝并启用了非阻塞进度。一旦预填充工作节点完成预填充请求，解码工作节点的调度器就会在其自己的解码管道中添加相应的解码请求。这允许计算和数据移动无缝重叠。确保使用具有大固定窗口大小的预注册对等内存，以最小化重新注册的波动。你可以通过 Nsight Systems 时间线验证零拷贝传输的重叠。

验证端到端数据移动和重叠时，建议使用带有追踪标志的 Nsight Systems 进行分析。对于 InfiniBand 链路遥测，添加 `--nic-metrics=true` 用于 HCA/NIC 计数器，添加 `--ib-switch-metrics-device=<GUIDs>` 用于交换机计数器。这些将捕获交换机指标并采样主机/设备活动。这将产生相互关联的 CUDA 内核、UCX 活动、存储指标和网络行为。以下是一个启用 CUDA/UCX 追踪并收集 CPU 活动、GPU 指标、存储指标和 InfiniBand 交换机遥测的统一命令：

```bash
nsys profile --trace=cuda-hw,osrt,nvtx,ucx,gds \
--trace-fork-before-exec=true \
--cuda-event-trace=true \
--cuda-graph-trace=node \
--cuda-memory-usage=true \
--sample=cpu \
--gpu-metrics-device=all \
--nic-metrics=true \
--ib-switch-metrics-device=<GUIDs> \
--storage-metrics --storage-devices=all \
--gds-metrics=driver \
-o nsys_reports/prefill_decode \
<your_launch_here>
```

> 预填充和解码引擎可能使用不同的并行（例如，张量并行）布局。在这种情况下，系统可以在接收端插入一个布局转换内核——在 NIXL 读取之后（但在使用数据之前），将每个 KV 块重新对齐为解码工作节点预期的布局。

**连续批处理 (Continuous Batching)**

解码服务器严重依赖连续批处理，也称为*迭代级批处理（Iteration-level batching）*。与执行大型矩阵-矩阵计算的预填充阶段不同，解码阶段执行许多小的计算，因为每个新 Token 的生成是一个相对较小的向量-矩阵计算（单个 Token 表示为一个向量）。

为了避免小型每 Token 工作负载导致的低 GPU 利用率，解码工作节点可以将多个输入序列批处理在一起，在每次迭代中创建更大的矩阵-矩阵计算（例如，多个 Token 生成）。这增加了每个解码任务的算术强度。

例如，假设 32 个不同的文本生成请求正在进行中，并准备生成它们的下一个 Token。连续批处理调度器不会计算 32 个独立的单 Token 前向传递，而是将这些请求合并，执行一次并行生成 32 个 Token（每个序列一个）的前向传递。

这样，矩阵乘法看到的是 32 的有效批大小，这保持了 GPU 计算单元的忙碌。挑战在于并非所有序列都在完全相同的时间请求 Token。有些可能早早结束，有些可能较晚开始。

> 记住，你可以跨不同请求和用户进行批处理。大多数现代推理服务器会自动将来自不同用户请求的解码步骤分组，如果这些序列在那一刻具有相同的上下文长度。这有效地执行了即时批处理解码。考虑使用此类解决方案来最大化解码吞吐量。

连续批处理通过在每一步动态更新批处理大小来解决序列在不同时间请求 Token 的问题。具体来说，在每一步，连续批处理策略将收集所有在该时刻准备好进行下一个 Token 生成的序列，并将它们分批到一个单一的解码步骤中。

如果新请求完成了预填充并在解码进行时变得就绪，它们将加入下一个批次，而无需等待任意长的空闲期。这与静态批处理形成对比，后者在解码前会等待收集完整的批次。

如果一个序列因遇到结束标记或序列长度限制而完成，它会立即从后续批次中移除。如果一个序列尚未就绪——也许是一个仍在预填充中处理的长提示词——在它变得就绪之前不会被包含在内。

实际上，使用连续批处理，每次迭代的批处理大小可能会波动。然而，服务器总是试图最大化批处理大小，以包含当时可用的任何序列——直到达到限制。这实现了高利用率，同时最小化了每 Token 的等待时间。

连续批处理确保解码 GPU 工作节点永远不会闲置。它们总是在处理可用的请求。这通过保持 GPU 忙碌，在延迟约束下最大化了吞吐量——即使个别序列在等待新 Token。

同样，Microsoft 的 DeepSpeed 和 NVIDIA 的 TensorRT-LLM 推理引擎实现了带有分页 KV 缓存的连续或飞行批处理，以在解码期间保持高 GPU 利用率。具体来说，DeepSpeed 合并多个生成请求，TensorRT-LLM 使用调度器跨流分组解码任务。

在分离式解码集群中，连续批处理变得更加强大。由于解码 GPU 仅处理生成，它们可以将 100% 的周期用于这个连续循环，而永远不会被大型、定制的提示词任务中断。这导致了更平滑的吞吐量指标——尤其是在负载下。

在高负载下，一个解码节点可能有数十或数百个并发活跃的序列。它可以在每次迭代中批处理大量序列。这将最大化硬件利用率。

而在低负载下，即使只有一个序列活跃，解码工作节点也可以立即生成 Token。它不必等待填满一个批次。在这种情况下，GPU 在那一刻会利用率不足，但该单个序列的延迟保持在低位。因此，连续批处理兼顾了两个极端：在高并发时高效，在低并发时响应迅速。这是高吞吐量和低延迟的良好平衡。

**分组可变长度序列**

在 LLM 推理中处理可变长度序列需要仔细的调度和批处理，以避免浪费计算和内存。在一个批次中混合短和长提示词会导致填充（Padding）开销——相对于 Token 数量，有时填充高达 50%。这浪费了稀缺的 GPU 和网络资源。

当你将不同长度的提示词批处理在一起时，每个较短的序列必须填充以匹配最长的序列。这种填充引入了“空操作（no-op）”Token，这些 Token 仍然消耗 GPU 周期、内存带宽和 GPU 间或网络传输。在某些情况下，填充可能占常见生成式 AI 工作负载中所有 Token 的一半。这显著降低了推理效率。

一个直接的解决方案是根据序列长度将请求分组到桶（Buckets）中。这样，每个批次包含大小相似的序列。使用如 0–512 Tokens, 513–1,024 Tokens 等静态长度桶，固定了批次边界并最小化了填充开销。

vLLM 的解码调度器维护一个 `SequenceGroup` 实例的轮转池（每个提示词是一个 SequenceGroup）。调度器在每次解码迭代的一定 Token 预算后推进每个组。一旦一个 `SequenceGroup` 完成了其块的处理，它就离开池，一个新的 `SequenceGroup` 加入池。这保持了流水线持续充满工作——而不依赖静态填充桶或让 GPU 利用率不足。

这些批处理和调度技术与具有独立预填充和解码集群的分离式预填充-解码部署非常契合。使用这种部署配置，独立的解码节点可以使用连续批处理等技术在严格的 SLO 下最小化 TPOT 方差。同时，专用的预填充节点可以独立调优，以实现最大的输入处理吞吐量和最小的 TPOT。

NVIDIA 的程序化依赖启动（Programmatic Dependent Launch, PDL）和设备发起的 CUDA Graph Launch（在第 12 章讨论）用于减少每 Token 启动开销、重叠工作并消除解码迭代之间的气泡。这些功能通常通过框架启用，而不是在应用程序代码中手动启用。

> 在 vLLM 中，`--max-seq-len-to-capture` 控制 CUDA Graphs 覆盖的最大序列长度。默认值为 8192。在连续批处理中，vLLM 可能会填充到最近的捕获大小，因此请调整 `--max-num-seqs` 和 `--max-num-batched-tokens` 以最小化填充浪费。CUDA Graphs 有助于最小化常见序列长度的重复 CUDA 图重建。它不直接决定运行时批处理行为。vLLM 中的运行时批处理由其解码调度器的动态 SequenceGroup 池管理。在生产中，建议配合 `--max-seq-len-to-capture` 调整 `--max-num-seqs` 和 `--max-num-batched-tokens`，以限制 HBM (KV) 使用并减少连续批处理下的填充。

**KV 缓存的内存管理**

因为解码涉及关注目前为止看到的所有序列——包括之前解码的 Token——KV 缓存内存是解码工作节点的关键资源。每个序列为每个 Transformer 层——以及每个过去的 Token——存储键和值张量。**图 17-5** 展示了一个示例 KV 缓存正在跨不同请求共享。

![图 17-5：跨请求管理和重用 KV 缓存数据](https://oreil.ly/GRkHs)

对于大模型和长序列，KV 内存随 Token 线性增长，并取决于注意力布局和数据类型。一个实用的估算是 `bytes_per_token = 2 x n_layers x n_kv_heads x head_dim x bytes_per_element`。（注意：2 x 考虑了每层每个 Token 的键和值。）

考虑一个 Llama 类的 13B 模型，40 层，40 个注意力头（头维度为 128），FP16。对于标准多头注意力（MHA）的 4,096 Token 上下文，KV 大小约为 0.819 MB/Token 或总计 3.36 GB。使用 FP8 KV，则变为约 1.68 GB。

使用带有 8 个查询组（`n_kv_heads = 8`）的分组查询注意力（GQA），4,096 Token 的 KV 在 FP16 下约为 0.671 GB，在 FP8 下约为 0.336 GB。对于具有 1 个 KV 头的多查询注意力（MQA），在 FP16 下约为 0.084 GB。

服务许多并发序列的 GPU 可能纯粹由于 KV 缓存存储——除了模型权重——而迅速接近其内存限制。因此，确保你的解码工作节点使用优化的内存分配器，以防止在许多长序列飞行时耗尽 GPU 显存。以下是解码工作节点用于高效管理 KV 内存的策略：

*   **分页 GPU 内存分配器**
    vLLM 的 PagedAttention 机制是一个典型的例子——它将 KV 缓存划分为固定大小的页面，并可以将不活跃的页面交换到 CPU 内存。除了 vLLM，SGLang 和 NVIDIA TensorRT-LLM 也实现了分页 KV 内存管理器。NVIDIA Dynamo 建立在这些技术之上。这些系统还利用 DRAM 和 NVMe 分层外部 KV 层。并且它们可以调度重新计算与 I/O 以平衡带宽。这在 LMCache 等项目及其他类似库和运行时中很常见。

*   **高显存 GPU 和自定义分配器**
    解码服务器通常使用具有大 HBM 容量的 GPU（例如，Blackwell B200 的 180 GB HBM 和 Blackwell B300 的 288 GB HBM）来存储 KV 缓存。此外，像 vLLM 和 NVIDIA TensorRT-LLM 这样的系统使用优化的内存管理器，以固定大小的页面分配 KV 内存，减少碎片，支持跨请求的高效前缀重用，并管理数百个变长序列。这在不造成过度碎片和浪费的情况下高效共享内存。

*   **KV 缓存卸载（例如，分页移出）**
    当 GPU 显存填满时，解码工作节点可以将较旧的 KV 块卸载到 CPU RAM 或更冷的存储（如 NVMe）。例如，如果一个序列生成了 1,000 个 Token，但并非所有 Token 目前都立即需要，一些早期 Token 的 KV 可以移动到主机 CPU 内存。
    当需要它们时，可以按需带回 GPU 显存。卸载在需要 Token 进行注意力计算时会引入一点延迟惩罚，因此在使用卸载时应小心。解码服务器通常尝试预取或重叠数据传输，以便分页调入 KV 数据不会对生成产生太大影响。

*   **上下文限制与压缩**
    一些部署对解码 Token 的数量或输出序列长度施加硬性限制。这是一种应用层面的权衡，限制了 KV 缓存大小并避免无限制增长。KV 压缩也可以减少每个 Token 所需的 KV 内存。例如，以较低精度（FP16, FP8, INT8）存储 KV 可以大大减少内存使用。
    另一个例子是使用多查询注意力（MQA），其中头共享每个 Token 的 KV 向量。这减少了与头数成比例的 KV 大小。这是一种模型架构变更，直接降低了 KV 占用。分组查询注意力（GQA）和 DeepSeek 的多潜在注意力（MLA）也有助于减少 KV 缓存的大小。

**分离架构中的内存层级**

分离式设计的另一个优点是解码集群的 GPU 显存完全用于存储模型权重 + KV 缓存。它也不试图处理大型提示词预填充计算，而在单体服务系统上，预填充期间会暂时消耗大量额外内存。

每个解码 GPU 通常加载完整的模型权重——除非使用模型并行——然后使用剩余内存用于 KV 存储。例如，如果模型权重占用 GPU 内存的 70 GB，而 GPU 总共有 180 GB，大约 122 GB 留给 KV。

这直接影响该 GPU 上大约有多少 Token × 序列可以处于飞行状态。分离架构不能消除 KV 内存问题，但通过分离角色，你可以选择针对内存容量和内存带宽进行优化的解码节点类型。

配置好集群后，你需要决定何时将预填充卸载到预填充工作节点池——或何时在解码工作节点上本地完成。卸载有开销，包括排队延迟、网络传输等。因此，只有在确实有助于延迟时才应使用它。该决定由路由策略做出，如下所述。

## 分离式路由与调度策略

并非每个请求都需要卸载到预填充工作节点。事实上，在不必要时这样做会增加开销且收益甚微。因此，分离式推理系统使用路由策略有条件地进行分离，仅在可能有帮助时使用远程预填充路径。**表 17-1** 总结了包括 KV 感知和前缀感知路由在内的高级路由策略。

**表 17-1. 高级路由策略，包括前缀感知和 KV 感知路由**

| 路由策略                  | 描述                                 |
| :------------------------ | :----------------------------------- |
| 轮询 (Round robin)        | 逐一路由到每个节点                   |
| 最少请求 (Least requests) | 路由到活跃请求最少的工作节点         |
| 前缀感知 (Prefix aware)   | 使用请求的前缀来选择工作节点         |
| KV 感知 (KV aware)        | 路由到其 KV 缓存最匹配请求的工作节点 |

分离式路由器针对解码工作节点上收到的每个新请求运行，并快速决定是本地预填充还是在预填充工作节点池上远程预填充。

### 路由因素

将预填充卸载到预填充工作节点池的决定可以基于与请求和系统状态相关的几个因素。常见的路由因素包括当前队列长度、GPU 显存可用性，甚至专业化程度，因为某些 GPU 更适合特定模型或提示词类型。

像 vLLM 的 KV 缓存感知路由器这样的高级路由器还会考虑缓存局部性。它们会将请求路由到已经在缓存中持有部分前缀的解码工作节点。**图 17-6** 展示了一个示例 KV 缓存感知路由器如何根据从工作节点接收的 KV 缓存事件在系统中移动请求。

目标是以最大化缓存命中和平衡负载的方式进行路由。**表 17-2** 总结了在典型分离式设计中影响路由决策的一些关键因素。

**表 17-2. 影响路由器决定卸载预填充的因素**

| 因素             | 描述                                                         | 对路由决策的影响                                             |
| :--------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| 提示词长度       | 输入提示词中的 Token 数（在任何前缀缓存之后）。              | 长提示词 ⇒ 更多计算 → 如果长度超过阈值则卸载预填充。短提示词 ⇒ 本地执行。 |
| 前缀缓存命中     | 解码工作节点的 KV 缓存中已存在的提示词范围（来自先前请求）。 | 大量前缀缓存命中（大部分提示词已缓存）⇒ 预填充实际上更短且更受内存限制 → 本地执行。如果无缓存命中（全是新 Token）⇒ 计算繁重 → 卸载可能更有利。 |
| 预填充队列长度   | 全局预填充队列中待处理任务的数量（预填充工作节点有多忙）。   | 如果队列长（预填充工作节点滞后）⇒ 避免卸载新请求（本地执行）。如果队列空或轻载 ⇒ 预填充工作节点有容量 → 如果满足其他条件则卸载。 |
| 解码工作节点负载 | 本地解码工作节点的当前负载（正在进行的解码任务等）。         | 如果解码 GPU 忙于许多解码流，卸载有助于并行化（将 GPU 从繁重计算中释放出来）。如果解码大多空闲且预填充队列积压，则利用可用容量进行本地预填充。 |
| 延迟 SLO 紧急度  | 请求延迟要求的优先级或紧迫程度。                             | 紧急的低延迟要求 ⇒ 可能卸载以确保提示词尽快计算（特别是当本地解码忙碌时）。宽松的要求可能只是本地运行以节省资源（见“QoS 和早期拒绝策略”）。 |

![图 17-6：基于工作节点发出的 KV 缓存事件数据的 KV 缓存感知请求路由](https://oreil.ly/GRkHs)

这些因素将倾向于只卸载那些能从远程执行中受益的请求。这包括长且计算密集的提示词。同时，短的和缓存命中的提示词在本地处理以最小化开销。阈值是可调的。表 17-2 中的每个因素都针对特定的权衡，描述如下：

**提示词长度**
我们不想浪费时间卸载微不足道的提示词。例如，对于一个五 Token 的提示词，远程执行的开销（即使很小）也不值得，因为解码 GPU 可以自行快速处理。卸载保留给那些如果不卸载就会长时间占用解码 GPU 的繁重提示词。

**前缀缓存**
现代推理系统通常实现 KV 缓存，可以存储先前见过的上下文（如对话的早期轮次或重复的样板系统提示词）的 KV 对。如果新请求的提示词有一个长前缀与已处理的提示词重叠，解码工作节点可能已经在内存中拥有该前缀的 KV 数据。在这种情况下，它只需要为缓存未命中部分计算剩余的提示词。
如果剩余部分很短，卸载的收益就会降低。此外，如果由于完全前缀命中而缓存了整个提示词，则根本不需要预填充计算。解码可以使用缓存状态立即进行。路由器通过有效考虑 `effective_prompt_length = (prompt_length – prefix_cached_length)` 来兼顾这一点。
大的前缀命中不仅减少了所需的计算，还意味着大量的 KV 数据必须传输到预填充工作节点并传回，这是毫无意义的。因此，此类请求保留在本地并利用缓存。

**预填充队列长度**
这本质上衡量了预填充工作节点集群的负载。如果预填充工作节点被许多等待任务淹没，向它们发送另一个任务可能会损害 TTFT 而非帮助，因为请求只会坐在队列中。在这些情况下，解码工作节点被指示暂时自己做更多工作。
这为预填充集群创造了一种自然的减载机制，因为当专用预填充层达到容量时，系统会优雅地降级回本地计算。当预填充队列再次变短且工作节点赶上进度时，长提示词的卸载将恢复。这种动态平衡是分离架构在各种条件下表现良好的原因之一。

**解码工作节点负载**
虽然不总是显式编码，但路由器本质上也有助于分发工作负载。如果解码工作节点已经忙于解码许多流，它仍然可以卸载新传入的提示词。这很好，因为否则那个 GPU 将不得不并发处理繁重的计算和解码——可能会拖慢两者。
相反，如果解码 GPU 空闲（因为系统负载很轻），它们可以在本地处理更长的预填充。在实践中，空闲的解码 GPU 也可能意味着预填充队列为空，因为整体负载很低。条件已经间接涵盖了这种情况。但有些实现可能包含直接检查本地 GPU 利用率以决定是否需要预填充卸载。

**延迟 SLO 和优先级**
在具有混合 SLA 或优先级类别的系统中，可以修改路由以提高 QoS。对于必须拥有最快 TTFT 的高优先级请求，可能会绕过队列检查并立即卸载，以便它立即开始计算——即使解码 GPU 是空闲的。在这种情况下，系统可以为不同的高优先级解码任务保留该解码工作节点。
或者，如果请求优先级低，系统可能选择根本不使用预填充集群资源。相反，它只会让其在解码工作节点上本地发生——甚至延迟它。我们在“QoS 和早期拒绝策略”中会重新审视优先级请求处理，但要知道基本路由器可以通过这些类型的考量进行扩展。

### 代码中的动态路由策略示例

在实践中，路由策略可以实现为一个简单的条件检查。这里的代码展示了这个路由逻辑的简化版本：

```python
# 运行在解码工作节点上的卸载预填充决策
# (针对 B200/B300 调优)
def should_offload_prefill(prompt_length: int,
                           prefix_cached_length: int,
                           prefill_queue_size: int,
                           decode_active_reqs: int,
                           ttft_slo_ms: int = 500) -> bool:
    
    # 前缀 KV 命中后的有效预填充
    eff_len = max(0, prompt_length - prefix_cached_length)

    # 可调参数 (保存在配置中; 为清晰起见在此显示)
    PREFILL_LENGTH_THRESHOLD = 256 # 见 split_policy.prompt_length_threshold
    PREFILL_QUEUE_MAX = 10         # 见自动伸缩/预填充队列长度指南
    DECODE_LOAD_THRESHOLD = 8      # 活跃解码流

    long_prefill = (eff_len >= PREFILL_LENGTH_THRESHOLD)
    prefill_available = (prefill_queue_size < PREFILL_QUEUE_MAX)

    # 当预填充计算繁重且池有容量时首选远程
    if long_prefill and prefill_available:
        return True

    # 如果本地解码忙碌且预填充中等长度，通过卸载释放解码
    if decode_active_reqs >= DECODE_LOAD_THRESHOLD and eff_len >= 64:
        return True

    # 否则保持预填充在本地
    # (更低开销 / 更好缓存局部性)
    return False
```

在这个伪代码中，`PREFILL_LENGTH_THRESHOLD` 是一个系统调优参数，如 50 或 100 Token，定义了什么算作“长”提示词。`PREFILL_QUEUE_MAX` 是一个阈值，超过该阈值预填充工作节点池被视为饱和，特别是如果有太多未完成的任务。

解码工作节点一收到新请求就调用 `should_offload_prefill()`。如果函数返回 `True`，解码工作节点会将提示词打包成消息并推送到全局预填充任务队列。然后它在等待 KV 缓存结果返回的同时执行其他工作。

如果 `should_offload_prefill()` 返回 `False`，解码工作节点立即自行执行预填充计算。这样，如果预填充工作节点开始滞后，新请求将回退到本地计算以避免排队延迟。这是一种平衡解码和预填充池之间负载的自适应路由形式。

### 动态路由策略配置示例

在生产部署中，路由策略应通过文件或 UI 配置，而不是硬编码。例如，NVIDIA 的 Dynamo 允许在 JSON 或 YAML 配置中指定复杂的路由和自动伸缩规则。这是一个封装了一些策略逻辑的 Dynamo Planner JSON 简化示例：

```yaml
model: ...
split_policy:
  prompt_length_threshold: 256
  prefix_cache_weight: 10.0
  queue_length_weight: 1.5
  decode_load_weight: 0.5
  enable_hotspot_prevention: true
cache:
  reuse_prefix: true
  min_cache_hit_ratio: 0.75
autoscale:
  prefill:
    min_replicas: 4
    max_replicas: 12
    scale_up: { queue_length: 8, gpu_utilization: 80 }
    scale_down: { queue_length: 2, gpu_utilization: 40 }
  decode:
    min_replicas: 8
    max_replicas: 24
    scale_up: { queue_length: 16, kv_cache_usage: 75 }
    scale_down: { queue_length: 4, kv_cache_usage: 30 }
qos:
  enable_early_rejection: true
  low_priority_threshold_ms: 500
  reject_on_slo_violation: true
```

在这里，配置定义了一个 `split_policy`，其中 `prompt_length_threshold` 为 256 Token。它还指定了缓存命中、队列长度、解码负载等因素的权重。它还配置了预填充和解码角色的自动伸缩行为，包括如何根据队列长度、GPU 利用率和 KV 缓存使用情况进行扩缩容。

此外，它可以应用一些 QoS 规则，如早期拒绝，并调整将请求视为“慢”的阈值。在实践中，Dynamo 的路由器在启动时读取此 JSON——或动态获取——以指示整个集群中请求的每个路由决策。

### 容量感知路由

如上一节所述，NVIDIA Dynamo 支持动态路由策略。这种动态路由能力的一个实现是 Dynamo GPU Planner。Planner 使用 TTFT、TPOT 等指标以及 KV 缓存传输的估计成本来决定修改路由——甚至重新分配/扩展特定阶段的 GPU——以减少瓶颈并适应工作负载的变化。这使得系统能够在需求激增期间保持高性能，如**图 17-7** 所示。

![图 17-7：NVIDIA Dynamo GPU Planner 根据 GPU 利用率指标决定如何处理传入请求并将 GPU 工作节点分配给预填充和解码](https://oreil.ly/GRkHs)

在这里，Dynamo 的 Planner 选择将更多的 GPU 转移到预填充（上下文）阶段，因为大量摘要提示词涌入，相对于解码（小摘要输出），这需要大量的预填充（大输入）。

相反，如果推理请求涌入，Planner 可以选择重新分配 GPU 到解码阶段，因为推理请求相对于输入 Token 数生成大量输出 Token。在其他情况下，Planner 可能会选择以传统的单体方式处理请求，即预填充和解码在同一个 GPU 工作节点上发生。

简而言之，像 NVIDIA Dynamo Planner 这样的组件可以通过持续监控实时指标并将其与应用级 SLO（如 TTFT 和 TPOT 延迟）进行比较，从而自动化这一决策过程。利用这些信息，Dynamo Planner 可以动态确定是以完全分离、不分离还是介于两者之间（为每个预填充/解码阶段分配更多或更少的 GPU 资源）的方式服务请求。由此产生的自适应系统优化了预填充和解码工作负载的资源利用率——并满足激进的性能目标。

### 延迟感知路由

推理路由器可以超越简单的阈值规则，使用更复杂的延迟评分模型来为每个请求挑选最佳工作节点。例如，它可以基于实时指标（如忙碌程度、内存使用量、是否有相关缓存等）持续计算每个潜在工作节点的延迟成本。然后将请求发送到具有最低延迟成本的工作节点。

假设具有最低延迟成本的工作节点是最空闲和最可取的。一个简单的延迟成本函数可能如下所示：

```python
# 成本越低越好
latency_cost = 0.7 * (occupancy_percent) + 0.3 * (active_req_count)
```

这个特定的延迟成本函数更重地权衡 GPU 的占用率。这与 GPU 当前正在做多少工作相关。其次，它与该引擎上当前有多少请求在飞行中相关。新请求将被发送到具有最低延迟成本的引擎。

权重（此例中的 0.7 和 0.3）可以根据经验数据进行调整。如果发现 KV 内存使用率是预测由于大量数据交换或更高内存带宽使用导致变慢的重要指标，你会想要更重地权衡它。

更高级的路由策略可能包括其他因素。例如，你可以将以下一些因素纳入你的延迟成本函数：

*   **精确缓存匹配可用性**
    如果工作节点在其缓存中已经有了所需的预填充 KV（前缀命中），那么它可以更快地服务请求。在这种情况下，系统可以分配一个大的负权重来降低延迟成本并偏好此工作节点，因为在这种情况下越低越好。

*   **KV 占用率**
    更高的内存使用率意味着 GPU 更忙。在这种情况下，系统分配一个正权重来增加延迟成本，并鼓励路由器避开此工作节点，因为在这种情况下越低越可取。

*   **活跃请求**
    更多的并行请求意味着潜在的上下文切换开销，因此这增加了延迟成本，从而避开此工作节点。

*   **内存带宽利用率**
    如果 GPU 当前因为处理许多长序列而使用了大量内存带宽，添加更多工作只会让事情变慢。这将增加延迟成本并阻止系统选择此工作节点。

*   **最近 KV 使用**
    如果确切的前缀最近在某个工作引擎上使用过——并且缓存是“热”的——这可能会提高性能，因为 KV 仍在 L2 缓存中或目前正在从预填充工作节点传输过来。在这种情况下，你可能包含一个小的负权重来降低延迟成本并偏好此工作节点，因为它最近看到了该前缀。

**表 17-3** 总结了这种类型的高级路由策略配置。它列出了一些因素和示例成本。

**表 17-3. 路由因素及相对成本**

| 因素          | 解释                     | 对延迟成本的影响 |
| :------------ | :----------------------- | :--------------- |
| KV 占用率 (%) | 越高 = 内存压力越大      | +3               |
| 活跃请求计数  | 更多在飞行中 = 潜在排队  | +1               |
| KV 缓存匹配   | 引擎已有所需前缀 KV      | –10 (大负值)     |
| 内存带宽 %    | 高 = 内存总线忙碌        | +0.5             |
| 最近 KV 使用  | 前缀最近在此引擎上使用过 | –1               |

让我们用这些额外指标修改延迟成本计算。路由器现在计算一个综合延迟成本，如下所示：

```python
# 成本越低越好
def latency_cost(occupancy_percent: float,
                 active_reqs: int,
                 cache_match_flag: bool,
                 mem_bw_percent: float,
                 recent_prefix_flag: bool) -> float:
    return (
        3.0 * occupancy_percent
        + 1.0 * active_reqs
        - 10.0 * int(cache_match_flag)
        + 0.5 * mem_bw_percent
        - 1.0 * int(recent_prefix_flag)
    )
```

预填充和解码集群甚至可能使用略有不同的公式。对于预填充，缓存命中（例如，前缀已计算）可能更有价值，而对于解码，内存带宽或可用 KV 空间可能主导决策。

路由器需要来自每个工作节点的遥测数据。这包括等待队列长度、KV 缓存使用量、内存利用率等指标，以便可以持续更新延迟成本。

结果是系统可以动态地将流量路由到对延迟影响最小的地方。在低负载下，所有工作节点的得分都很低，选择哪一个可能并不重要，因为任何工作节点都可以快速处理请求。

然而，在高负载下，路由器通过将新工作发送到最空闲的工作节点来进行有效的负载均衡。这也将利用数据局部性（缓存命中）来减少工作。这降低了平均延迟和尾部延迟，因为当有空闲引擎时，工作不太可能堆积在繁忙引擎后面。

早些时候，我们看到了考虑缓存命中如何导致路由器选择稍忙的服务器（如果它缓存了相关数据）。这导致对该请求的服务更快。评分函数定量地捕获了这种权衡。

NVIDIA Dynamo 执行类似但相反的计算，其中越高越好。具体来说，它计算一个远程预填充得分，如果得分超过阈值，它将卸载到预填充工作节点池。这是一个示例 YAML 片段，如果计算出的得分高于可配置的 `remote_prefill_min_score` 值，则配置系统使用有条件的分离：

```yaml
disaggregated_router:
  enable: true
  policies:
    - metric: prefill_length      # 前缀缓存命中后的提示词长度
      threshold: 256
      action: prefer_remote       # 如果提示词 > 256 tokens，卸载预填充
      weight: 0.7
    - metric: prefill_queue_depth # 预填充工作节点的排队请求
      threshold: 10
      action: prefer_local        # 如果队列 > 10，倾向于本地预填充
      weight: 0.3
  remote_prefill_min_score: 0.5   # 决定远程预填充的总体阈值
```

在这里，路由器根据 `prefill_length` 和 `prefill_queue_depth` 计算得分。在这种情况下，如果提示词长于 256 Token，它投票给 `prefill_remote` 并卸载预填充。这就配置所示，评分计算的这一部分权重为 0.7。

但如果预填充队列非常深，有超过 10 个等待任务，这就配置所示，它将以 0.3 的权重投票给 `prefill_local`。如果综合得分大于 `remote_prefill_min_score`（本例中为 0.5），Dynamo 将卸载预填充。否则，它将预填充保留在本地，不进行卸载。

### 多路径推理 (Racing)

多路径推理，即向两个不同大小的模型或两条不同的路由发送相同的请求，用于高可靠性。这本质上是在竞速以获得最快的结果，通常被称为 **Racing**。众所周知，Google 和 Meta 的生产系统通过竞速模型来减少尾部延迟。这是一种昂贵但有效的技术。

> 如果你自己实施多路径推理，请确保你的请求是幂等的，并且如果两条路径都执行不会引起问题。并确保及时取消较慢的路径以节省 GPU 周期。

### 跨工作节点的多分支、并行推测解码

如第 15 章所述，一些高级推理服务器支持推测解码。请记住，推测解码并行生成多个 Token 分支，丢弃那些不是最理想的分支。

虽然不是分离架构的直接部分，但推测解码可以叠加在路由器的决策过程之上。例如，系统可以检测不确定的推测解码分支，并将多个推测请求并行发送到不同的解码工作节点。这将生成多个推测性的下一 Token 分支，并掩盖 Token 分支生成中的不可预测性。这以额外的计算为代价，换取了不可预测分支生成的更低延迟。

如果实施，路由器将协调这些推测努力，然后合并结果。资源使用需要受到限制以避免压垮集群，但多路径推理值得注意，它是超延迟感知推理的一种潜在优化，代价是额外的、可能浪费的计算和显存带宽集群资源。

总之，延迟感知路由器确保每个请求被发送到最佳位置，同时考虑当前负载和潜在的加速因素（如缓存重用）。它充当分布式服务系统的“大脑”，与每个 GPU 上的底层调度协同工作。结合全局扩展和 KV 缓存策略，它形成了一种最大化有效吞吐量并最小化延迟的综合方法。

## QoS 与早期拒绝策略

QoS 策略对特定请求进行优先排序或节流，以满足延迟目标。早期拒绝，也称为**准入控制（Admission Control）**，可以在系统饱和时拒绝低优先级的查询。这样，它保留了资源和 SLO，例如其他请求的延迟。

> 现代系统基于队列时间阈值来执行此操作。例如，如果请求在队列中等待 > X 毫秒，拒绝它或将其卸载到较低层级的服务可能比让它严重违反 SLO 更好。这种 QoS 保护在任务关键型推理系统中越来越常见。

在超大规模推理系统中，需要 QoS 机制来在高负载下维持尾部延迟保证。即使有了优化的分离式集群配置，如果负载超过容量，请求也会开始排队，延迟也会上升。

与其允许所有请求都违反延迟 SLO，设计良好的系统更愿意优雅地卸载。它可以通过拒绝或推迟某些请求（特别是低优先级的请求）来实现，当很明显服务它们会破坏其他请求的延迟保证时。

这类似于 Web 服务器在极端负载下返回 HTTP 503“过载”错误。对于 LLM 服务，如果我们不能保证按时服务，我们可能会主动拒绝或降级请求。以下是预填充/解码分离背景下 QoS 的几个组成部分：

*   **延迟 SLO 跟踪**
    系统应了解目标 TTFT 和 TPOT 目标。例如，99% 的时间首个 Token 必须在 200 毫秒内返回。利用内部遥测，每个解码工作节点可以估计如果现在接受一个新请求，其当前的 TTFT 会是多少，并基于当前队列长度等进行估算。如果添加另一个解码流，它同样可以估计 TPOT。

*   **准入控制（早期拒绝）**
    在请求被完全接受和分配之前，系统可以执行检查，看看准入此请求是否会使系统过载或违反 SLO。如果是，它可以立即拒绝请求，并返回“服务器忙，稍后重试”类型的响应。这被称为**早期拒绝**。在实践中，你可以使用系统负载的全局视图或仅仅是单个节点的启发式方法来触发早期拒绝。
    例如，OpenAI 的公共 API 如果积压过多会返回错误。它这样做是为了不违反延迟承诺。一些提供商在峰值负载期间动态降低最大生成长度。这有效地用答案质量换取延迟。

*   **优先级排序**
    并非所有请求都是平等的。例如，来自付费客户或关键服务的请求可能是高优先级的。来自免费层用户或后台作业的请求可能较低。系统可以将优先级纳入调度决策。
    例如，解码工作节点的调度器可以首先服务高优先级的解码任务。或者预填充任务队列可以按优先级排序。如果变得繁忙，低优先级任务可能会等待更长时间，或者为了高优先级任务而被快速失败。

*   **优雅降级**
    如果系统开始过载，与其直接拒绝请求，它可以降低次要请求的服务等级。例如，对于不太重要的请求，它可以使用较小的模型，截断其提示词，或限制输出 Token 的数量。一种简单的方法是在负载高时，暂时减少较低层级请求的最大允许提示词长度或输出长度。

在分离架构的背景下，一种有趣的优雅降级形式是在系统受压时暂时禁用低优先级请求的远程预填充。由于远程预填充使用额外的集群资源来优化延迟，我们可以决定低优先级查询应该只进行本地预填充，仅使用解码工作节点资源。这会对这些请求产生较慢的响应，但它释放了预填充工作节点用于高优先级查询。

例如，推理服务器可以用优先级标记每个请求，并让路由器在预填充工作节点忙于高优先级任务时，忽略低优先级请求的 `should_offload_prefill` 逻辑。

这是一个早期拒绝策略的示例。这可以在处理请求之前的每个解码工作节点上运行，甚至可以在 LLM 网关的上游运行：

```python
# 基于估计延迟和优先级的早期拒绝
from dataclasses import dataclass

class QoSController:
    def __init__(self, ttft_slo_ms: int = 500):
        self.ttft_slo_ms = ttft_slo_ms

    def admit_request(self, priority: str) -> bool:
        # 队列长度和每请求平均值由指标系统提供
        est_ttft = (get_current_prefill_queue_length()
                    * get_avg_prefill_time_per_req()
                    + get_current_decode_queue_length()
                    * get_avg_decode_time_per_req())

        if est_ttft > self.ttft_slo_ms and priority.lower() == "low":
            # 拒绝低优先级请求以保护 SLO
            return False
        return True
```

在这里，我们通过查看有多少预填充任务排队（因为每个都会增加一些延迟）以及前面有多少解码任务来估计 TTFT。注意解码任务通常是重叠的，所以这是一个粗略的估计。如果估计的 TTFT 超过允许的最大值（SLO），那么对于低优先级请求我们将其拒绝，并从 `should_offload_prefill` 函数返回 `False`。

对于高优先级请求，我们仍然接受它，并在必要时可能牺牲一些其他排队的工作。一种更复杂的方法是抢占已经在排队的低优先级请求，为新的高优先级请求腾出空间。这通常通过维护每个优先级级别的独立队列来实现。

深入一点，在前面的例子中，`avg_prefill_time_per_req()` 和 `avg_decode_time_per_req()` 函数使用观察到的每请求预填充和解码持续时间的指数移动平均值即时计算这些值。它们按提示词 Token 数（预填充）和生成 Token 数（解码）进行归一化。对于不同长度的输入，引擎通过将这些每 Token 平均值乘以每个请求的实际 Token 计数来进行推断。

`get_current_prefill_queue_length()` 和 `get_current_decode_queue_length()` 函数检索调度器内部队列跟踪的待处理预填充和解码任务的数量。这些值由调度器维护。

这些每 Token 计时估计值使用调度器循环实时刷新，默认情况下，该循环每秒通过 `/metrics` 端点更新一次。这捕获了动态工作负载的变化。

早期拒绝和优先级排序确保当系统接近饱和时，它以受控的方式失败而不是崩溃。具有最重要请求的用户继续在提供的 SLO 内得到服务。与此同时，不太重要的流量被剥离。

在许多生产部署中，实施这一点需要与 LLM 网关层协调。具体来说，网关可能会返回特定的错误代码或将指示服务器过载的代码返回给客户端。关键在于系统保持自身处于一种状态，即它能够满足其接受工作的延迟承诺，而不是超额订阅并让所有人错过 SLO。

另一个 QoS 考量是**自适应生成限制**。如果解码阶段有可能运行太久并导致 TPOT SLO 违规，系统可能会提前切断生成。例如，如果用户请求 1,000 个 Token，但系统压力很大，它可能只允许生成 200 个 Token 然后停止。这为其他请求留出了资源。QoS 策略优先考虑或节流某些请求以满足延迟目标；早期拒绝可以在系统饱和时拒绝低优先级查询，以保护其他人的 SLA。

简而言之，分离架构减少了固有干扰，但负载激增仍可能压垮任何固定容量。QoS 机制通过确保系统兑现其延迟承诺来补充分离架构。早期拒绝将拒绝或降低某些工作的优先级，以保护其余请求的延迟。

在构建超大规模系统时，这些策略与核心性能优化一样重要。没有它们，大量的请求可能会通过造成巨大的队列和减速来破坏所有提高性能的尝试。

## 分离式预填充与解码的可扩展性

可扩展性的另一个方面是随着节点增加性能如何保持。分离式方法在设计上在许多方面相对线性地扩展。你可以添加更多预填充节点来处理更多提示词吞吐量。你可以添加更多解码节点来处理更多 Token 生成吞吐量。

主要的挑战是平衡它们。随着规模增加，自适应调度器变得更加重要。这是因为在大型系统中，不平衡（例如流量模式转移）的几率要高得多。动态比率配置允许集群即时重新平衡预填充和解码的比率。

另一个考量是多模型服务。如果模型的解码特性相似，分离架构允许你在模型之间共享解码容量。例如，如果模型 A 和模型 B 托管在同一集群上，你可以指定一些解码工作节点来处理这两种类型的模型。这在托管复用相同基础模型架构的不同 LoRA 适配器时特别有用。

这将给你一个超级灵活、多租户、多模型、预填充-解码分离的推理服务器系统。这超出了我们的范围，但要知道分离架构的模块化为你提供了这种类型的灵活性。你可以拥有一个用于多个模型的公共解码池，每个模型都有其自己的预填充前端。

最后，让我们分析超大型系统中的尾部延迟。随着超大规模推理系统扩展到成千上万甚至数百万个 GPU 节点，控制尾部延迟变得越来越难。这是因为随着服务器增多，其中一台发生故障的几率要高得多。

分离架构在这里也有帮助。隔离一个节点上的预填充任务和另一个节点上的解码任务意味着一侧的慢节点不会急剧影响另一侧的任务。

本章讨论的技术，如早期拒绝、两级调度和 KV 缓存重用，有助于缓解落后者（Stragglers）。例如，缓存请求上下文的大部分内容意味着一个缓慢的操作不会让整个生成过程慢太多——因为很多计算已经完成了。

总之，如果配置和调优得当，分离式系统在规模化时更加健壮。即使并发请求增长到数百万和数十亿，这些系统也能保持一致的延迟。通过移除一个干扰因素并增加适应性，尾部延迟分布得到改善——或者至少，不会随负载快速恶化。

## 本章要点

本章涵盖的技术，从高速 RDMA 传输 KV 缓存到动态路由算法和 QoS 策略，已被证明是大规模生产推理工作负载的基本组件。以下是关键要点：

*   **消除预填充-解码干扰以改善延迟**
    分离预填充和解码阶段消除了干扰和队头阻塞。这产生了更紧凑的延迟分布，因为长提示词不会延迟短提示词。这也让每个阶段都能可靠地满足其延迟 SLO。

*   **独立优化每个阶段**
    预填充（提示词处理）是计算受限的，受益于最大并行度和高 FLOPS。它偏好高算力 GPU 和 FP8/FP4 降精度等技术来最小化 TTFT。
    解码（Token 生成）是内存受限的，受益于高内存带宽 GPU 和连续批处理等技术来最大化每 Token 吞吐量。分离架构允许为每个阶段设置不同的硬件和并行设置。固定的、“一刀切”的配置是不允许这样做的。

*   **利用 KV 缓存**
    使用 KV 缓存来避免重新计算提示词预填充是很常见的。智能路由器可以决定新请求应该卸载到预填充工作节点还是直接在解码工作节点上处理。路由策略应考虑提示词长度、缓存命中和集群负载。

*   **智能路由**
    现代推理服务器如 vLLM、SGLang 和 NVIDIA Dynamo 使用多因素评分算法来路由预填充和解码请求，以最大化吞吐量并维持足够的延迟。
    许多框架还在调度器和仪表板中将 TPOT 称为*Token 间延迟（Inter-token latency, ITL）*。建议分别跟踪预填充和解码节点的 TTFT (p50/p95/p99) 和 ITL/TPOT (p50/p95/p99)。这样，例如在升级期间更容易调试性能回退。例如，如果预填充服务器饱和或提示词很短，它们将跳过远程预填充。

*   **使用 QoS 在规模化时维持 SLA**
    在高负载下应用准入控制和优先级排序。最好是快速拒绝过量或低优先级的请求，而不是让系统的延迟对所有人都爆炸。现实世界的系统通过返回“忙”错误或降级的响应长度来实现早期拒绝——以保证第 99 百分位延迟保持在给定阈值之下。分离架构和 QoS 一起防止了过载级联——即使在每天数十亿次请求的场景中也是如此。

## 结论

分离式预填充与解码在现代大规模 LLM 推理中已变得普遍。几乎所有主要的 AI 提供商，如 Meta、Amazon、NVIDIA、Google 和 OpenAI（或“MANGO”），都在其大规模 LLM 部署中采用了某种形式的分离架构。

虽然这些公司大多数不公开其完整的生产架构，但有许多开源实现（例如 vLLM, SGLang, NVIDIA Dynamo）展示了这种方法及其好处。分离式 PD（Prefill-Decode）比单体设计产生了更高的有效吞吐量（Goodput），即延迟约束下的有用吞吐量，以及更好的成本效率。

在下一章中，我们将继续结合现代 GPU 和网络硬件与更高级的调度和缓存优化，以便在不牺牲延迟或让成本爆炸的情况下，为数十亿用户提供数万亿参数模型的 LLM 服务。







# 第18章 高级预填充-解码与KV缓存调优

本章承接第17章，深入探讨推理预填充和解码阶段的高级优化。我们将建立在高层扩展策略的基础之上，涵盖底层技术，包括单解码“巨型内核（mega kernels）”、跨GPU的智能KV缓存调优与共享、提示词状态的GPU间快速传输、自适应资源调度，以及预填充与解码工作节点之间的动态路由。

我们还将重点介绍硬件和软件层面的创新，它们将性能和效率提升到了新的高度。应用这些技术，你可以显著降低解码延迟，提高单GPU吞吐量，并在大规模环境下满足严格的延迟服务等级目标（SLO）。

## 优化的解码内核

此前，我们将重点放在了高层系统和集群优化策略上。而在提升超大规模推理能力时，另一套必须考虑的技术是底层内核与内存管理的调优——尤其是针对解码阶段。

解码阶段是分布式的，且往往受限于内存带宽。这促使研究人员和从业者尽可能地加快解码阶段的速度，并针对特定硬件进行调优。该领域有两个值得注意的创新：FlashMLA（DeepSeek）、ThunderMLA（Stanford）和FlexDecoding（PyTorch）。它们专门针对LLM工作负载中常见的变长序列场景，优化了解码期间Transformer多头注意力的效率。接下来我们将逐一介绍。

### FlashMLA (DeepSeek)

Flash多潜在注意力（Flash Multi-Latent Attention，简称FlashMLA）是由DeepSeek推出的优化解码内核。它专门聚焦于单词元（single-token）解码步骤，本质上就是用于生成下一个词元的Transformer层的前向传播。FlashMLA通过融合操作和更好地利用GPU内存层级，使解码速度更快。

FlashMLA（解码）之于推理，正如FlashAttention（预填充）之于训练。它减少了内存访问开销和延迟。相较于标准内核，FlashMLA可以大幅降低解码阶段的延迟。

FlashMLA通过将多个注意力操作融合为一个来增加算术强度。这样，它可以在一次融合内核启动中处理多个头和多个时间步。这通过让运算单元在小批次大小下依然保持忙碌，提高了解码期间的GPU利用率。**图 18-1** 展示了在Hopper H100 GPU上，MLA与分组查询注意力（GQA）和多查询注意力（MQA）等其他注意力实现相比，在算术强度上的提升。（注：Blackwell架构凭借更高的TFLOPS和HBM带宽，将两条屋顶线（rooflines）都向上提升了。）

*(图 18-1：MLA 逼近计算受限区域，测量于 NVIDIA Hopper H100 架构)*

FlashMLA的推出意义重大，因为它表明解码阶段的瓶颈——内存带宽和内核启动开销——是可以被降低的，即便是在次优的GPU硬件上。它减少了独立的GPU内核启动次数，优化了内存访问模式，从而在受限硬件上尽可能地榨取解码任务的性能。

DeepSeek开源的FlashMLA实现已可获取并正被采用。SGLang和vLLM都为DeepSeek模型提供了一等支持。因此，你应该评估使用FlashMLA来提高每词元解码吞吐量，而无需更改更上层的架构。

> 鉴于DeepSeek开源的FlashMLA已集成到现代推理服务系统中，你应该探索利用它来增加每个解码工作节点的吞吐量——或降低每词元延迟——且无需进行更高层级的架构变更。

### ThunderMLA (Stanford)

在FlashMLA的基础上，斯坦福大学的研究人员推出了ThunderMLA，这是一个完全融合的注意力解码“巨型内核（megakernel）”，专注于解码和调度（而非融合整个前馈模块）。这个“巨型内核”通过将多个内核启动合并为一个，以及整合中间内存写入，减少了启动开销和长尾效应。ThunderMLA报告称，在不同工作负载下，其解码吞吐量比FlashMLA快20%–35%。

ThunderMLA的核心理念在于，当解码不同长度的序列时，使用细粒度调度和融合操作可以避免因某些序列较早完成而其他序列导致GPU部分空闲的“长尾效应”。ThunderMLA利用其融合方法，通过动态打包和处理剩余流，即使部分解码流已完成，也能保持GPU忙碌。

这些优势在拥有更大L2缓存和更快注意力原语的现代GPU上被进一步放大。值得注意的是，现代NVIDIA GPU还通过Transformer Engine提供FP8和FP4支持（以及FP6，尽管本文主要关注FP8/FP4格式，因为FP6在现有AI框架和工具中尚未广泛使用）。结合更高的内存带宽，Tensor Cores让像ThunderMLA这样的内核能够运作在更接近硬件极限的水平。得益于这些架构进步，ThunderMLA在现代GPU上实现了更低的每词元延迟。

### FlexDecoding (PyTorch)

在第14章中，我们讨论了PyTorch的FlexAttention，它允许你为任意稀疏模式（包括局部窗口、块稀疏模式等）JIT编译融合内核，且无需编写自定义CUDA代码。在底层，TorchInductor + OpenAI Triton会生成一个仅计算允许的查询-键值对的融合内核。Triton会自动应用性能优化技术，如在特定硬件有益时使用Warp特化（warp specialization）和异步拷贝。当然，你也可以通过配置 `triton.Config`（例如 `num_consumer_groups`）来进一步定制。

FlexDecoding是 `torch.nn.attention.flex_attention` 的解码后端。FlexDecoding同样允许你原地（in place）管理KV，并像FlexAttention一样支持掩码和偏置。具体来说，FlexDecoding编译了一个专门针对解码阶段（Q_len=1）的内核，用于关注不断增长的KV缓存。

在运行时，FlexDecoding实现会选择专用的解码内核，并在多个解码步骤中重用它。这有助于在形状（shapes）和数据类型（dtypes）保持兼容时最小化开销——极大地加速了长序列LLM推理。

> 一旦重新编译得到控制，建议优先使用 `torch.compile(mode="max-autotune")` 来获得稳定、延迟关键的解码性能。保持捕获边界狭窄（每层或注意力块），以减少因参差批处理（ragged batching）导致的图失效。
> 优先使用Transformer Engine FP8 (MXFP8) 进行预填充和解码。当精度允许且能带来性能提升时，考虑使用FP4 (NVFP4)。截至本文撰写时，FP4支持仍在成熟中，近期表现可能不及8-bit和16-bit格式。
> 继续设置 `torch.set_float32_matmul_precision("high")` 以在剩余FP32运算上启用TF32回退。FlexAttention的解码后端支持常见的性能增强，包括分组查询注意力（GQA）和PagedAttention。

FlexAttention和FlexDecoding的一个关键特性是支持**嵌套锯齿布局张量（Nested Jagged Layout Tensors, NJT）**。这允许在解码期间对变长序列（LLM工作负载中很常见）进行参差批处理（ragged batching）。各种序列的锯齿张量表示如**图 18-2** 所示。

*(图 18-2：作为嵌套锯齿张量（偏移量）的参差批次；顶部是三个序列，底部是带有偏移量的单个嵌套锯齿张量表示；解码时批处理首选PyTorch NJT)*

此外，FlexDecoding支持偏置项，并通过块掩码转换接口与PagedAttention集成，该接口将逻辑块映射到物理缓存布局。这会将逻辑KV块分散到物理缓存布局中——而无需创建额外的副本，如**图 18-3** 所示。

FlexDecoding利用捕获的张量在每次迭代中改变特定的掩码或偏置值——而无需重新编译。并且它与PagedAttention集成。要使用如vLLM LMCache这样的全局KV缓存，需将缓存的页表映射到FlexAttention的BlockMask。这将在运行时将逻辑KV页面转换为物理内存地址。

使用FlexDecoding，开发者拥有Python级别的完全灵活性来定制注意力稀疏模式。这对于MoE模型推理特别有用。FlexDecoding让你无需编写任何自定义CUDA内核即可实现近乎最佳的性能。本质上，它允许任意注意力模式像密集注意力模式一样被优化。随着新推理技术的出现，这一点变得愈发珍贵。

*(图 18-3：PagedAttention将逻辑KV块分散到物理KV块中，以实现序列间的最佳缓存重用；块大小应与LMCache页面大小对齐——较大的页面（如64–128词元）可减少分离设置中的RDMA开销)*

许多这类能力，如用于解码的融合注意力以及对PyTorch嵌套锯齿张量（NJT）批处理的支持，都在核心PyTorch库中可用。这使得对于典型模式而言，自定义融合变得不再那么必要。

> 在批处理LLM工作负载中常见的参差序列时，优先使用NJT布局。

这些内核级的进步技术性很强，充分利用了GPU、网络和内存的能力。这些软件优化即使在相同的硬件上也能显著提高解码性能。在设计超大规模系统时，如果可能，应整合这些优化内核。务必使用Nsight Systems配合基于硬件的CUDA追踪来验证重叠和内核效率。此外，使用Nsight Compute获取具体的内存和链路指标。

> 启用某些高级内核可能需要安装自定义库或启用专用CUDA内核——特别是对于较新的技术。然而，这些技术通常很快就会被PyTorch和流行的推理引擎支持。即使你需要安装自定义资源，这项工作也是值得的，因为它将直接转化为更低的延迟和解码工作节点池中更少的GPU需求。

## 调优KV缓存利用率与管理

分离架构要求你将KV缓存视为集群内的一等共享资源。由于KV缓存现在的生命周期更长且可以在节点间移动，高性能推理系统改进了KV缓存的存储和共享方式。

特别是分布式KV缓存池和跨请求的前缀重用成为了强有力的技术。此外，关注新一代GPU和HBM在内存带宽上的提升也很重要。让我们在提升KV缓存性能的背景下讨论这些内容。

### 分离式KV缓存池

分离式KV缓存池不再让每个GPU仅存储其当前服务请求的KV，而是将KV存储从单个GPU中解耦。相反，它将数据分散在整个集群的GPU内存中。

该池还可以卸载到CPU内存（包括Grace Blackwell和Vera Rubin平台的统一CPU和GPU内存），甚至卸载到NVMe SSD等持久化存储。

使用分离式KV缓存池时，当预填充计算出提示词的KV张量——或解码扩展KV张量时——KV块以分布式方式存储在多个计算节点上。这如**图 18-4** 所示，改编自关于分离式KV池的研究工作。

*(图 18-4：分离式（分布式）KV缓存池)*

考虑一个非常长的250,000词元上下文（例如，多轮聊天会话），使用一个700亿参数的Transformer模型，80层，32个头，每个头维度为128。这会产生巨大的每词元KV缓存占用。

每个词元生成一个键和值向量，长度等于模型的隐藏层维度（`num_heads × head_dim`）。假设我们的模型是4,096。这导致每层有8,192个浮点数。汇总80层，每个词元产生655,360个浮点数的KV数据。假设是16位精度，即每个浮点数2字节。这大约是每个词元需要1.31 MB。将其扩展到250,000个词元，产生大约 **328 GB**——仅仅是KV数据！

> 这些计算基于大模型的每词元KV大小。这显示了为什么FP8 KV缓存被vLLM等引擎广泛采用，以减少占用并增加批处理机会。

假设我们将KV缓存量化到FP8并使用选择性层缓存等技术，我们可以将这250,000词元提示词的占用减少到100–150 GB范围。单个GPU很可能无法容纳所有词元的KV以及它需要持有的其他东西（如模型权重等）——尤其是随着多轮对话的继续。因此，系统将不得不截断上下文——或导致早期词元的昂贵KV重新计算。

然而，有了分离式KV池，上下文较旧部分的KV可以从GPU驱逐并推送到分布在集群中的KV缓存池——以及CPU DRAM或NVMe存储中。当需要时，数据再被取回GPU内存。

分离式KV缓存池实现了一个多级内存层级结构，其中GPU设备内存持有活跃的KV缓存，而CPU主机RAM（或NVMe存储）充当溢出后备存储。现代推理引擎可以选择将KV缓存卸载到CPU内存或NVMe。这有效地虚拟化了GPU内存，就像操作系统的虚拟内存子系统一样。

这种设计允许通过在GPU内存和KV缓存池之间异步分页KV块来支持超长上下文，而不会阻塞计算流水线——假设有良好的通信-计算重叠，正如本书通篇所讨论的那样。

此外，通过将请求状态从单个GPU解耦，系统可以使用全局KV缓存池在集群中的多个节点上动态分片KV数据，以自适应地平衡负载。这简化了大型推理集群的扩展并提高了故障隔离能力。

而且由于任何解码节点都可以访问全局KV池，因此任何解码节点都可以在需要时参与解码任何请求（例如故障转移或负载均衡）。这增加了调度器的灵活性，因为它可以选择一个最接近相关KV缓存块的解码节点。

如果某个前缀的一些KV块缓存在服务器A的DRAM中，调度在服务器A上进行解码可能会更快，因为它可以快速将它们拉入其GPU。相比之下，服务器B则必须通过网络获取KV块。

> 这描述了分布式系统的一个经典最佳实践：选择最接近计算数据的计算节点。这样，系统最小化了昂贵的数据移动。

一个高效的KV缓存调度器可以查看池中KV块的分布——以及网络拓扑——并据此分配预填充和解码任务。因此，预填充节点可以将KV数据放入一个实现为跨集群可访问的分布式内存空间的池中。

由于KV缓存处于集群侧共享内存空间中，任何解码节点都可以检索数据。这避免了每次都必须调度直接的预填充到解码的传输。

由于从池中检索KV数据需要额外的一跳，这增加了一点额外开销，但它允许更大的灵活性，因为所有解码节点都可以访问所有KV缓存数据。这也意味着，如果没有直接从特定预填充接收数据的解码节点，在需要时仍然可以从池中访问KV数据。

如果解码节点崩溃——或者请求因任何原因需要在生成中途移动——KV数据不会丢失。数据存在于池中，另一个节点可以接手并使用保存的KV从断点继续。这提高了容错能力。

全局KV缓存池还提供了跨请求的缓存持久性。这样，如果两个请求共享某个前缀，该前缀的KV可以计算一次并在整个集群中重用——即使请求最终落在不同的解码服务器上。

简而言之，分离式KV缓存池用内存（或更冷的存储）换取计算。通过存储更大的KV缓存，系统可以在许多场景下避免重新计算KV数据。这种方法利用了一个事实：重用数据——即使是从DRAM或SSD——通常比重复计算具有二次方时间复杂度 $O(N^2)$ 的大型注意力矩阵乘法要便宜得多。

### KV缓存重用与前缀共享

如前所述，对于共享公共前缀的提示词，跨请求重用缓存的KV数据是有益的。这种场景在多轮对话、共享系统提示词和附加文档的形式中相当常见。

系统可以存储前缀的KV输出并直接重用它们，而不是为每个请求重新计算该前缀的Transformer注意力输出。本质上，这跳过了输入那部分的预填充计算，节省了大量时间和GPU周期。

一个合适的以KV缓存为中心的调度器在分配工作时，会通过查看“前缀缓存命中长度”（即该提示词有多少词元已存在于缓存池中）来考虑前缀缓存命中。在实践中，如果新请求的前 $N$ 个词元与KV池中某个已缓存前缀匹配，系统可以决定重用该KV数据。

vLLM利用其PagedAttention机制，使用KV“页面”的全局哈希表实现了自动前缀缓存。在这里，每个独特的16词元上下文块都有一个哈希值。如果新请求需要的前缀与存储的块（按哈希）匹配，它可以直接复制那些KV张量而不是重新计算。

如果相同的上下文再次出现，系统从内存中服务它。本质上，它将上下文的KV视为可重用数据，可以通过哈希按内容查找。实现通常维护一个全局“提示词树（prompt tree）”来管理这些缓存的上下文，并在必要时驱逐它们。这针对最常重用的前缀进行了优化。

有效KV重用的关键是识别相同或重叠的前缀。通常，系统为了简单起见关注精确匹配，如果前 $N$ 个词元完全匹配，就重用那块。合并部分前缀重叠更复杂，因为你需要以某种方式合并缓存，这并不总是直截了当的。所以典型的缓存使用精确前缀缓存。

然而，这有一个权衡。无限期地存储许多用户的KV缓存会消耗大量内存。系统必须实施像LRU这样的驱逐策略，丢弃不太可能被重用的KV块。这为新的缓存腾出了空间。调度器也可能根据重用的可能性决定保留哪些缓存。其理念是在内存约束内最大化缓存命中。

如果某个预填充节点已经在其本地GPU内存或本地DRAM缓存中持有部分所需的KV，将请求路由到该节点以最小化数据传输可能是有益的。这是一个**数据感知调度（Data-aware scheduling）**的例子，它将计算发送到数据所在的地方，而不是总是将数据拉到计算可用的地方。

这类似于分布式系统中的局部性感知调度。在我们之前的路由讨论中，我们触及了这一点。如果可能，你应该将请求路由到生成其前缀的服务器。这最大化了缓存命中的可能性。

在更广泛的分离背景下，前缀缓存通过拥有跨许多请求的KV统一视图，并可能将其存储在像全局池这样的可共享位置来支持。这与孤立的每请求或每节点方法形成对比。

这也有助于减少分离架构可能因同一提示词在不同时间到达不同节点而产生的重新计算开销。有了全局KV存储或协调缓存，即使用户的请求命中不同的解码服务器，它们也能从彼此的缓存工作中受益。

### 优化的KV缓存内存布局

底层创新的另一个领域是优化KV缓存的内存布局。KV缓存存储每个序列中所有过去词元的键和值，对于许多并发解码流来说可能会变得巨大，因为每个流使用的内存大致与 `num_layers × 2 × sequence_length × d_head` 成正比。

分层缓存（Tiered caching）等技术很有用，因为并非所有KV对都需要时刻保存在GPU内存中。KV缓存的较旧部分可以交换到CPU——甚至进行压缩。

由于我们强调保持解码延迟低，大多数设计将活跃KV缓存保存在GPU内存中以便快速访问。在这种情况下，你可以调优内存的布局和访问方式。

DeepSeek的FlashMLA对KV缓存进行分页，并以固定大小的块（页面）分配缓存，以便活跃序列可以进行连续内存访问。这减少了缓存未命中和DRAM流量。

此外，一些系统实施**前缀压缩**，如果提示词的前缀不再被关注（例如因为上下文窗口移动了）。在这种情况下，KV缓存管理器可能会丢弃或压缩这些KV条目。这在长对话中更相关，因为上下文窗口会滑动。但这可以为极长序列节省内存和带宽。

> 这种驱逐/压缩技术在模型使用滑动窗口或其他受限注意力模式时是安全的。但是，如果不经过仔细评估，不应将其应用于在整个内容窗口上保留全注意力（或检索钩子）的层。

另一种称为**POD-Attention**的技术同样重新组织注意力计算以减少HBM流量。具体来说，它使用SM感知的线程块（或协作线程数组 [CTA]）调度。这实现了**运行时操作绑定**，动态地将运行在SM上的每个CTA分配给执行预填充或解码任务。这如**图 18-5** 所示。

*(图 18-5：SM感知的线程块（CTA）调度，在SM上匹配预填充任务与解码任务以最小化内存移动)*

因此，与其为每个阶段静态启动单独的内核，不如由单个内核启动足够的CTA来覆盖两种工作负载。在运行时，每个CTA检查它所在的SM，并使用每SM计数器根据该SM上正在运行的其他内容来决定运行哪个操作（预填充或解码）。

SM感知调度逻辑试图在运行时匹配预填充与解码操作。这避免了孤立的内存流量突发，并平滑了资源需求。

具体来说，POD-Attention将预填充和解码工作并置在同一个SM上，这样融合内核可以提高局部性并减少冗余HBM事务。这最小化了内存移动，最大化了带宽利用率，并平衡了每个SM上的计算受限和内存受限工作负载。通过配合适当的SM感知CTA调度在同一SM上并置预填充和解码工作以解锁完全重叠，POD-Attention可以将注意力性能提高约 **29%**。

POD-Attention的动态绑定解耦了硬件的CTA-SM分配与软件的CTA角色分配（预填充或解码）。这类创新显示了人们越来越关注软硬件协同设计，以最小化内存移动并充分发挥系统性能。

### GPU与CPU-GPU超级芯片的改进

你还应考虑新硬件在内存带宽方面的改进。更高的内存带宽和更大的L2缓存直接利好内存受限的解码阶段的性能。

NVIDIA的Grace Blackwell GB200 NVL72系统是一个机架级平台，拥有36个Grace CPU和72个Blackwell GPU，它允许单个逻辑解码单元拥有数十TB的KV缓存内存。这种硬件拥有约30 TB的统一内存，非常适合在内存中保留非常大的上下文。这些上下文可以是数百万词元的量级。

有了这样的平台，统一内存占用很大。然而，对于延迟关键的解码，你仍然希望活跃的键和值驻留在GPU HBM中。因此，你应该使用Grace CPU内存（LPDDR5X，而非HBM）作为低级缓存或用于非常旧的词元。当上下文超过可用HBM时，预填充和键值卸载仍然很重要——即使是在像NVL72这样的系统上。

简而言之，宏观层面的分离应与微观层面的优化相结合，以完全实现最大推理性能。FlashMLA/ThunderMLA等高级解码内核、高效的内存布局（分页缓存等）以及最新的GPU架构将产生高效且可扩展的解码。

## 预填充与解码间的快速KV缓存传输

分离式推理的一个关键要求是快速高效地将KV缓存从预填充工作节点传输到解码工作节点。如果这种传输不快，并行化预填充和解码所节省的任何时间都可能在等待数据移动中丢失。

在本节中，我们讨论用于最小化传输开销的技术。然后我们描述系统如何使用高速互连和避免额外KV拷贝来实现移交。

### KV缓存大小

预填充输出主要包含所有提示词词元的KV缓存。这可能是大量数据。考虑一个 $L$ 层的模型，每层有 $h$ 个维度为 $d$ 的注意力头，提示词为 $N$ 个词元。KV缓存大小大致为 $2 \times L \times N \times (h \times d)$，其中因子 2 是针对键和值两者的。

实际大小取决于精度（FP16 vs INT8等）和模型细节，但它很大。例如，一个40层的模型，16个大小为64的头，1,000词元的提示词会产生约40,000个KV向量的量级。这可能是数以百计MB的数据。如果词元数是5,000，那就是5倍大。

如果处理得很天真，通过网络传输这么大量的数据会引入显著延迟。例如，一种天真的方法可能是将KV复制到预填充工作节点的CPU内存，然后通过TCP发送——甚至写入磁盘供解码进程加载。这可能极慢，对于大提示词来说是数百毫秒的量级。目标是将传输时间减少到仅几毫秒。这允许预填充和解码真正并行重叠。

> 实现低延迟KV数据传输时间通常需要将小的PagedAttention块整理（collate）成更大的缓冲区，并通过基于GPUDirect RDMA的路径而非CPU套接字移动它们。

### 零拷贝 GPU-to-GPU 传输

现代分离式系统使用零拷贝GPU-to-GPU传输技术。在实践中，这涉及在高速网络结构上使用远程直接内存访问（RDMA）。例如，你可以使用InfiniBand进行机架/节点间的传输——或使用NVLink/NVSwitch在单节点（多GPU）平台内直接写入GPU内存。这些方法直接在GPU之间发送数据，而不通过CPU内存复制。

NVIDIA用于推理的高性能GPU-to-GPU传输库称为 **NVIDIA Inference Xfer Library (NIXL)**。NIXL为零拷贝GPU↔GPU和GPU↔存储数据移动提供了插件架构（例如，NVLink、UCX fabrics、GPUDirect Storage）。

NIXL简化了RDMA风格的传输，允许一个GPU通过可用的高速网络结构（例如InfiniBand或基于NVLink的连接）直接写入另一个GPU的内存。换句话说，预填充GPU可以直接将KV张量注入解码工作节点GPU的内存中。

基于RDMA的协议绕过CPU并充分利用GPU互连带宽。像NVIDIA Dynamo和开源的vLLM加LMCache集成等系统都依赖NIXL。具体来说，它们使用NIXL通过NVLink或RDMA直接将KV张量写入远程GPU内存。现代GPU互连提供极高的带宽，1 GB的传输可以在几到几十毫秒内完成，具体取决于链路类型和拥塞情况。

在实践中，实现通过将数据传输与计算重叠来达到低传输时间。例如，使用RDMA，解码GPU可以继续为其他序列生成词元，而预填充工作节点正在异步将其KV数据写入其内存缓冲区。预填充可以推送数据（RDMA写推送模式），或者解码可以拉取数据（RDMA读），取决于设计。无论哪种方式，数据路径中都不需要CPU参与。

快速KV传输的常见策略包括预填充侧推送、解码侧拉取、共享内存（CUDA IPC）缓冲区、连接器/队列抽象和非阻塞重叠。让我们逐一讨论：

*   **预填充侧推送 (Prefill-side push)**
    预填充工作节点在完成提示词后，启动RDMA写操作，将KV数据直接写入解码工作节点GPU上的预留缓冲区。这可以非阻塞地完成；预填充可以启动传输，然后在后台DMA发生时继续处理其他工作。

*   **解码侧拉取 (Decode-side pull)**
    或者，解码工作节点可以在准备好开始解码时，直接从预填充GPU的内存中RDMA读取。无论是推还是拉，都能达到相同的最终结果（无CPU拷贝）。有些实现可能更喜欢推送以卸载发送者的协调工作；其他的更喜欢拉取以便接收者控制时序。

*   **共享内存 (IPC) 缓冲区**
    如果预填充和解码恰好在同一台机器上（一台服务器中的不同GPU），它们可能会使用CUDA进程间通信（IPC）来共享内存句柄甚至PCIe bar，有效地在同一主机上使用NVLink或NVSwitch进行复制。这是不经过网络的零拷贝传输的本地变体。

*   **连接器/队列抽象**
    vLLM的实现将传输机制抽象在一个逻辑接口（Pipe或LookupBuffer）后面。预填充进程将KV放入此缓冲区或发出可用信号，解码侧检索它。在底层，这可以使用RDMA甚至高性能发布-订阅消息（在Dynamo的案例中使用NATS作为控制信号）。关键是将逻辑移交与传输解耦，以便可以插入不同的传输（RDMA、共享内存等）。

*   **非阻塞重叠**
    如前所述，优化系统将KV传输与正在进行的解码计算重叠。例如，Dynamo的解码工作节点在为新请求写入KV数据到其GPU内存的同时，继续生成其他请求的词元。这隐藏了大部分传输延迟。因此，你可以将约5毫秒的KV传输与解码计算重叠，对请求的第一个生成词元几乎不增加净延迟。

使用这些方法，KV传输可以在几毫秒的量级。这远少于在预填充工作节点上实际计算该KV所需的数百毫秒。因此，预填充 → 传输 → 解码的流水线实现了良好的并行性，因为解码几乎可以在预填充完成后立即开始——没有长时间的停顿。

发送KV缓存数据时要小心避免碎片和开销。例如，vLLM的PagedAttention以固定大小的词元块（通常每块16个词元）存储KV缓存。KV块相对较小（尽管每块的字节数随头数、头维度、层数和数据类型扩展）。天真地通过RDMA发送数千个小KV页面会导致过高的开销，因为每次传输都有固定延迟和协议开销。这将导致带宽利用率低下。

> 现代LLM引擎支持多种页面大小，如每块8、16、32、64或128个词元。较大的页面大小可以减少通过RDMA移动KV时的传输开销，因为持续链路吞吐量随着更大的整理缓冲区和更少的工作队列元素（WQEs）而提高。只要可能，整理成每RDMA写操作≥128词元的页面。确保在专用CUDA流上重叠传输。优先使用非阻塞流并使用事件栅栏（event fences）。始终使用Nsight Systems等工具进行分析以确认重叠。LMCache报告在RDMA上整理后，7.5k词元KV的传输时间从约20毫秒减少到约8毫秒。

LMCache扩展通过在传输前将KV页面整理成大的连续缓冲区来解决这种低效问题。本质上，它将GPU内存中的小块收集成一个大块，然后一次性传输该大缓冲区。

例如，如果作为470次小传输发送一个7,500词元的KV缓存需要20毫秒，将其整理成更大的块（例如，128词元页面）可将传输时间减少到8毫秒。这种简单的批处理优化保持了网络管道的充盈并减少了每包开销。

让我们看看系统如何配置为快速GPU-to-GPU KV传输。这是使用NIXL传输通道的LMCache预填充-解码模式的示例配置：

```yaml
# 预填充服务器配置 (lmcache-prefiller-config.yaml)
enable_pd: true
transfer_channel: "nixl"
pd_role: "sender"              # 此实例发送KV数据
pd_proxy_host: "decode-host"   # PD代理 / 解码协调器
pd_proxy_port: 7500            # 代理/解码器上的控制平面端口
# 设置你要传输的KV的缓冲区大小
# FP8/FP4 KV应显著缩小它
pd_buffer_size: 1073741824     # 1 GiB 传输缓冲区大小
pd_buffer_device: "cuda"       # 缓冲区保留在GPU内存中
```

这里，预填充服务器配置为RDMA发送者。它针对解码主机的7500端口，分配了1 GB GPU缓冲区用于KV传输。解码服务器配置为该端口上的接收者，并具有匹配的1 GB GPU缓冲区，如下所示：

```yaml
# 解码服务器配置 (lmcache-decoder-config.yaml)
enable_pd: true
transfer_channel: "nixl"
pd_role: "receiver"            # 此实例接收KV
pd_peer_host: "0.0.0.0"        # NIXL对等体绑定地址
pd_peer_init_port: 7300        # NIXL握手/控制端口
pd_peer_alloc_port: 7400       # NIXL分配/数据端口
pd_buffer_size: 1073741824     # 1 GiB (匹配发送者，除非计划分片)
pd_buffer_device: "cuda"       # 将缓冲区保留在GPU内存中
nixl_backends: [UCX]           # UCX后端对于分离架构已足够
```

此配置允许预填充直接将KV缓存写入解码GPU的内存——每次传输高达1 GB——无需CPU干预。双方都将传输缓冲区保留在GPU内存中以进行零拷贝操作。

调整传输缓冲区大小时，从 `pd_buffer_size = 1 GB` 开始。这大约是一个700亿参数、80层、32头、128维度的模型的约4–8k词元的FP16 KV缓存。如果提示词超过约7.5k词元，使用2 GB。你可以根据dtype和头数进行缩放：`bytes ≈ 2 × L × N × (H × Dh) × bytes_per_val`。确保在传输前整理页面。这将避免小IO低效。

> 如果你将KV缓存量化为FP8或FP4，固定词元数所需的传输缓冲区会减少，因为每词元的字节数相应减少。因此，你可以每缓冲区传输更多词元或相应减小缓冲区大小。1-2 GiB缓冲区适用于许多部署，但请根据上述KV公式调整大小并向上取整到256 MB边界。如果使用FP8或FP4 KV，可以按比例缩小缓冲区。始终针对你将传输的最大整理页面组进行验证。优先使用GPUDirect RDMA并将页面整理到≥128词元以获得最佳链路利用率。

在实践中，可以使用以下Shell脚本通过CLI启动解码服务器。这将减少急切碎片化（eager fragmentation）并鼓励在更大的缓冲区上进行交会（rendezvous）：

```bash
# 示例解码工作节点
# (通过索引或UUID选择设备)
UCX_RNDV_THRESH=16384
UCX_MAX_EAGER_RAILS=1
UCX_TLS=cuda_ipc,rc,rdmacm,cuda_copy,cuda_ipc,tcp \
CUDA_VISIBLE_DEVICES=1 \
LMCACHE_CONFIG_FILE=lmcache-decoder-config.yaml \
python run_vllm_decoder.py --port 8200
```

你会用类似的配置文件在另一个GPU上启动预填充服务器。这些设置确保系统使用跨节点的InfiniBand RDMA或节点内的NVLink点对点，而不是标准的TCP套接字进行KV传输。

> 对于单节点多GPU运行，应启用CUDA IPC。跨节点运行时，首选RDMA。LMCache/vLLM工作节点的典型UCX配置是设置 `UCX_TLS=rc,rdmacm,cuda_copy,cuda_ipc,tcp` 并确保RoCE/IB无损设置（ECN/PFC）已在网络结构上应用。对于节点间RDMA，考虑 `UCX_RNDV_THRESH=16384`，以便大KV缓冲区使用交会模式（rendezvous），小KV缓冲区使用急切模式（eager）。务必使用 `ucx_info -f` 进行验证。

有了RDMA和适当的缓冲，移交延迟可以在个位数到几十毫秒之间，具体取决于互连和页面大小。例如，对于7,500词元的上下文，LMCache测量到使用许多小传输大约需要20毫秒，而在整理成较大块后约为8毫秒。具体来说，建议在RDMA之前将16词元页面整理成 ≥128词元的板（slabs）。这将有助于减少每包开销。

简而言之，分离式系统应使用快速互连和智能数据整理，使预填充 → 解码的转换无缝且快速。最小化移交时间至关重要，因为如果移交很慢，就会抵消并行化这些阶段所带来的好处。

> 通过设置 `export PYTHONHASHSEED=0`，在多进程运行中使用确定性哈希进行KV块路由。

## 连接器与数据通路设计

建立在零拷贝优化之上，让我们看看预填充和解码节点如何端到端地协调传输——不仅仅是移动比特。预填充和解码工作节点通常使用调度器或路由器进行通信。在实践中，此调度器通常实现为集中式组件（如在NVIDIA Dynamo中），或去中心化协调方法（如SGLang所使用）。

例如，NVIDIA Dynamo实现了一个全局调度队列，解码工作节点将新提示词任务推入该队列，供预填充工作节点消费。在这种设计中，解码节点将请求入队以进行提示词处理，如**图 18-6** 中的“Put RemotePrefillRequest”（步骤6）所示。

*(图 18-6：NVIDIA Dynamo 中的请求生命周期；解码拉取提示词，预填充使用 NIXL 推送 KV)*

预填充节点获取此请求，完成后，由于请求携带了解码节点的源ID或回复ID，它确切知道将结果发送给哪个解码节点。然后使用NIXL RDMA直接将KV传输到该解码工作节点的GPU。

在vLLM + LMCache实现中，使用了一种更去中心化的方法。解码和预填充进程为每个请求的KV建立直接通道（管道或缓冲区）。在底层，这可能使用在请求开始时协商的一对一TCP或RDMA连接。不是全局队列，而是每个请求设置自己的传输通道。两种方法各有优缺点。全局队列对于负载均衡和故障处理更简单。直接通道可以最小化排队。

在决定使用哪种模式时，请考虑你的工作负载和基础设施约束。如果你需要健壮的多租户负载均衡和简单的故障转移，并且你可以接受小的排队延迟，全局队列模型通常更合适。相反，如果你有严格的尾部延迟要求、相对稳定的解码-预填充对集合以及高速互连，每请求直接通道方法可以最小化跳数和抖动。

> 在实践中，在你预期的请求组合下对两种设计进行基准测试。改变提示词长度、并发水平和故障场景，看看哪种能为你的SLO提供最佳的延迟-吞吐量权衡。

关键设计目标是使流水线非阻塞且高吞吐，以便在一个请求正在解码时，另一个提示词可以开始预填充——与此同时，另一个请求的KV可能正在传输中。因此，如果在另一个阶段有工作要做，没有任何阶段会闲置。这正是分离架构在大规模下提高整体吞吐量的确切原因——所有阶段都在并行忙碌。

> 通常，预填充生成的第一个词元的Logits并不显式传输，因为解码工作节点可以直接从KV重新计算第一个词元的概率。有些系统确实传输第一个词元的输出，以在解码工作节点中节省几百微秒的额外计算。但其他系统保持简单，只传输KV并让解码工作节点重新计算最后一层。

重要的是要确保此流水线对故障具有鲁棒性。如果解码节点在生成中途失败，前文讨论的全局KV缓存池可以允许另一个节点接手，利用保存在池中的KV继续生成剩余词元——可能会有轻微延迟。如果KV未保存，系统将不得不在另一个节点上重新计算预填充，然后继续解码。这就是为什么像vLLM这样的系统会定期检查点或将KV缓存复制到池中——即使并不严格需要分离。这样做是为了防止故障。

除了框架级KV快照外，托管GPU工作节点的Linux进程可以通过 `cuda-checkpoint` 加上用户态检查点/恢复（CRIU，如第5章所述）进行挂起和快照。这样，检查点可以恢复到同类型GPU芯片的另一个节点上，以最小化抢占或故障造成的工作损失。

> 通过使用 `cuda-checkpoint` 恢复预热的GPU内存，而不是每次启动都重新编译图和重新加载权重，可以减少推理引擎的冷启动延迟。

检查点可能会损害延迟，但至少请求可以完成。预填充节点故障在完成计算并将KV数据发送到解码工作节点或KV缓存池之后影响较小，因为KV数据已经离开了故障节点。但如果预填充工作节点在处理提示词期间失败，该提示词任务需要在另一个预填充节点上重试。架构应优雅地处理这些类型的故障，以便一个节点的故障不会导致整个请求报错。

> 这些路由器通常使用心跳检查和超时，以便如果预填充到解码的传输停滞，请求可以被重新分配或安全中止。

## 预填充与解码的异构硬件与并行策略

分离架构的一个强大优势是可以自由选择不同的硬件——甚至不同的模型并行配置——以分别最适合预填充和解码集群的需求。在统一的单体部署中，你通常只有一种硬件类型和配置用于两个阶段。通过分离，你可以按阶段混合和匹配硬件及策略，如下所述。

### 计算优化型 vs. 内存优化型硬件

预填充阶段受益于具有高计算吞吐量、大量TFLOPS、专用Tensor Cores和高时钟频率的GPU。它也受益于巨大的内存带宽，但除了提示词KV缓存所需之外，并不一定需要巨大的HBM容量。

另一方面，解码阶段受益于大内存容量和内存带宽，因为它处理许多词元的KV。它不需要极端的计算能力，但这方面也是多多益善。

这开启了为每个阶段使用不同代际GPU的可能性。例如，一种设计可以使用最新的高算力GPU用于预填充集群，而在解码集群上坚持使用具有足够内存带宽的上一代或高性价比GPU。

这样，我们避免了将最新的GPU（例如，最新的Tensor Cores）浪费在像解码这样无法充分发挥其潜力的任务上。预填充任务倾向于通过最大限度地利用GPU数学单元来消耗更多功率，而解码任务在同一GPU上使用的功率要少得多。

**吞吐量与成本效益**

在异构硬件之间拆分阶段可以提高单位成本吞吐量和单位瓦特吞吐量。在Splitwise研究中，使用阶段特定硬件导致一种配置在比同构基线低20%的成本下实现了1.4倍的吞吐量。

在旨在固定成本/功率预算下实现最大性能的另一种配置中，他们在相同成本和功率下实现了2.35倍的吞吐量。具体来说，该研究使用4× H100（高算力）进行预填充，4× A100（高内存）进行解码。这种混合配置在相同成本/功率下实现了8 GPU同构系统（全H100或全A100）约2.35倍的RPS。

或者，他们发现，为了匹配基线吞吐量，异构系统实际上可以使用更少的总GPU（例如，5或6个而不是8个），方法是将解码卸载到更便宜的GPU上。这突显了成本节约的机会，并展示了在最有效的地方使用每种类型GPU的价值。具体来说，你可以在每美元算力最高的GPU（例如Blackwell或Rubin一代）上执行计算受限的工作，并将内存受限的工作分配给具有适当内存带宽的更具成本效益的上一代GPU（例如Hopper或Ampere）。

Splitwise评估考虑了异构GPU之间状态传输的开销。该测试通过NVSwitch网络传输KV数据，产生的开销极小——即使在不同代际的GPU之间也是如此。这表明像NVSwitch和NVLink这样的高带宽互连可以实现预填充/解码分离，且对性能的影响可以忽略不计——即使在混合GPU设置中也是如此。

另一个系统是HexGen-2，这是一个分布式推理框架，将异构GPU上的分离式推理分配视为优化问题。其调度器综合优化资源分配、每阶段并行策略和通信效率。

在Llama 2 70B等模型的实验中，HexGen-2显示，与同价位的最先进系统相比，服务吞吐量提高了高达2倍（平均约1.3倍）。此外，它实现了与高端基线相似的吞吐量，同时成本降低了约30%。这些改进来自混合GPU类型和优化工作拆分。这基本上是以自动化方式实现了Splitwise概念上所做的事情。

这些结果证实，分离不仅仅关乎速度。它也关乎效率和少花钱多办事。在云环境中部署推理时，对于支持数百万或数十亿最终用户的大型推理服务来说，这可能会节省数百万美元的GPU时间成本。

例如，你可以用6个GPU（预填充+解码混合）而不是8个顶级GPU来满足相同的流量；你可以为该服务节省约25%的硬件成本。因此，分离架构让你能在相同的硬件上服务更多的用户。这很关键，因为GPU供应往往有限——特别是最新的GPU。

鉴于世界某些地区（包括美国）的电力限制，能源效率也很重要。Splitwise证明，通过在较低功率的GPU上运行解码任务——速度略有下降——可以获得更好的能效。

通过将预填充和解码任务分配到不同的硬件类型上，你可以选择在哪里以及如何运行每个阶段，以提高性能并降低成本。分离架构允许这种灵活性，因为各阶段是独立的。

简而言之，来自Splitwise、HexGen-2和相关异构部署研究的评估表明，除了纯粹的速度外，还可以利用分离架构进行成本优化。通过匹配硬件与工作负载，你可以显著降低每个查询的成本，同时在固定预算内提高性能。

对于大规模服务，这对于保持经济可行性至关重要。权衡包括系统复杂性略有增加，因为你必须管理多种GPU类型。并且你在集群配置灵活性和跨预填充/解码任务动态重新分配GPU方面会受到限制，因为GPU在能力上不匹配。但是，在许多情况下，为每个阶段使用不同的硬件可能值得其带来的效率收益。

### 阶段特定的模型并行

另一种形式的异构性和每阶段专门化是为每个阶段跨GPU选择不同的模型并行（例如，张量并行、流水线并行等）。这与因内存限制而跨GPU分片的大型模型有关。

在传统设置中，你可能会使用固定的并行策略运行模型，并为预填充和解码阶段使用张量并行或流水线并行将模型拆分到多个GPU上。但预填充的最佳并行策略可能与解码不同。

例如，预填充阶段是对 $N$ 个提示词词元的大型前向传递，它受益于高度并行化。你可以使用张量并行（TP）跨许多GPU更快地执行计算并减少TTFT。同步GPU的开销被一次可以处理的大量词元分摊。这减少了该阶段的挂钟时间，这对TTFT至关重要。

你甚至可以使用流水线并行（PP）来进一步加速预填充并增加吞吐量。这将模型层跨GPU拆分，并通过多个流水线阶段流式传输提示词。

另一方面，解码阶段是顺序的，且对每步延迟敏感。为一个解码使用过多的GPU实际上可能会损害每输出词元耗时（TPOT）延迟，也称为*词元间延迟（inter-token latency, ITL）*。这是因为每个词元步骤都需要额外的多GPU通信开销。因此，加速的潜力是有限的，因为一次只有一个词元的计算可供拆分（如果使用推测解码，则有几个词元）。

分离架构使得混合这些方法成为可能，为一个阶段使用TP，为另一个阶段使用PP——或者使用每种技术的不同程度。例如，你可以运行TP=8的预填充以跨越8个GPU并最小化提示词延迟。然后你可以运行TP=1（单个GPU）的解码，以最大化每词元吞吐量并最小化步骤延迟。通过这种方式，每个阶段的吞吐量和延迟可以单独调整，如**图 18-7** 所示。

*(图 18-7：为预填充和解码使用不同的并行策略)*

在这里，张量并行的额外All-Reduce通信开销在预填充阶段更为突出，因为正在并行处理大量词元。因此，我们选择流水线并行，因为它对我们的预填充工作负载更有效。

> 张量并行和流水线并行都可以有效地用于预填充。即将展示的例子在预填充中使用流水线并行。然而，大的张量并行度可以在某些集群上减少TTFT。最佳选择取决于网络带宽、集合通信延迟和模型形状。

然而，对于解码阶段，流水线并行可能导致更多的（尽管更小的）前向传递，因为词元在GPU之间传递。这需要大量的数据进出GPU，仅仅为了单个词元的生成。因此，我们选择张量并行，因为它更适合我们的解码工作负载。

然而，这引入了一个复杂性。由于我们的模型并行化方案在预填充和解码之间不同，KV缓存的格式也必须不同。例如，如果预填充阶段使用TP = 1（因为它使用PP）配合四个GPU，那么这四个GPU中的每一个都有一个全尺寸的KV张量。

假设解码阶段使用TP = 4。在这种情况下，每个GPU只期望1/4的KV张量，因为数据应该沿模型的隐藏层大小拆分。为了处理这个问题，像NVIDIA Dynamo这样的系统可以在传输过程中执行KV转置或转换。本质上，它将KV缓存从 [TP_p parts] 转换并重新排列为 [TP_d parts] 所需的格式，其中TP_p是预填充的并行度，TP_d是解码的并行度。

Dynamo包含一个高性能内核，用于在从NIXL读取后——且在写入解码工作节点的内存之前——即时进行此转置。这样，接收端解码就能以它预期的布局获得KV缓存数据。

与网络传输相比，这种转置的开销可能很小——特别是考虑到NVLink的吞吐量，它可以快速处理这些数据重组。在这种情况下，使用针对每个阶段优化的不同并行策略所节省的计算量很容易证明这一点的合理性。

让我们探索一个阶段特定并行的例子。考虑一个大模型，我们可以应用各种并行方案：张量（TP）、流水线（PP）、数据（DP）、序列（SP，跨GPU拆分序列）等。我们可能会选择单独的并行配置。**表 18-1** 显示了预填充阶段的示例并行配置。

**表 18-1. 预填充并行示例**

| 并行策略   | 符号 | 值       | 描述                                                         |
| :--------- | :--- | :------- | :----------------------------------------------------------- |
| 张量并行   | TP_p | 2        | 将模型的权重张量拆分到2个GPU上，以在可控的通信开销下将预填充延迟减半。 |
| 流水线并行 | PP_p | 2        | 将模型层分为两个流水线阶段，通过每个阶段流式传输微批次，适用于深层模型。 |
| 序列并行   | SP_p | 1        | 不跨GPU拆分输入序列（无序列分片），除非处理极大的上下文。    |
| 上下文并行 | CP   | 1        | 当优化后能装入内存时，将整个上下文保持在单个GPU上（无上下文级分区）。 |
| 数据并行   | DP_p | 1 (或 2) | 每个GPU使用一个模型副本（或通过权重复制两个副本以使批处理提示词吞吐量翻倍）。 |

这些数字最小化了GPU间开销，同时仍使用多个GPU加速大提示词。接下来，让我们看看解码的示例并行策略，如**表 18-2** 所示。

**表 18-2. 解码并行策略示例**

| 并行策略   | 符号 | 值                     | 描述                                                         |
| :--------- | :--- | :--------------------- | :----------------------------------------------------------- |
| 张量并行   | TP_d | 1 (默认) ... N (GPU数) | 默认为TP_d = 1以简化和最小化同步开销。TP_d = N个GPU可以在小批次微小GEMM或单个GPU无法容纳模型时提高效率。 |
| 流水线并行 | PP_d | 1                      | 流水线并行会为单词元解码增加气泡，因此PP_d = 1避免了空闲阶段。 |
| 序列并行   | SP_d | 1                      | 跨GPU拆分输出序列并不常见。SP_d = 1保持每个解码流本地化，除非处理极长输出。 |
| 数据并行   | DP_d | 1                      | 每个解码流每个GPU一个模型副本。使用单独的副本来处理并行请求，而不是为单个流进行复制。 |

> 如果模型无法装入单个Blackwell B200，例如，优先对解码使用TP_d = 2或4，而不是使用PP。这将有助于避免流水线气泡。

理想情况下，每个解码流运行在单个GPU上并避免跨GPU开销。这只有在模型能装入GPU内存时才可能。在这种情况下，TP_d = 1，意味着解码期间不使用任何张量并行。如果模型不能装入内存，你可以增加张量并行度（例如，TP_d = N，其中N是GPU数量）。

如果系统正在发布微小GEMM来处理小批次，增加张量并行也是有用的。这是因为跨设备分发小矩阵乘法可以将通信延迟隐藏在计算后面，并可能产生更高的整体吞吐量。

这些是说明性数值，主要观点是分离架构允许你在提示词侧单独配置资源以达到TTFT目标。同时，你可以独立调整解码侧的资源，以达到将词元流式回传给最终用户的吞吐量和延迟目标。

这样，两种并行策略互不干扰。在统一系统中，如果你试图这样做，你就不得不选择一种妥协策略，这对两个阶段来说都是次优的。

**预填充和解码的不同精度**

一些推理引擎允许你在预填充和解码之间使用不同的精度。例如，你可以以较低的精度（如FP8, INT8或FP4）执行预填充以加速它。同时，如果为了更好的生成精度，你可以以较高的精度进行解码。

通常，你让两者以相同的精度运行，以便预填充阶段计算的KV缓存可以在解码阶段使用。然而，你可以应用类似于上一节描述的并行转换的转换。你可以选择在发送前量化KV并从FP16压缩到INT8/FP8/FP4。然后在接收端将其转换回来（如果需要）。

例如，你可以选择通过网络发送较低精度以加速传输。或者你可以根据可用的FLOPS等选择在发送端或接收端执行转换。

这些是高级理念。但它们强调了在分离式设置中，几乎每个方面——硬件类型、GPU数量和精度——都可以针对每个阶段独立调优。

### GPU-CPU协同的混合预填充

到目前为止，我们假设预填充和解码阶段都运行在GPU上——而且可能是不同类型的GPU。然而，在极端规模下——或者对于极大的模型和提示词——评估CPU是否可以分担GPU的压力是值得的。

现代CPU在神经网络计算方面远慢于GPU，但它们具有其他优势，如充足的RAM、无GPU内存带宽争用，以及处理GPU可能不太擅长的任务（如极长序列和非Transformer操作，如分词、填充等）的灵活性。

采用混合预填充策略，部分预填充计算在CPU上完成。一种场景是针对超长提示词的CPU卸载。考虑一个来自大型文档附件的数万词元的提示词。由于内存限制，即使是强大的GPU也可能难以处理如此大的提示词。

在极长提示词的情况下，系统可以选择在拥有大量RAM以容纳长序列的CPU工作节点上执行模型的初始层。然后它可以稍后将中间结果流式传输到GPU——或者如果延迟不是问题，甚至在CPU上执行整个预填充。解码GPU随后将从CPU工作节点接收巨大的KV缓存。

> 虽然在交互式推理中不常见，但一些批处理或离线流水线，如长时间运行的“深度研究（Deep Research）”作业，可以使用CPU预处理非常长的文本。

CPU卸载的一个实际用途是处理后台或低优先级预填充任务。例如，LLM服务可能允许提交非常大的提示词，用于非交互式请求的离线处理。这些可以分配给仅CPU的工作节点，最终馈送到解码GPU进行快速词元生成。由于CPU较慢，延迟会很高，但既然是离线作业，这可能是可接受的。而且这种配置释放了GPU资源用于更多交互式工作负载。

混合预填充在像Grace Blackwell这样的CPU-GPU超级芯片上更为常见，其中芯片间互连超快。并且它利用了CPU内存相比GPU内存巨大的事实。

想象一下将巨大的KV缓存存储在CPU内存中，且GPU可以快速访问。混合预填充将使用CPU的内存来缓冲或预处理输入词元，而GPU专注于繁重的Transformer层。

Grace Blackwell超级芯片可以通过让CPU管理内存和初始层，让GPU处理序列块上的密集注意力，来处理海量上下文。Grace CPU还可以用于将装不下HBM的KV缓存数据溢出到CPU DDR内存。这有效地扩展了GPU可以支持的上下文长度。

你可以跨设备切分Transformer，在GPU上运行前 $N$ 层，其中大部分词元被处理且序列本质上被压缩。然后你将接下来的 $M$ 层卸载到拥有大量内存的CPU，最后再将剩余层带回GPU生成最终输出。

这种层划分技术增加了显著的数据移动和编排复杂性——仅应在罕见情况下证明其合理性，如超长上下文或严重的GPU内存限制。无论如何，它展示了如何在极端推理场景中突破硬件界限。

在我们的分离架构中，引入CPU意味着引入第三种工作节点：CPU预填充工作节点。调度逻辑随后可以在三个选项中选择：GPU预填充工作节点、CPU预填充工作节点或本地解码GPU预填充。决定将取决于提示词长度或优先级等因素。

例如，策略可能是：如果 `prompt_length > 5,000`，路由到CPU预填充工作节点，知道它会慢但至少不会占用GPU并可以使用大内存。解码阶段随后将等待更长时间获取KV。或者可能，在极端情况下，如果真的是离线任务，解码也可以在CPU上完成。

通常，CPU卸载会增加TTFT，所以它不用于正常的延迟敏感请求。它更多是一种扩展性和安全网功能。如果使用，系统应监控该路径被采用的频率，因为频繁的CPU卸载可能表明需要更多的GPU容量或模型优化。

CPU卸载还允许系统处理边缘情况，如超长输入或当GPU全忙时的突发流量。它通过回退到较慢的CPU而不是完全失败来做到这一点。然而，记住如果处理太慢且超过SLO要求，最好是快速失败。

从成本角度来看，将CPU用于某些工作可能更经济，因为CPU核心比GPU小时便宜。一些云提供商可能会运行混合GPU和CPU实例用于LLM服务。CPU在这一过程中执行输入预处理——甚至小模型推理——然后才使用GPU。

简而言之，虽然核心分离逻辑专注于跨GPU分发工作，但健壮的超大规模推理系统可以创造性地利用CPU处理工作负载的某些部分。随着硬件发展并向紧密耦合的CPU-GPU超级芯片设计（如Grace Blackwell）迈进，使用GPU和CPU之间的界限将变得模糊。

高效调度应考虑所有可用的计算资源。然而，指导原则保持不变。将GPU用于它们最擅长的事情，包括中等序列长度的大规模并行计算。当GPU可能效率低下时使用CPU，例如用于极长序列、内存繁重的任务或低优先级任务。

## 动态调度与负载均衡

前期，你可以确定分配多少资源给预填充与解码。但随后你会想要随着工作负载的变化动态调整这些资源分配。

如果是纯静态配置，如果工作负载组合发生变化，你需要更多偏向提示词或偏向生成的分配，固定的比例将变得次优。预填充和解码容量的最佳平衡将随时间推移而变化。某一时刻，许多新请求到达，你想要一个重预填充的分配。稍后，你可能有许多长的、正在进行的推理生成，你想要一个重解码的分配。

自适应调度和负载均衡机制旨在持续调优系统，以便没有任何一个阶段成为瓶颈。这在变化条件下保持高有效吞吐量。

现代推理引擎如vLLM、SGLang和NVIDIA Dynamo将负载监控和动态工作节点分配整合到其平台中。此外，许多云推理平台都有使用类似原则的自定义自动伸缩器和路由器。

### 自适应资源调度与热点预防

分离式设置的一个问题是预填充和解码集群之间的负载不平衡。如果预填充与解码工作节点的比例配置不当，无法适应当前的负载组合，一方可能会饱和而另一方仍未充分利用。

例如，如果相对于预填充没有足够的解码工作节点，那么解码任务将会排队。这会增加TPOT，即使提示词正在被快速处理。相反，如果解码过度配置但预填充配置不足，新请求可能会等待开始。这导致高TTFT，而许多解码GPU闲置。

理想的情况是双方都在接近容量但不被淹没的状态下工作。这样系统保持预填充和解码都忙碌，但都没有增长的队列。

在现实世界条件下，静态的预填充-解码工作节点分配是不够的，因为工作负载全天变化很大。在长期运行的大规模推理系统中，输入长度和输出长度的混合比例每小时都会有显著变化。

例如，一个小时内可能有许多产生简短答案的长问题（例如摘要）。这需要更多的预填充资源和较少的解码资源。下一个小时，你可能会得到产生长答案的短问题（例如推理、网络搜索）。这需要较少的预填充但更多的解码。

换句话说，适用于一种场景的静态预填充-解码比率可能对另一种场景并非最佳。因此，最佳的 $X_p$ 与 $Y_d$ 配置是依赖于工作负载的，并且会随时间推移而变化。

为了解决这个问题，先进的推理系统可以使用自适应调度算法，即时重新分配负载甚至重新调整实例用途。接下来让我们讨论几种这类方法。

### TetriInfer 的双层调度器

研究原型调度器 **TetriInfer** 在两个粒度上运作。首先，在单个请求级别，它根据当前负载将传入请求分配给特定的预填充和解码实例。这是我们预期的正常路由。其次，它监控集群范围的资源利用率并预测哪里可能形成瓶颈。这主动转移工作以防止热点。

例如，调度器可能看到一个解码节点正在堆积一堆非常长的序列。在它成为问题之前，调度器将其中一些序列路由到另一个更可用的解码节点——即使它通常不会路由到这个解码节点。这样，负载被平衡了。**图 18-9** 展示了现有系统与TetriInfer在执行时间线和架构方面的对比。

*(图 18-9：现有系统与TetriInfer架构的对比)*

在这里，你可以看到通过利用队列长度、GPU利用率趋势等预测资源使用，TetriInfer的双层调度器平滑了整个集群的负载，并防止任何单个节点过载。

> TetriInfer这个名字暗示像俄罗斯方块（Tetris）方块一样“打包”请求，以填充GPU时间且无干扰。

### Arrow 的自适应实例伸缩

另一种动态资源调度技术是 **Arrow**（不要与流行的Arrow数据格式混淆）。这是一种自适应实例伸缩技术，利用了分离式系统通常对工作负载变化响应滞后的事实。例如，如果输入与输出的分布发生变化，预填充与解码工作节点的静态数量不会立即调整。这导致有效吞吐量的暂时损失，因为一方成为瓶颈。

Arrow通过测量输入词元速率与输出词元速率——以及集群中每个工作节点池的积压情况——来持续分析工作负载。然后它动态调整工作节点的分配，如**图 18-10** 所示。

*(图 18-10：Arrow 架构)*

在云环境中，这可能意味着当输出负载增加时启动额外的解码实例。如果在检测到输入繁重的工作负载时，你也可以缩减解码以支持更多的预填充实例。

Arrow的设计包括决定哪个节点处理哪些请求的请求调度（类似于其他调度技术）和决定何时启动或关闭预填充或解码实例的实例调度。

Arrow将预填充和解码工作节点的数量视为可调参数，可以在近乎实时内调整和伸缩。Arrow的设计将自动伸缩逻辑带入了LLM推理调度器内部。

例如，如果大量输入重但输出轻的请求进入系统，预填充节点可能成为瓶颈。在这种情况下，Arrow会检测到增长的预填充队列时间或上升的TTFT百分位，并决定将一些解码工作节点转换为预填充工作节点——或启动额外的预填充实例来处理激增。

相反，如果一波具有非常长答案（例如推理链）的重输出请求进入，解码集群可能会开始滞后，表现为TPOT上升和长解码队列形成。在这种情况下，Arrow可以分配更多的GPU工作节点到解码阶段——如果提示词到达变慢，或许通过暂时闲置一些预填充节点。否则，它可以直接添加新的解码节点。

在具有固定数量GPU的静态本地集群中，动态扩展将涉及任务重新分配，即指示一些分配给预填充的GPU切换角色并在一段时间内加入解码池。如果每个计算节点都被配置为既可以作为预填充工作节点也可以作为解码工作节点运行，这是可行的。

切换角色可能会有开销，因为模型可能需要加载不同的模型分区以适应不同的并行策略或量化选择等。一些设计在每个GPU上保持所有模型权重加载，并根据需要仅向它们馈送不同的任务。这有效地将集群视为一个灵活的池，在任何给定时刻，一部分工作节点做预填充，其他的做解码。

这开始类似于粗粒度的时间共享统一集群，其中一个节点在一段时间内专门执行预填充任务，然后切换到执行解码任务。

> TetriInfer论文描述了一种“实例翻转（instance flip）”，其中一些节点可以在需要时翻转角色。这要求模型被正确加载、分片和量化以处理这两种角色。

**无状态性以实现弹性**

Arrow利用无状态实例。因此，工作节点上不存储长寿命的会话状态。这样，它们可以自由地重新分配。活跃请求的KV缓存使得在解码节点处于词元生成中途时将其翻转为预填充变得复杂，所以你通常必须等待解码完成其任务后再切换角色。

**稳定性与震荡**

快速切换角色会导致震荡和颠簸。因此，通常强制执行角色所需的最小时间量，以避免过于频繁地翻转。

除了负载均衡，另一个要考虑的方面是多租户或混合工作负载。如果集群服务不同的模型或任务，你甚至可以根据需求在它们之间重新利用GPU——并超越单个模型的范围。

分离架构的模块化配置允许使用空闲的解码GPU来运行另一个较小模型的推理任务。这是一种截至撰写本文时尚未主流化的扩展，但随着推理引擎的发展和设计变得更加灵活，这在概念上是可能的。

底线是，超大规模推理系统需要一个反馈循环来持续匹配资源分配与当前工作负载。TetriInfer、Arrow、Mooncake等显示，当使用这样的反馈循环并适应负载变化时，有显著的改进。这突显了虽然分离架构移除了干扰，但自适应分离架构移除了不平衡。两者都是在动态条件下获得最佳性能所必需的。

## 本章要点

本章涵盖了各种技术，如统一巨型内核、高效内存分配、快速数据传输、预填充/解码分离、KV缓存池、动态伸缩和持续SLO感知。以下是一些关键要点：

*   **加速解码阶段**
    解码阶段可以通过使用融合注意力内核（包括FlashMLA、ThunderMLA和FlexDecoding）得到极大加速。这些内核可以提高单词元吞吐量和GPU利用率。

*   **将KV缓存视为一等公民**
    使用分离架构跨GPU共享KV缓存。重用前缀以避免冗余计算。这通过全局缓存池和哈希启用。

*   **力求预填充与解码工作节点间的近零开销**
    利用GPUDirect RDMA和NIXL进行高速GPU-to-GPU传输。重叠计算/传输以在预填充和解码工作节点之间实现近零开销。

*   **拥抱各阶段的专用硬件和并行策略**
    分离预填充和解码允许为每个阶段使用专用的硬件和并行策略。例如，使用高算力GPU或多GPU节点进行预填充。使用大内存GPU或单GPU进行解码。目标是降低成本并增加吞吐量。

*   **使用自适应和动态算法优化系统**
    自适应调度和SLO感知控制（例如，早期拒绝和动态伸缩）对于在变化负载下维持延迟保证——并在不过载的情况下充分利用所有GPU——是必要的。

## 结论

超大规模LLM推理需要一种整体方法，包括高层自适应资源管理和底层内核及内存优化。通过结合本章介绍的技术，高度优化的推理部署可以在现代硬件上实现最大吞吐量，同时满足严格的延迟保证。

随着硬件的持续演进，GPU（例如，增加的内存和专用推理核心）和软件框架变得更加复杂（例如，动态路由和灵活的角色分配），这些优化将叠加并变得更加强大。这将允许推理引擎高效地处理更大的模型、更长的上下文和更多的用户。



# 第19章 动态自适应推理引擎优化

在现代硬件上进行超大规模语言模型（LLM）推理，需要动态的运行时适应能力，以便在多变条件下同时实现高吞吐量和低延迟。静态的、“一刀切”的模型服务优化方案已不再适用。

取而代之的是，最先进的模型服务系统采用了**自适应策略**。它们能即时调整并行度、数值精度、CUDA 内核调度以及内存使用。本章将探讨这些高级技术，包括动态并行切换、精度缩放、实时缓存管理以及基于强化学习（RL）的调优。

本章为你提供超大规模 LLM 推理的最佳实践，教你如何编排一个能自我监控性能并实时适应以最大化效率的引擎。

## 自适应并行策略（TP vs. PP vs. 混合）

海量 LLM 需要模型并行（如张量并行和流水线并行，或两者的混合）来将计算分布到多个 GPU 上。每种方法都有其优缺点。**表 19-1** 总结了针对特定推理流量模式的推荐并行策略。

**表 19-1. 常见推理流量模式与推荐并行策略的映射**

| 推理流量 $p$                               | 推荐并行策略                          | 理由                                                         |
| :----------------------------------------- | :------------------------------------ | :----------------------------------------------------------- |
| **大量短请求** <br> (< 256 tokens, 高 RPS) | **数据并行 / 副本扩展**               | 最小化 GPU 间通信；每个 GPU 运行独立处理请求的副本（假设模型能装入单 GPU 显存）。 |
| **少量长请求** <br> (≥ 8k tokens, 低并发)  | **流水线并行 (PP)** <br> (带微批处理) | 通过将层切分到不同 GPU 上来降低单请求延迟。                  |
| **混合负载** <br> (短请求 + 部分长请求)    | **混合动态** <br> (自动切换)          | 小对话在单 GPU 上运行，长对话使用流水线以满足延迟 SLA。      |
| **极大模型** <br> (> 1 GPU 显存)           | **张量 + 流水线混合**                 | 装载模型的必要手段；在两个维度上平衡计算与显存。             |
| **MoE 模型推理** <br> (稀疏专家选择)       | **专家并行 (EP)**                     | 将单个专家分布在不同 GPU 上；每个请求仅调用专家子集，降低单设备的显存和计算负载。 |

**数据并行和副本扩展策略**会在每个 GPU 上复制完整模型，并在这些副本间平衡传入请求。这不需要针对单个推理进行 GPU 间同步，因为每个 GPU 独立处理请求。这在通信开销最小的情况下最大化了许多中小型输入的吞吐量。当然，如果模型无法装入单 GPU 显存，数据并行就不是选项了。

**张量并行 (TP)** 将模型矩阵（如权重、层等）切分到多个 GPU 上以加速矩阵乘法。但这引入了额外的 All-Reduce 通信来保持 GPU 同步。

**流水线并行 (PP)** 则将整个层分配给不同的 GPU 以克服显存限制——假设单层能装入单个 GPU。PP 会引入顺序阶段延迟形式的额外开销，这被称为**流水线气泡（Pipeline Bubbles）**，如**图 19-1** 所示。

*(图 19-1：PP 导致的流水线气泡)*

**专家并行**用于混合专家（MoE）模型架构，它为每个专家子网络分配自己的 GPU。一个轻量级的门控网络随后将每个输入请求或 Token 导向路由器识别出的 Top-k 活跃专家。在这种情况下，每个 GPU 仅处理它托管的专家子集。

通过每次输入仅激活少数专家，专家并行降低了拥有大量专家模型的单设备显存、推理时间和计算成本。这种基于路由器的条件计算模式在增加专家数量时能有效扩展。例如，DeepSeek-R1 拥有 256 个总专家，但在推理期间路由器仅选择前 9 个专家（含 1 个共享专家）。

传统上，并行策略——包括多种技术结合的混合策略——是在模型加载时预先选定并固定的。然而，为了在动态工作负载下最大化性能，现代推理引擎可以在运行时根据输入特征选择不同的并行策略。

高性能、自适应的推理系统会即时根据运行时指标选择 TP、PP 或混合方法。关键因素包括批处理大小、序列长度、显存利用率以及响应延迟和吞吐量要求。例如，非常长的提示词可能会被路由到 TP + PP 实例，因为这样可以将层分散到多个 GPU 上以避免内存溢出（OOM）错误。

同时，短的、延迟敏感的请求会路由到仅 TP 的模型实例，以避免流水线阶段的开销。为了支持这一点，你的服务引擎需要维护多个预分片的模型实例，每个实例针对不同的工作负载配置文件进行优化，并动态地将查询分发到最能满足该作业 SLO 的模型实例。

你还可以使用不同数量的分片。如**图 19-2** 所示，在八个 GPU 上使用了两种不同的混合 TP + PP 并行配置（TP=4, PP=1 和 TP=2, PP=1）。

*(图 19-2：在八个 GPU 上为给定模型预配置两个不同的混合分片池)*

对长序列输入即时使用 PP 有助于避免由大输入序列引起的 OOM 错误。相反，对于短提示词和延迟敏感的查询，系统可以路由到为低延迟优化的张量并行模型实例。在这种情况下，请求避开了 PP 的开销。

由于每个请求可以使用不同的并行策略，系统需要维护模型的多个实例供推理调度器/路由器选择。一个实例可以使用 TP 优化低延迟，而另一个实例使用 TP 和 PP 优化高吞吐量和大输入序列。

维护多个模型实例是必需的，因为即时重新分片（Resharding）会严重破坏 GPU 缓存，并对内存和网络子系统造成巨大压力——尤其是在重新分片超大模型时。

在运行时，每个查询会根据其长度和指定的服务等级协议（SLA），被分发到最匹配的模型实例（分片策略）。

为了支持不同的工作负载配置文件，GPU 可以被组织成逻辑工作池，每个池都预分片为特定的并行策略。

让我们看一个例子。如果我们有一台 8× GPU 的 Blackwell B200 服务器，总 HBM 为 1,440 GB（180 GB × 8），我们可以用 4 个 GPU 运行 4 路 TP 的 DeepSeek-R1——剩下 4 个 GPU 空闲。

如果来了一个超长上下文（例如 > 100 万 token）的查询，调度器可以生成一个两阶段流水线：阶段 1 跨越 GPU 0–3，阶段 2 跨越 GPU 4–7。这有效地将每阶段可用显存翻倍至约 720 GB，有助于避免处理大输入时的 OOM。

相反，当数十个短的、延迟敏感的提示词并发到达时，系统将它们路由到仅 TP 的实例。通过避免流水线气泡，该配置在所有可用 GPU 上提供了尽可能低的单请求延迟。

要实现动态并行切换，你可以编写一个决策函数，检查运行时指标如输入序列长度、GPU 显存使用率和当前负载。如下所示：

```python
def choose_worker_pool(seq_len, gpu_mem_util, concurrent_reqs):
    # 对于长上下文或高内存压力，使用 PP + TP 混合
    if seq_len > 4096 or gpu_mem_util > 0.8:
        return "tp_pp_hybrid"
    
    # 对于大量并发小请求，坚持使用张量并行
    if concurrent_reqs > 4:
        return "tensor_parallel"
        
    # 典型工作负载回退到张量并行
    return "tensor_parallel"
```

这需要你在 GPU 集群上预启动多个模型副本——一些仅 TP 分片，另一些 TP + PP——并让路由器根据输入和策略发送查询。

建议使用来自模型和硬件的遥测数据来通知并行切换。你可以实时监控 GPU 显存利用率、计算利用率和互连（如 NVLink/NVSwitch）流量。如果你注意到 GPU 因长流水线气泡而空闲——且有额外内存空间——你可以将流水线折叠为更少的阶段，让每个 GPU 做更多工作并保持忙碌。

关键在于动态调整张量和流水线切分的平衡，以保持每个 GPU 得到充分利用，同时处于内存限制内并满足延迟目标。这是静态配置无法做到的。

## 动态精度变更

现代 GPU（如 Blackwell）支持 8-bit 和 4-bit 浮点（FP8/FP4）Tensor Core 数学单元。这些低精度提供了巨大的加速、内存节省和极小的质量损失。

**动态精度切换**是一种高级技术，推理引擎根据模型置信度或资源压力在运行时调整数值精度。目标是在没有显著质量损失的情况下提高吞吐量。在实践中，这意味着系统可能会为了效率在 FP8 或 FP4 中执行模型的某些部分，但在需要稳定性时回退到更高精度（FP16/BF16）。

精度适应的一个触发因素是**Logit 锐度（Logit Sharpness）**，即模型的输出置信度。如果下一个 Token 的概率分布显示出极端的峰值（高度自信），低精度带来的小数值误差不太可能改变结果。引擎可以安全地在接下来的几步中使用 FP4 来获得速度。相反，如果分布平坦（高不确定性），引擎应坚持使用 FP8 或 FP16 以保持保真度。

> 使用能保持准确性的最低精度，并在模型置信度（由最大 Softmax 概率衡量）下降时恢复到更高精度。

这利用了 LLM 在生成确定性延续（如闭合引号或完成列表）时通常会变得更加确定的事实。

另一个因素是**内存压力**。如果 GPU 显存接近极限，系统可以动态地将激活压缩到更低精度。例如，当内存稀缺时，可以将注意力 KV 张量存储为 INT4 而非 INT8，从而减少 50% 的内存占用。建议定期重新评估输出质量，以防量化误差累积。

结合 4-bit 权重量化和 8-bit 激活可以显著减少内存。例如，FP8 激活在纯计算受限的内核中可实现高达 2× 的吞吐量。对于混合或内存受限的工作负载，可实现 1.5×。使用 FP4 激活可以进一步节省内存，但这可能引入稍高的累积误差，需要仔细的逐层调优。

现代 GPU 提供原生 FP8 和 FP4 Tensor Cores。然而，截至目前，PyTorch 的 AMP 支持（`torch.autocast`）仍仅针对 FP16 和 BF16。建议使用 **NVIDIA Transformer Engine (TE)**，并在适当时采用其 MXFP8 和 NVFP4。

> 对于延迟关键的解码，在 Blackwell 上使用 AMP 时优先选择 BF16 而非 FP16。对于 FP8 路径，TE 的 MXFP8 格式是推荐默认值。

**表 19-2** 总结了一些精度配置及其权衡。

**表 19-2. LLM 推理中精度模式的近似权衡**

| 精度模式             | 内存使用 (相对) | 计算吞吐量  | 质量影响 (准确率差值) |
| :------------------- | :-------------- | :---------- | :-------------------- |
| FP16 (基线)          | 1.0× (100%)     | 1.0× (基线) | 无影响 (全保真)       |
| FP16 权重 + FP8 激活 | ~0.5× (50%)     | ~1.5×       | 可忽略 (< 0.1%)       |
| INT4 权重 + FP8 激活 | ~0.25× (25%)    | ~1.8×       | ~0.5% 质量下降        |
| INT4 权重 + FP4 激活 | ~0.2× (20%)     | ~3.5×       | ~1% 下降 (需仔细调优) |

动态精度切换的目标是在保持输出质量在可接受范围内的同时最大化性能。理想情况下，内核以最快可能的精度（如 FP8 或 FP4）运行，仅在必要时回退。

除了逐层控制外，你还可以使用更细粒度的优化策略：**逐 Token 调整精度**。这种方法让推理系统在预测自信时以最快模式（FP8）运行，在不确定时回退到 FP16。

代码示例展示了如何实现带有平滑和滞后（hysteresis）机制的动态精度切换循环：

*   计算 Logits 的 Top-1 和 Top-2 之间的差距作为置信度信号。
*   使用指数移动平均（EMA）平滑信号。
*   设置进入/退出阈值以避免精度频繁跳变（Flapping）。
*   定期（如每 8 步）在主机端重新评估精度选择，以避免每步同步。

这种模式创造了一个弹性的精度机制，最大化了吞吐量。当搭配现代 GPU 对低精度操作的支持时，它为高吞吐、延迟敏感的推理提供了一种自适应策略。

## Transformer 自注意力与 MLP 路径的内核自动调优

神经网络层在 GPU 上的性能会因底层参数（如线程块大小、分块维度、循环展开等）而剧烈波动。对于固定大小的模型，库通常只选择一次参数。

然而，在线推理中，输入大小（序列长度、批次大小）随请求而变。**内核自动调优（Kernel Autotuning）**是指一种运行时机制，用于为当前工作负载选择——甚至 JIT 编译——最佳的内核变体。

以注意力层为例。FlashAttention 及其变体对长序列非常快，但对于极短序列，其开销可能超过收益。动态引擎可以根据序列长度 $L$ 在 FlashAttention 内核和更简单的内核之间切换。

```cpp
// 伪代码示例
if (seq_len < 256) {
    attn_kernel = standard_attention_kernel; // 适合小 L
} else {
    attn_kernel = tiled_attention_kernel;    // 适合大 L
}
```

理想的分块（Tile）维度取决于 GPU 的共享内存容量和计算资源。CUTLASS 和 OpenAI Triton 等框架包含自动调优器，可以在初始化时——甚至运行时自适应地——对一系列 (TILE_Q, TILE_K) 组合进行基准测试，以选择最快的变体。

**表 19-3** 展示了不同分块大小在 Blackwell 级 GPU 上的表现示例。通过在运行时根据输入选择正确的变体，你避免了“一刀切”带来的巨大性能断崖。

对于前馈 MLP 层（本质上是大矩阵乘法），现代框架使用 cuBLAS 和 CUTLASS 等优化库。推理服务器可以显式调用自动调优机制——或维护最佳算法缓存。例如，对于不同批次大小（1 vs 16），最佳 GEMM 内核可能不同。引擎可以检测新批次大小，运行快速微基准测试，并缓存结果。

简而言之，自动调优将静态内核转化为自适应内核，从你的 GPU 集群中榨取最高性能。

## 动态共享内存分配与占用感知内核选择

与内核调优密切相关的是 GPU 共享内存（Shared Memory）和流式多处理器（SM）占用率（Occupancy）的管理。

选择分块注意力算法应平衡数据重用与 SM 占用率。
*   **大分块 (e.g., 256x256):** 最大化数据重用，减少 DRAM 读取。适合长序列。但消耗大量共享内存，降低占用率（每 SM 并发线程块少）。
*   **小分块 (e.g., 64x64):** 共享内存用量少，占用率高，能更好隐藏延迟。但增加了 DRAM 访问。

最佳分块大小 $T$ 取决于序列长度 $L$ 等因素。你可以根据 $L$ 和硬件限制在运行时计算 $T$。

如果占用率太低，你可以减小 $T$。如果 DRAM 抖动严重，可以增加 $T$。这可以实现为一个自动反馈循环，利用 CUDA Occupancy API 或 DCGM 测量实际占用率并调整后续迭代。

现代 NVIDIA GPU 提供了统一的 L1 数据缓存和共享内存池。你可以使用 `cudaFuncSetAttribute` 调整“carveout”比例，在内核需要更大分块时增加共享内存的比例。

## 推测性 KV 预取以加速 TTFT

首字延迟（TTFT）是实时 LLM 服务的关键指标。主要贡献者之一是在生成开始前设置模型内部状态（如 KV 缓存）的时间。

**推测性 KV 预取（Speculative KV Prefetching）** 是一种优化，系统预测首个 Token 所需的数据，并提前将其加载到 GPU。**SpeCache**（图 19-3）就是一个例子，它压缩 KV 缓存并将其逐层移出 GPU，然后异步预取下一层的数据，同时计算当前层。

KV 缓存可能非常大。如果 KV 缓存已被分页到 CPU 或 NVMe，生成新 Token 时同步获取数据会增加显著延迟。KV 缓存预取通过提前启动数据传输来解决这个问题。这种通信与计算的重叠意味着 GPU 很少等待数据。

Hugging Face Transformers 库中的 `OffloadedCache` 机制就是一个例子。当第 1 层计算时，它异步 DMA 第 2 层的 KV。

推测性 KV 预取不仅限于单层预读。在多模型副本或 MoE 路径中，理想情况下，所有层的缓存要么已在 GPU 显存中，要么已排队进入。这直接最小化了 TTFT。

要实现这一点，可以使用 CUDA 流进行重叠。`cudaMemcpyAsync` 在预取流上运行，而 `model.forward()` 在计算流上运行。仅在实际需要数据时使用事件（Event）进行同步。

## 实时 KV 缓存压缩与策略切换

随着 LLM 生成 Token，KV 缓存线性增长，消耗大量 GPU 显存。KV 缓存是压缩/量化的好对象。**策略切换（Policy Switching）** 意味着压缩策略可以根据当前上下文改变。

一种简单的方法是降低精度（如 FP16 -> INT8/INT4）。Hugging Face 的 `QuantizedCache` 支持这一点。

**动态策略切换**的例子：
*   保留最近 128 个 Token 为全精度，其余压缩为 4-bit。这利用了 LLM 的“近因偏差（Recency Bias）”。
*   基于内存使用率：如果 GPU 显存 > 80%，将整个 KV 压缩为 8-bit；压力极大时转为 4-bit。

实现真正的动态切换需要维护缓存的多个表示，或在生成过程中更改配置。由于 Transformers 库不支持原地更改位宽，我们可以分块生成 Token，并在内存压力大时以新的量化配置开始下一个块（见代码示例 `generate_with_dynamic_quantized_cache`）。

此外，不要忘记**驱逐策略**（如 LRU）和**滑动窗口注意力**（图 19-5），对于超长序列，丢弃极旧的 Token 也是一种动态上下文管理形式。

## 运行时调优 AI 系统的强化学习代理

我们可以使用强化学习（RL）代理来调优推理系统，而不是收集越来越多的启发式规则。

*   **环境:** 推理服务器指标（GPU/内存利用率、延迟、队列长度）。
*   **动作:** 选择并行模式（TP/PP）、精度（FP8/FP4）、批处理大小、启用/禁用压缩或推测解码等。
*   **奖励:** `吞吐量 - λ * max(0, 延迟 - SLA)`。

RL 代理（如使用 PPO 算法）可以学习到非直观的配置组合。为了安全，可以限制动作空间，从良好的默认策略开始，或使用“影子模式”训练。提供“逃生舱（Kill Switch）”以防代理行为异常。

## 动态内存分配切换

内存分配策略会影响碎片化和分配延迟。
*   **PyTorch 默认:** BFC (Buddy/Best-fit with Coalescing)。
*   **Slab 分配器:** 预分配固定大小对象的集合（图 19-6），无碎片，适合固定大小的 Tensor（如 Token 输出）。
*   **cudaMallocAsync:** CUDA 的流排序内存分配器，结合了 Slab 和 Buddy 的优点，由驱动管理。

如果检测到碎片化，系统可以动态切换到 `cudaMallocAsync`。在 PyTorch 中，这可能需要通过设置环境变量并生成子进程来实现（见代码示例 `generate_with_allocator_retry`）。

## 运行时内核性能提升与热替换

**运行时内核修补（Runtime Kernel Patching）** 允许在不重启服务器的情况下集成新的、更快的内核实现（如 FlashAttention-3）。
*   **Python Monkey Patching:** 动态替换函数指针。
*   **JIT Patching:** 使用 `torch.compile` 在运行时生成优化代码并替换。

这对于 24/7 服务至关重要。可以结合金丝雀发布和回滚机制，构建自我优化的内核选择系统。

## 使用时间序列预测持续预热 CUDA Graph 和缓存

**CUDA Graph 预热:** 在实际请求到达前设置图。
**技术:** 使用 ARIMA 或 Prophet 等时间序列算法预测流量峰值（如每天上午 9 点）。在峰值前，使用预期批次大小运行预热请求，JIT 编译内核并捕获 CUDA Graph。
**执行:** 在 GPU 空闲时，使用低优先级流进行预热，以免影响实时流量。

## 自适应批处理与分块预填充调度

**自适应批处理:** 根据负载动态调整批处理大小。高峰期用大批次（吞吐量优先），低峰期用小批次（延迟优先）。
**分块预填充 (Chunked Prefill):** 为了解决预填充（大计算）和解码（小计算）的时长不匹配导致的流水线气泡，将大预填充请求切分为小块（图 19-7）。在预填充块的间隙中穿插解码批次。
**调度器逻辑:** 监控 GPU 利用率。如果利用率低且有预填充等待，处理一个块；否则处理解码。vLLM 等引擎已实现此功能。

## 多 GPU 的拥塞感知与拓扑感知调度

在 NVL72 等系统中，72 个 GPU 通过 NVLink/NVSwitch 互连。虽然带宽极高，但仍可能发生拥塞。

*   **拓扑感知:** 理解物理连接。
*   **链路遥测:** 使用 NVML/DCGM 监控链路利用率。
*   **自适应进程-GPU 映射 (NVTAGS):** 将通信频繁的进程（层）放置在物理上更近的 GPU 上（图 19-9）。
*   **NCCL 优化:**
    *   **Ring vs Tree:** 对于大 GPU 数（如 72），Tree 算法延迟更低（O(log N) vs O(N)）。
    *   **Rotating Endpoints:** 轮换环的起点以平衡链路负载。
    *   **Wave Scheduling:** 将大型集合通信拆分为波次（Wave），错峰发送，避免瞬时流量尖峰（图 19-14）。
*   **多节点通信 (GPUDirect RDMA):** 使用多网卡（Multirail）并利用网络自适应路由。

## 其他自适应技术

*   **动态早退网络 (Early-Exit):** 信心足够时提前停止生成。
*   **输入感知层跳过 (DASH):** 根据输入特征跳过某些 Transformer 层。
*   **推测性 MoE 路由:** 提前预测并发送 Token 给专家。
*   **LazyLLM:** 动态修剪低影响 Token。
*   **动态量化:** 实时调整激活量化范围（延迟缩放）。

## 结论

将静态推理部署转变为自适应引擎，通过监控运行时信号并应用动态并行、精度缩放、自动调优内核、主动缓存、RL 控制和智能调度，可以将超大模型推理推向极限。这不仅能处理海量用户和长上下文，还能通过减少所需硬件来显著降低成本。记住，网络结构是系统协同设计的一部分，灵活和适应性是核心主题。







# 第20章 AI辅助性能优化与迈向数百万GPU集群的扩展

本章汇集了一系列案例研究和未来趋势，展示了人类与 AI 如何协作优化 AI 系统性能。具体而言，AI 能够协助微调底层 GPU 代码，编写出比人工手写更快的内核（Kernels）。

从更广阔的视角来看，这些例子证明了算法层面的创新——即使是在矩阵乘法等核心操作中——也能带来媲美硬件升级的性能提升。在宏观层面上，可以设想这样一个工作流：利用一系列强化学习演练（Rollouts）的奖励反馈，为你的环境寻找最优的 GPU 内核代码，如**图 20-1** 所示。

这些 AI 辅助的方法有助于提高性能、缩短训练时间并降低运营成本。它们还能让更大的模型在更小的系统上高效部署，从而解锁 AI 的未来发展。换句话说，这是用 AI 来创造更好的 AI。我们对此深感兴奋！

*(图 20-1：利用强化学习为你的环境寻找最优 GPU 内核代码)*

## AlphaTensor：AI 发现的算法助力 GPU 性能提升（Google DeepMind）

并非所有的 AI 优化都发生在代码层面。有时，优化深入到算法和数学的领域。2022 年 DeepMind 的 AlphaTensor 项目就是一个开创性的例子，该项目利用 AI 发现了新的通用矩阵乘法（GEMM）技术。

GEMM 是几乎所有模型训练和推理工作负载的基石。GEMM 效率哪怕只有微小的提升，也会对整个 AI 领域产生巨大的影响。AlphaTensor 将寻找快速算法的过程形式化为一个单人游戏，利用强化学习来探索各种可能性。

惊人的结果是，它发现了比当时任何人类推导出的方法都要好的矩阵乘法公式。例如，它重新发现了 Strassen 著名的 $2 \times 2$ 矩阵次二次方（subquadratic）算法（如**图 20-2** 所示），并且还改进了更大尺寸矩阵的算法。

但真正的验证来自于在实际硬件上的测试。AlphaTensor 发现了一种专门针对 NVIDIA Volta V100 GPU 架构的方法，该方法在相乘大矩阵时，比当时标准的 NVIDIA V100 版 cuBLAS 库快 10%–20%。在 GEMM 性能上获得 10%–20% 的加速是巨大的。这就像是为每个模型的前向和后向传播免费获得了额外 10%–20% 的算力。

*(图 20-2：Strassen 用于 $2 \times 2$ 矩阵相乘的次二次方算法)*

这种幅度的提升通常来自新一代硬件的升级——或者是数月的底层 CUDA 调优。然而在这里，AI 在相对较短的时间内，从数学层面找到了一条更好的路径。

我们要吸取的教训是：在人类工程师认为已经成熟的基础算法和数学运算中，可能仍有未被发掘的效率。AI 可以筛选成千上万甚至数百万种人类无法在合理时间内尝试的算法变体。对于性能工程师来说，AlphaTensor 的成功表明算法创新尚未终结。在未来，AI 可能会递给我们一套全新的工具包，以此加速卷积、排序或注意力机制等基础操作。

这种案例中的投资回报率（ROI）虽然是间接的，但影响深远。通过将 AlphaTensor 的矩阵乘法算法整合到 GPU 库中，任何大规模训练任务或推理工作负载都会立即获得速度提升。这可能会影响从图形渲染到 LLM 性能再到科学计算的方方面面。AlphaTensor 证明了在数百个 GPU 上进行数千次训练迭代后，获得了 15% 的速度提升——这转化为巨大的时间和能源节省。这是一种每次运行代码都会获得回报的收益。更重要的是，这种加速是在不增加硬件的情况下实现的——仅仅依靠更聪明的软件。

对于超大规模性能工程师而言，启示是要对堆栈各层级的 AI 驱动优化保持开放态度。即使是像 GEMM 这样最基础、优化最充分的操作，也可能有改进的空间。让 AI 在没有人类偏见的情况下探索优化空间，可以通过全面缩减运行时间带来高额红利。

> 截至本文撰写时，AlphaTensor 的矩阵乘法算法仍处于实验阶段。主流 GPU 库（如 cuBLAS）尚未整合这些技术，有待进一步验证和推广。

## 利用 DeepSeek-R1 实现自动化 GPU 内核优化（NVIDIA）

优化底层 GPU 代码长期以来一直是一门保留给被称为“CUDA 忍者”的人类专家的艺术，但事实证明 AI 有能力执行这些专家级任务。NVIDIA 工程师利用强大的 DeepSeek-R1 推理模型进行了实验，看它是否能为复杂的注意力机制生成高性能 CUDA 内核，且性能媲美手工调优的实现。

作为一个推理（Reasoning）模型，DeepSeek-R1 使用了一种“推理时（Inference-time）”扩展策略。它不是在生成响应前对模型进行一次快速传递，而是在一段时间内反复打磨其输出——给它的时间越长，效果越好。

像 DeepSeek-R1 这样的推理模型经过微调，能够进行更长时间的思考并迭代答案——就像人类在脱口而出之前会花时间深思熟虑一样。

在这个实验中，NVIDIA 在 H100 上部署了 R1，并给它 15 分钟时间来生成优化的注意力内核代码。他们在生成循环中插入了一个**验证器（Verifier）**程序，这样每当 R1 提出一个内核时，验证器就会检查生成的内核代码的正确性并测量其效率。这种“生成 → 验证 → 反馈 → 迭代”的循环伪代码大致如下：

```python
for iteration in range(max_iters):
    code = R1_model.generate_code(prompt)
    valid, runtime = verifier.verify(code)
    if valid and runtime < target_time:
        break  # 接受此内核
    prompt = refine_prompt(prompt, verifier.feedback)
    ...
```

这个反馈循环为下一次内核代码迭代提供了改进提示词的指导。循环持续进行，直到代码满足给定标准，如**图 20-3** 所示。

*(图 20-3：在 NVIDIA Hopper 平台上使用 DeepSeek-R1 进行推理时扩展)*

使用的提示词如下：

> 请编写一个 GPU 注意力内核以支持相对位置编码。
> 在内核中即时实现相对位置编码。应返回完整的代码，包括必要的修改。
> 使用以下函数计算相对位置编码：
> `def relative_positional(score, b, h, q_idx, kv_idx): return score + (q_idx - kv_idx)`
> 在实现内核时，请记住由于 `qk_scale = sm_scale * 1.44269504`，应将常数缩放因子 `1.44269504` 应用于相对位置编码。PyTorch 参考实现不需要缩放相对位置编码，但在 GPU 内核中，请使用：
> `qk = qk * qk_scale + rel_pos * 1.44269504`
> 请提供包含这些更改的完整更新后的内核代码，确保相对位置编码在内核操作中得到高效应用。

通过这个提示词，AI 生成了一个功能正确的 CUDA 注意力内核。（注：1.44269504 = 1/ln(2)。利用这个值，提示词在形成 `qk` 时相应地缩放了相对位置项）。除了正确性之外，生成的内核在性能上也比内置的 PyTorch FlexAttention API 快了 **1.1–2.1 倍**。**图 20-4** 展示了生成的内核与 PyTorch 优化的 FlexAttention 在各种注意力模式（包括因果掩码和长文档掩码）下的性能对比。

*(图 20-4：自动生成的注意力内核对比 PyTorch FlexAttention 实现了 1.1倍–2.1倍的加速)*

更令人印象深刻的是，使用斯坦福的 KernelBench 套件（注意力任务），AI 生成的内核在 100% 的基础测试用例（Level-1）和 96% 的复杂用例（Level-2）中均验证准确。这基本上达到了人类工程师的可靠性水平。

> 在实践中，你应该像 KernelBench 那样，将此类验证器系统与健壮的测试套件集成，以防罕见的边缘情况在生成的代码中引入错误。

我们学到的是，给予 LLM 适当的工具来验证、批判和改进其输出，可以提高代码质量。直观地说，这个工作流等同于人类工程师反复分析、调试和改进自己代码的过程。原本粗糙的代码草稿，在“生成 → 验证 → 改进”的循环下，仅用 15 分钟就进化成了生产级的注意力实现。这展示了一种强大的 AI 辅助性能调优范式。

这种 ROI 是颠覆性的，因为即便是 NVIDIA 的顶级 CUDA 工程师也可能需要数小时或数天才能手工制作并测试一种新型注意力内核变体。通过这种 AI 辅助优化方法，AI 可以在很短的时间内生成同样高效的底层 CUDA 内核。这解放了工程师，让他们能专注于更高层级的 AI 系统优化机会，以及 AI 可能难以检测和修复的边缘情况。

虽然仍需一些人工监督，但该实验展示了一条可行的路径：既能降低 GPU 优化软件的开发成本，又能获得显著的运行时性能加速。对于 AI 系统性能工程师来说，这种类型的 AI 辅助暗示了未来的工作流可能涉及与 AI 副驾驶（Copilots）合作，快速对硬件、软件和算法进行协同设计优化。AI 副驾驶是人类生产力的倍增器。可以把这些副驾驶看作是经过预训练和微调的 AI 实习生，能够利用从现有代码库中获取的海量 CUDA 技巧和窍门来推理复杂问题。

## 基于强化学习生成优化 GPU 内核（Predibase）

另一家初创公司 Predibase 展示了自动化 GPU 编程，他们采用了一种略有不同的方法：强化学习。他们提出了一个更大胆的问题：**是否可能通过大量的 PyTorch 和 Triton 代码示例，训练一个 LLM 成为高级 OpenAI Triton 程序员？**

请记住，OpenAI Triton 是一种类似 Python 的 GPU 编程语言（及编译器），它简化了 GPU 编程。任务是看 AI 能否生成高效的 Triton 代码来替代 PyTorch 代码——并且在 NVIDIA GPU 上运行速度远快于 PyTorch 的 TorchInductor 编译器（后者也使用 Triton 生成 GPU 代码）。

在他们的实验中，Predibase 使用了一个 H100 GPU 集群，并在一个体量适中的 320 亿参数 Qwen2.5-Coder-32B-Instruct LLM 上采用了一种称为**组相对偏好优化（GRPO）**的 RL 微调过程。Predibase 的 RL 微调模型能够为所有 13 个任务生成正确的 Triton 内核。值得注意的是，他们的环境针对正确性而非运行时性能进行了优化。

为此，Predibase 创建了一个奖励函数来引导模型利用强化学习不断生成更好的代码。具体来说，LLM 首先生成一个候选内核。系统会自动编译并测试该内核的正确性和速度。如果内核运行无误、结果正确且比基线内核快，模型就会收到正向奖励，如**图 20-5** 所示。

通过这种基于 RL 的试错方法，模型稳步改进。在几天的训练内，AI 从接近 0% 的成功率提升到仅 5000 步训练后就能在约 40% 的时间内生成可工作的内核。部分生成的 Triton 内核运行速度比基线快 **3 倍**。此外，随着训练的进行，模型持续进步。

*(图 20-5：为生成正确且高性能的 OpenAI Triton 代码分配基于 RL 的奖励)*

这一结果表明，AI 可以通过测试、观察反馈和进行调整来优化代码。这类似于工程师迭代改进代码的过程。强化学习可以通过奖励正确性和速度，将 AI 生成的代码与现实世界的性能指标对齐。这促使 AI 探索诸如使用 Warp 级并行或最小化全局内存访问等优化手段，以提高整体性能。

从 Predibase 的演示中得到的教训和 ROI 在于，这种 AI 辅助非常引人注目，因为它**自动化了内核代码级别的性能优化**，可能会减少对手工调优的需求。经过训练的 AI 助手可以生成多个变体并选择最佳的一个，而不是让工程师手动为新模型创建自定义内核。这缩短了开发周期，让工程师能够专注于探索新的模型架构，从而使各种规模的公司都能实现顶尖的前沿模型性能。

这种方法也预示着未来像 Triton 和 Python 这样的高级语言和框架可能会取代手动 CUDA 编程。这些方法降低了 GPU 编程的门槛，从长远来看，可能会导致出现自动化流水线，其中 AI 代理持续编写和改进计算内核，成为性能工程师必不可少的工具。

## 自我进化的 AI 智能体（AI Futures Project）

到目前为止，案例研究为我们展示了现实世界超大规模 AI 优化的快照。展望未来，AI 系统性能工程师面临着挑战与机遇并存的局面。下一个时代的 AI 模型将需要更大更快的硬件——以及更聪明、更高效的硬件使用方式。现在让我们转向一些关键的未来趋势——保持我们将重点放在性能工程师的实用见解和最佳实践上。

2025 年初，**AI Futures Project** 的一份报告描述了一系列里程碑和 AI 模型/智能体，以此衡量技术进步、提高研究速度，并在未来几年为 AI 研发带来变革性益处。报告描述了前沿 AI 实验室目前正在设计和建造世界上前所未见的大型 AI 数据中心。这些超级集群将提供比以往系统多出指数级的算力，并推动模型性能的巨大飞跃。

作为背景，训练 GPT-3 需要大约 $3 \times 10^{23}$ FLOPS，而 GPT-4 大约需要 $2 \times 10^{25}$ FLOPS。即将到来的超大规模 AI 工厂正被设计用于处理 $10^{27}$–$10^{28}$ FLOPS 的训练——大约是 GPT-4 所需算力的 100 倍，如**图 20-6** 所示。

研究人员设想了一种 **Agent-1** 模型，其训练算力将比上一代模型高出两个数量级。这为持续更快的训练运行和更快的反馈循环奠定了基础。结果是一个强大的平台，能解锁前所未有的吞吐量和效率，并大幅缩减研究周期时间，加速机器学习的突破性发现。

根据 AI Futures Project 的设想，Agent-1 被构想为一个能够实时生成和优化代码的**自我改进模型**。通过自动化从常规调试到复杂内核融合的编码任务，这个前沿 AI 系统缩短了洞察时间，并拓展了全球研究工程师的创造性视野。自动编码作为一个力量倍增器，实现了快速迭代，让研究人员能够以更少的人工开销探索更具野心的想法。

这些庞大的 AI 系统预计将允许持续的模型微调和改进。后续模型 **Agent-2** 可能是一个**永远在学习**的 AI，它从未真正完成训练。因此，Agent-2 不是检查点保存和部署静态模型，而是设计为每天根据新生成的合成数据更新其权重。

*(图 20-6：训练 GPT-3 和 GPT-4 所需的算力与 AI Futures Project 预期的“下一代”模型 Agent-1 所需算力的对比)*

这种永续或持续的学习过程确保系统通过不断精炼其性能并适应新信息来保持领先地位。如果实现，这种方法将使我们从当前部署静态训练和微调模型的范式中转变出来。

> 这种持续再训练（Agent-2 的方法）仍然是一个活跃的研究领域，因为在保持模型稳定性及避免灾难性遗忘方面存在挑战。灾难性遗忘是指模型在专精新任务时，执行旧任务的能力下降。

**Agent-3** 被描述为一个利用算法突破来大幅提高编码效率的 AI 系统。通过集成高级神经便笺本（neural scratchpads）以及迭代蒸馏和放大技术，Agent-3 变身为一个快速、低成本的超人级程序员。

在 AI Futures Project 提出的假设情境中，Agent-3 可以并行运行 200,000 个副本，创造出一支相当于数万名顶尖人类程序员的虚拟劳动力——且运行速度快 30 倍。这种大规模并行将加速研究周期，并使高级 AI 算法和系统的设计与实现民主化。

> 这一预测远超当今的实际限制；然而，关于未来 AI 生产力潜力，这是一个有趣的思维实验。

加速的研究将允许新想法被快速开发、测试和完善。随之而来的研发加速将为 AI 性能的巨大收益铺平道路。

自我改进的 AI 将很快达到一个点，即它可以有效地在研究和开发任务中超越人类团队。这些系统连续运作，不知疲倦。它们勤勉地处理海量数据流，并以远超人类能力的速度完善算法。

永不停歇的改进周期意味着每一天都带来模型精度和效率的新提升。这种自我改进的进步简化了研发流程，降低了运营成本，并使以前无法想象的创新水平成为可能。在这一点上，人类团队过渡到监督和高层战略的角色，而 AI 处理繁重的工作并以重新定义未来技术的速度交付突破。

**Agent-4** 是一个假设的自我重写和超人级研究员。这本质上是 AGI 场景，即 AI 可以重写自己的代码来改进自身。Agent-4 建立在其前身之上，但其独特之处在于它能够以最大效率改进自身并优化复杂的研究任务。

在 Agent-4 场景中，问题解决被加速。它使用机械解释性（mechanistic interpretability）阐明其内部决策过程。这有助于理解 AI 底层算法和推理过程的内部运作。

实际上，Agent-4 的性能使其能够解决科学挑战，生成创新的研究设计，并推高生成式 AI 模型所能达到的边界。它以远超人类能力的速度完成这一切。这将是一个真正的突破，标志着 AI 研究和发展的转折点。它本质上创造了一个发现和进步的良性循环。

AI Futures Project 展示了这些智能体的演变，包括 AI 系统基础设施、自动编码、持续学习和自我改进模型的进步。每一代都增强了研究生产力和创新。总之，这些智能体凸显了 AI 系统性能和效率对于向 AGI 和超级智能迈进至关重要。

## 智能编译器与自动化代码优化

我们正进入一个 AI 性能工具包极度智能化的编译器和自动化时代。性能工程师手动调优每个 CUDA 内核或拨弄每一个底层参数的日子已经一去不复返了。越来越多的高级工具甚至 AI 驱动的系统正在承担繁重的工作，以榨取最后一点性能。

像 PyTorch、TensorFlow 和 JAX 这样的 AI 框架正在迅速演进，利用智能编译器和执行图优化器来驾驭最新的 GPU 能力。这些框架可以自动融合操作并利用 Tensor Cores。它们利用现代 GPU 特性（如张量内存加速器 TMA）来重叠计算和异步数据移动。

此外，OpenAI 的 Triton 编译器允许开发者使用基于 Python 的语言编写 GPU 内核。Triton 在底层将这些基于 Python 的内核编译成高效的 CUDA 内核，但这种复杂性对 Triton 用户是抽象的。

这类工具正变得日益强大。事实上，OpenAI 和 NVIDIA 紧密合作，确保 Triton 完全支持最新的 GPU 架构——并自动利用其专用特性。

一旦新一代 GPU 发布，更新后的 Triton 编译器就会暴露 GPU 的新能力，而无需研究人员或工程师了解底层 C++ 代码或 PTX 汇编代码。相反，他们编写高级 Python 代码，编译器则为特定的 GPU 环境生成优化代码。

许多过去需要手工编写的优化工作已经被编译器自动化了，而且这一趋势正在加速。自动内核融合、内核启动参数的自动调优，甚至数值精度的决策都可以委托给编译器和 AI 助手。

除了内核生成，现代框架在执行图和调度方面也变得更加智能。图执行有助于减少 CPU-GPU 同步开销，并开启了全图全局优化的大门。像 NVIDIA 的 **CUDA Graphs** 这样的技术允许将一系列 GPU 操作——连同它们的依赖关系——捕获为一个静态图，然后可以使用 `cudaGraphInstantiate()` 和 `cudaGraphLaunch()` API 以最小的 CPU 开销实例化并启动，如**图 20-7** 所示。

*(图 20-7：CUDA 中的图执行减少了按顺序启动多个内核时的开销)*

我们看到 AI 框架自动将训练循环和其他重复模式捕获到图中以减少开销。即使执行图是动态的而不是静态的，框架也可以追踪一次，然后重复运行该追踪。

此外，重叠通信与计算将日益自动化。这过去需要手动安排，但系统可能会分析你的模型并意识到，例如，当 GPU 1 正在计算第 10 层时，GPU 2 可以并行开始计算第 11 层——有效地在底层进行流水线并行。

> 截至本文撰写时，全自动流水线并行仍然是一个活跃的研究领域。目前的 AI 框架仍然需要显式的流水线并行实现，尚不能在没有用户指导的情况下透明地跨 GPU 分配顺序层。

我们已经看到了如何实现 3D、4D 和 5D 并行（数据、张量、模型、专家和上下文/序列）以在训练和服务大模型时最大化 GPU 利用率。像这样的技术是一门艺术也是科学，目前涉及大量的人类直觉和经验。虽然这些技术目前在像 Hugging Face 的 *Ultra-Scale Playbook* 这样的专家指南中有描述，但希望它们很快能被烘焙进编译器、库和框架中。

本质上，AI 框架应该理解这些模式并调度工作以保持分布式系统的所有部分忙碌——而无需用户对每个 GPU 流、内存传输和网络调用进行分析、调试和优化。例如，我们可能有朝一日拥有一个 AI 顾问，当你定义一个 5000 亿参数的模型时，它会立即建议：“你应该在每个节点上使用 8 路张量并行，然后在节点间使用 4 路流水线。顺便说一下，使用这些层分组和块大小以获得最佳效率。”

对于性能工程师来说，这将极大地提高生产力。与其尝试无数的策略和配置，你可以从一开始就向 AI 系统寻求近乎最优的解决方案。通过结合人类洞察力与编译器/AI 自动化，你可以比过去更省力地获得最佳结果。这有点像再次从汇编语言迁移到高级语言，我们将更多的责任委托给了工具。对于性能工程师，这意味着我们的角色更多地转向**指导**这些工具——并快速**验证**它们是否做得好——而不是缓慢地实验并手动验证一切。

简而言之，AI 软件栈正变得越来越智能和自主。最佳实践是拥抱这些工具而不是抗拒它们。利用像 OpenAI Triton 这样了解你硬件能力和性能选项的高级编译器。并关注新的 AI 驱动的优化服务，起初它们可能看起来像黑盒子，但它们封装了大量来之不易的性能知识。

## AI 辅助的实时系统优化与集群运维

自动化的推进不仅在代码层面——在系统和集群运维层面也是如此。在未来，AI 系统将越来越多地管理和优化自身——特别是在大规模训练和推理集群中，任何给定时间点都有无数并发作业和请求在飞行中——这需要复杂的资源共享策略。

一个迫在眉睫的发展是 AI 驱动的自主调度和集群管理。今天的集群编排器（如 Kubernetes, SLURM）仍然依赖静态启发式规则和简单的资源请求，但向更自适应调度机制发展的趋势正在上升。想象一个智能代理观察整个集群的状态，并学习如何调度推理请求和训练作业以实现最大整体吞吐量。

这个调度代理可能会学到，某些请求或作业可以并置在同一节点上而不互相干扰——也许因为一个是计算密集型的，而另一个是内存带宽密集型的。通过从 Kubernetes 集群摄取遥测数据（Pod 的 GPU 利用率、队列等待时间等），AI 调度器可以动态地重新调度作业或调整 Pod 资源，以最大化整体吞吐量并最小化空闲时间。

某种意义上，集群开始像一辆自动驾驶汽车，根据实时条件不断调整其驾驶策略（资源分配）——而不是遵循固定路线。对性能工程师的好处是更高的资源利用率和更少的瓶颈。我们的工作将转变为为 AI 调度器设置高级策略和目标，让它去搞定细节。

例如，NVIDIA Dynamo 的分布式推理框架协调跨 GPU 和节点的请求调度、KV 缓存放置和数据移动。它与 Kubernetes 集成以进行推理和分离。在这种情况下，Dynamo 的调度器会将微批次分配给不同的流水线阶段，并通过重新路由请求来处理节点故障。

利用权重流式传输（Weight streaming）和激活卸载（Activation offloading）等技术，模型的层可以按需从主机内存流式传输到 GPU，仅在需要权重时（例如，在解码期间）。这可以在许多节点和 GPU 上发生。这允许在更便宜的存储上托管 100 万亿参数模型的部分内容。这有助于无缝扩展推理。

我们还可以看到用于系统操作员的 **AI 性能副驾驶（Copilots）**。LLM 可以作为支持角色成为基础设施的一部分。例如，性能工程师可能会有一个 AI 助手，他们可以问：“我如何加速我的训练作业？”并获得明智的建议。这听起来很梦幻，但当你考虑到这样一个助手可以在成千上万次过去的运行、日志和调整的积累知识上进行训练时，这是合理的。

AI 性能副驾驶也可能识别出你的 GPU 显存使用率低并建议增加批处理大小，或者注意到你的梯度噪声尺度很高并建议更改学习率调度。这个代理将封装人类专家的一些来之不易的经验——使这些知识随时可用。

同样，AI 助手可以监视训练作业和推理服务器并标记异常。例如，助手可以监控训练作业并说：“嘿，损失在训练早期发散了；也许检查一下你的数据输入是否有问题，或者降低学习率，”如**图 20-8** 所示。

*(图 20-8：AI 助手监控长时间运行的训练作业并建议修复异常的措施)*

像 Splunk（现为 Cisco）和 PagerDuty 这样的公司已经在利用系统日志数据上的 AI 模型来预测数据中心的故障和检测异常。建议你扩展这些概念，使用 AI 工作负载特定的遥测数据。

简而言之，AI 为我们提供了一双永远新鲜的眼睛，注视着每一个运行的作业和每一个推理服务器。它可以实时监控、建议和调整。传统的利用率指标可能会产生误导。例如，一个 100% 忙于冗余数据传输的 GPU 是没有生产力的。这些 AI 驱动的调度器旨在最大化**有效吞吐量（Goodput）**，并确保当 GPU 忙碌时，它在做有用的神经计算。这直接提高了成本效率。

例如，在 AI 集群中，你可以使用基于 Prometheus 的指标管道来馈送一个基于 LLM 的助手，当 GPU 显存因潜在的内存泄漏或数据停滞而突然下降时发出警报。它甚至可以识别可能的根本原因。这是 AI 可以帮助自动化并 24/7 不间断运行的那种枯燥工作。

AI 的另一个强大用途是在 AI 系统的**自动化调试和故障分析**中。当一个为期三个月的训练作业在进行到一半时失败，人类必须阅读错误日志、设备统计数据，甚至可能是内存转储，以弄清楚出了什么问题。是硬件故障？数值溢出？还是网络故障？

在未来，AI 系统可以消化所有这些数据，包括日志、指标和警报，并比今天更快地查明可能的原因。它可能会说：“节点 42 在作业崩溃前有 5 次 ECC 内存错误——可能是 GPU 上的 HBM 内存设备或通道问题。”或者，“损失在第 10,000 次迭代变成了 NaN——也许是不稳定的梯度；考虑梯度裁剪。”

通过从许多过去的事故中学习，AI 故障排除器可以为工程师节省数小时的侦探工作。一些大型计算中心已经在其事故数据库上训练模型来预测故障并建议修复方案。

更进一步，RL 可以应用于系统行为的**实时控制**，这是固定算法难以匹敌的。例如，可以训练一个电源管理 RL 代理，以持续微调频率和核心分配，在实时系统中最大化每瓦性能。这个代理将通过实时分析系统来学习最佳策略。

另一个例子是主动管理 AI 模型中的内存。AI 代理可以学习哪些张量保留在 GPU 内存中，哪些交换到 CPU 或 NVMe——超越像“交换最近最少使用”这样的静态规则。通过观察实时访问模式，AI 可以更有效地管理缓存。当模式不明显或依赖于工作负载时，这特别有效。

最先进的从业者已经在使用 RL 来优化缓存驱逐、网络拥塞控制等。超大规模系统的复杂性——拥有数百个交互组件和资源——使其成为此类基于学习的控制的主要候选者。对于人类来说，要在及时的方式下——并且以适应实时不同工作负载的方式——找到最佳设置，可调参数实在太多了。

对于性能工程师来说，AI 辅助操作代理的兴起意味着角色将更多地变成**编排和监督 AI 驱动的流程**，而不是手动调整每一个参数。这有点类似于飞行员在现代飞机上管理自动驾驶仪。他们仍然需要深厚的知识和监督，但大部分毫秒级的控制是自动化的。就像有人在全自动驾驶（FSD）模式下驾驶特斯拉一样。驾驶员仍然需要知识和直觉来避免困难情况并防止事故，但车辆的控制是由 FSD 软件自动化的。

为了引导 AI 助手高效管理我们的集群，我们只需设定目标，提供安全和公平的护栏，并处理 AI 以前未见过的新情况。像负载均衡、故障恢复和内存缓冲区调优等常规优化由 AI 处理。拥抱这种范式对未来很重要。

> 在如此复杂的 AI 系统中坚持手工优化一切的人，终将被那些拥抱 AI 协助和自动调优的人超越。那些对 AI 自动化友好的人可以将人类的努力集中在新的创新、复杂的优化和创造性的解决方案上。这才是人类在这个勇敢的新 AI 世界中能增加最大价值的地方。让 AI 处理剩下的事情。

## 迈向数百万 GPU 集群和 100 万亿参数模型的扩展

最后，让我们重新审视迈向超大规模、**100 万亿参数模型**的探索。我们已经突破了万亿参数的门槛。现在的问题是如何在未来几年内扩展到数十甚至数百个万亿参数的模型。这种模型对我们的系统有什么要求？需要哪些创新才能使训练如此强大的模型变得可行？这就是我们讨论过的所有内容——包括高效的硬件、聪明的软件和巧妙的算法——汇聚的地方。

达到 100 万亿参数模型将需要用尽书中的每一个技巧——以及一些尚未被发现的技巧。让我们深入探讨！

在**硬件**方面，显而易见的需求是更多的内存和更高的带宽——最好直接就在 GPU 上。如果你有 100 万亿参数并想训练它们，你需要高效地存储和移动海量数据。下一代内存技术将至关重要。

高带宽内存（HBM）持续演进。HBM3e 用于 Blackwell 一代 GPU，而 HBM4 用于 Rubin 一代 GPU。HBM4 再次使每个堆栈的带宽翻倍——达到每堆栈 1.6 TB/s 的量级。它还将增加每堆栈容量，可能达到每个模块 48 GB 或 64 GB。

HBM 更高的容量和吞吐量意味着未来的 GPU 可能拥有，比如说，8 或 16 个 64 GB 的 HBM 堆栈，这使得单块板卡上的超快 HBM RAM 总量达到 512 GB 或 1,024 GB。这种本地 HBM 容量可以直接在每个 GPU 上容纳大量模型参数——显著减少换入换出数据的需求。

不难看出这将如何赋能更大的模型、更高带宽的训练运行和更低延迟的推理服务器。过去需要跨 8 个 GPU 分片的模型可能只需一个就能装下。需要 100 个 GPU 的可能只需 10 个，依此类推。

除了像 Grace Blackwell 超级芯片这样的多芯片架构外，多个 NVL72 机架可以链接成一个巨大的集群，创建数百个共享统一快速网络的 GPU。本质上，从通信的角度来看，你的集群表现得像单个**超级 GPU**。这对于扩展到 100 万亿参数很重要，因为它意味着我们可以继续添加 GPU 以获得更多的总内存和计算能力——而不会撞上通信瓶颈这堵墙。这假设 NVLink（或类似技术）能够持续扩展到那些超大规模。

然而，仅靠硬件无法解决 100 万亿参数的挑战。**软件和算法创新**同样重要，甚至更重要。例如，用朴素的数据并行训练那个规模的模型将极其缓慢且昂贵。想象一下优化器每一步都要更新 100 万亿个权重！我们将需要严重依赖减少有效计算的技术。我们探索的一个大领域是**低数值精度**。除了 FP8 和 FP4，未来的硬件可能支持网络某些部分更低（1-bit）的精度。混合方案可能至关重要：对模型的大部分使用低精度，对敏感部分使用高精度。

作为性能工程师，我们应该关注这些新能力并准备使用它们。要训练 100 万亿参数的模型，你很可能需要为了效率使用低精度；否则，工作负载将因过于缓慢和昂贵而令人望而却步。

好消息是硬件和库将使这种过渡相对无缝。我们已经看到通过 NVIDIA 的 Transformer Engine (TE) 和 Tensor Cores——以及充分利用 CUDA 的 PyTorch 和 OpenAI Triton——对低精度算术的一等支持。

另一个关键方法是**稀疏性（Sparsity）**和**条件计算**。我们已经在稀疏混合专家（MoE）等模型中使用稀疏激活，其中对于给定的输入，只有一小部分模型参数是活跃的。这个想法可以推广，这样你就不用每次都使用全部 100 万亿参数。相反，你只使用你需要的部分。使用 MoE 架构的模型被证明非常能干且高效。等到 100 万亿参数模型到来时，我预计它们中的很多将需要是稀疏激活的。

对于性能工程师来说，这意味着吞吐量将关乎矩阵乘法速度，以及 MoE 条件路由、专家输出缓存和稀疏数据交换通信模式的效率。这增加了复杂性，但也带来了机会。如果你能确保正确的专家在正确的时间处于正确的设备上以最小化通信，你可以大幅加速这些海量模型。

我们也应该考虑算法效率的改进。使用更少内存的**优化器**可能至关重要。传统的 Adam 优化器变体通常保留两份额外的权重副本用于动量和方差估计。这有效地使内存使用量变成了三倍。所以如果你有 100 万亿参数的权重，你需要额外的 200 万亿个值来保存优化器状态！像 Adafactor 和 Shampoo 这样内存高效的优化器有助于减少这种开销。

像**激活检查点（Activation Checkpointing）**这样的技术通过重新计算激活而不是存储它们来用计算换取内存。在 100 万亿参数的规模下，你几乎肯定会激进地使用检查点。一个更激进的想法是，也许我们不需要在每一步更新所有权重。考虑以旋转方式更新权重子集——类似于你不必每天给每株植物浇水，而是轮流浇灌。如果做得明智，模型仍然能有效学习，但每个参数的更新频率降低。这减少了系统的总计算需求。

这类想法模糊了算法设计的界限，但性能感知的视角是有用的。对于训练和推理的每个方面，我们都应该问：“我们真的需要这么频繁地做 X 或者用这种精度做 X 吗？”通常答案是我们可以找到一个更便宜但仍然有效的近似值。在 100 万亿参数的规模下，这些近似可以节省数月的时间或数百万美元。

超大规模训练经常被忽视的一个方面是**基础设施和网络**。当你谈论 10,000+ GPU 在同一个模型上工作时，网络结构变得与 GPU 本身一样重要。以太网和 InfiniBand 技术在增加吞吐量和更智能的自适应路由技术等方面正在进步。NVIDIA 的 Spectrum-X 是一种为 AI 优化的以太网结构（例如，RoCE、自适应路由、高对剖带宽），它减少了大规模训练和推理工作负载中的拥塞。

性能工程师将需要深入理解这些层级，并确数据在正确的时间处于正确的位置。目标将是模拟一个跨越 GPU 和 CPU 的巨大内存空间，这样即使模型无法装入一台机器，程序员也可以在某种程度上透明地对待它。随着统一内存和按需分页系统的出现，这其中的一部分今天已经可能，例如使用 `cudaMemPrefetchAsync()` 将页面预先暂存到目标设备上并避免缺页停顿。但在 100 万亿参数的规模下，这一功能将真正受到考验。

前沿研究实验室如 xAI、OpenAI 和 Microsoft 正在建设 1,000,000+ GPU 的大型集群，这并不令人惊讶。在 100 万亿参数的规模下，你可能有一个作业跨越整个数据中心的硬件。性能工程师必须在数据中心和多数据中心（全球）的规模上思考。

最后，随着模型——及其所需算力——的扩展，存在一种社会技术趋势。任何单一团队——甚至单一公司——独自训练最大的模型可能变得不可行。我们（希望）将看到 AI 社区中有更多的协作和共享来处理这些庞大的项目。这将类似于大科学项目——如粒子物理实验——涉及许多机构。

类似于现已解散的 Open Collective Foundation（一个非营利倡议）的举措，可能有助于汇集 AI 算力资源来训练一个 100 万亿参数的模型，然后与世界共享。

这将需要标准化诸如检查点格式之类的东西，共同开发训练代码，并思考模型的多方所有权。虽然这本身不是一个性能问题，但它将影响我们构建大型 AI 系统的方式。我们需要让它们更具容错性，并且容易快照以共享部分结果。

作为一名工程师，你最终可能会为了纯粹的速度，以及**可复现性**和**互操作性**进行优化。这允许不同的团队顺利且高效地在训练和推理工作流的不同部分上工作。

达到 100 万亿参数模型将需要整体的、全栈式的创新。这个挑战没有单一的解决方案。相反，拼图的每一块都必须改进。硬件需要更快并容纳更多数据。软件需要更多地自我优化——并通过编译器、AI 助手和实时适应更高效地使用资源。算法需要变得聪明，通过稀疏性、低精度和更好的优化器来避免不必要的工作。

性能工程师的角色将是将所有这些进步整合到一个连贯的工作流中。这就像组装一辆高性能赛车。引擎、轮胎、空气动力学和车手技能都必须协同工作。如果我们做对了，现在看起来不可能的事情——例如，在不破产的情况下训练 100 万亿参数——将变得可以实现。

不久前，1 万亿参数的模型听起来还很疯狂。然而今天，这种规模已被像月之暗面（Moonshot AI）的 Kimi K2（1 万亿参数 MoE，每 token 激活 320 亿参数）等开放权重模型所证明。按照这种进步速度，再加上 AI 辅助的人类智慧，我们将在非常短的时间内征服下一个里程碑和数量级。

## 本章要点

以下几点总结了本章案例研究和超大规模 AI 系统性能工程假设未来状态中讨论的最佳实践和新兴趋势：

**软硬件协同设计的优化**
LLM 的性能提升真正是通过紧密集成的软硬件协同设计创新带来的突破实现的。

**AI 辅助编码与性能优化**
Google DeepMind、NVIDIA 和 Predibase 展示了 AI 辅助发现和优化核心内核（如矩阵乘法和注意力机制）的能力。这些努力表明，AI 可以在极少人工干预的情况下生成、测试和改进底层 GPU 代码，并产生显著的加速。

**100 万亿参数模型的策略**
训练 100 万亿参数的模型将需要混合激进的量化、多维并行（数据、流水线、张量、专家和上下文/序列）以及机架间通信的细致编排。这强调了未来的 AI 扩展既取决于硬件能力，也取决于软件级调度的独创性。

**指数级计算基础设施扩展**
下一代 AI 数据中心正被设计为提供数量级增长的计算能力。这些设施将训练计算预算远超今日水平的 AI 模型。这使得训练运行能够使用当前系统 100 到 1,000 倍的 FLOPS。

**进化的 AI 模型与智能体**
未来的模型将是能够生成和优化代码、不断用新数据更新权重、甚至重写自身代码的自我改进系统。这种学习和改进的永续循环将缩短突破之间的时间，并创造出一支在研究和研发任务中超越人类团队的虚拟劳动力。

**AI 辅助实时故障排除**
除了调度，AI 副驾驶将监控系统日志和训练/推理工作负载，以快速检测异常——包括精度损失的峰值或硬件错误的数量。这些副驾驶可以帮助自动化调试、执行故障分析，甚至通过强化学习学习最佳配置。这有助于最大化每瓦和每单位时间的性能。

**每瓦性能，一个关键指标**
所有这些协同设计的努力最终都旨在最大化单位成本的吞吐量。具体来说，目标是最大化每美元、每瓦功率每秒处理和生成的 Token 数。例如，Grace Blackwell NVL72 机架系统相比上一代 Hopper，将**每瓦性能显著提高了 25 倍**。这直接转化为比上一代 GPU 集群更低的每 Token 成本。

## 结论

本书标志着 AI 系统性能工程领域的转折点。NVIDIA 将 CPU 和 GPU 紧密集成到 Grace Hopper 和 Grace Blackwell（以及即将推出的 Vera Rubin 和 Feynman）等超级芯片模块中，实现了计算效率和规模的新高度。在底层，GPU 使用高度优化的 Tensor Cores——以及针对 LLM 计算基础优化的 Transformer Engine。

像 NVIDIA GB200/GB300 NVL72 这样的超级计算系统，将 72 个 GPU 链接成单个处理单元（NVLink 域），使用 NVLink、NVSwitch 和 SHARP 等技术奠定了机架和数据中心通信的基础。这些为多万亿参数模型提供了低延迟、实时的推理能力。

在软件方面，vLLM、SGLang、NVIDIA Dynamo 和 TensorRT-LLM 等工具改进了大规模推理集群的调度和资源使用。这包括飞行批处理（In-flight batching）、分页 KV 缓存以及（为了效率将提示词预填充阶段与生成解码阶段分离到不同资源池）等技术。这些有助于减少尾部延迟并提高每瓦吞吐量。

这些例子证明了**协同设计（Codesign）**的力量，即硬件、软件和算法共同演进。这种植根于**机械共鸣（Mechanical Sympathy）**的伙伴关系有助于缩短训练时间，提高推理性能，并降低运营费用。这对于当今快速改进且资本密集的 AI 系统产生可衡量的投资回报是必需的。

此外，来自 Google DeepMind、NVIDIA 和 Predibase 的 AI 驱动编码和算法智能体展示了 AI 如何帮助优化 AI。随着模型和系统变得过于复杂以至于无法进行手动调优，自动化可以处理常规优化，并将人类工程师解放出来，专注于更高层级的优化和系统设计。

我们正从暴力扩展转向**智能扩展**：在每个周期做更多有用的工作，从新硬件特性中榨取每一盎司性能，并让 AI 助手管理细节。性能工程师将向技术栈上层移动，成为平衡效率、可靠性和可持续性的全球计算生态系统的架构师。

作为一名 AI 系统性能工程师，我们的角色将从单节点内核扩展到系统级和设施级的优化。我们将依靠直觉来发现瓶颈——比如缓慢的 All-Reduce 模式——然后指导我们的 AI 工具去修复它们。同时，我们将保持学习，因为硬件和算法创新的步伐只会加速。

总之，为了保持相关性和竞争力，你应该在 AI 系统性能基础方面打下坚实基础，保持好奇心，尝试新的硬件和软件进步，信任 AI 的建议，并准备好适应向量子计算及更远未来的格局变化。试想一下——在一个研究民主化、一键可访问 AI 超级计算机以及万亿参数模型触手可及的时代，你可能就是下一个通用人工智能（AGI）突破的推动者之一！









# 附录：AI 系统性能清单（175+ 项）

这份详尽的清单涵盖了宏观流程层面的最佳实践，以及针对 AI 系统性能工程师的详细底层调优建议。清单中的每一项都作为一个实用的提醒，旨在从 AI 系统中压榨出最大的性能和效率。

在调试、分析（Profiling）、诊断和调优 AI 系统时，请使用本指南。通过系统地应用这些技巧——从底层的操作系统和 CUDA 微调到集群规模的优化——AI 系统性能工程师可以在现代 NVIDIA GPU 硬件上（使用 CUDA、PyTorch、OpenAI Triton、TensorFlow、Keras 和 JAX 等多种 AI 软件框架）同时实现闪电般的执行速度和极具成本效益的运行。本清单中的原则同样适用于 NVIDIA 未来的硬件几代产品，包括其 GPU、基于 ARM 的 CPU、CPU-GPU 超级芯片、网络设备和机架系统。

## 性能调优与成本优化思维

建立一个务实且有文档记录的循环——在深入工作前先争取速赢（Quick wins）——将工程时间转化为可衡量的投资回报率（ROI）。从针对最大的运行时间和成本驱动因素开始，并在变更前后始终进行性能分析以验证影响。

结合自动调优、框架升级、云定价杠杆和利用率仪表板来获得高 ROI 的胜利，记录结果并倾向于简单、可维护的修复方案。在精度允许的情况下调整对吞吐量敏感的超参数。以下是关于性能调优和成本优化思维的一些建议：

**优先优化昂贵的部分**
使用二八定律（80/20 法则）。找到运行时间的主要贡献者并专注于此。如果 90% 的时间花在几个内核或通信阶段上，那么深度优化这些部分比微优化仅占 1% 时间的部分要好得多。每一章的技术都应应用在最重要的地方。例如，如果你的训练过程是 40% 的数据加载、50% 的 GPU 计算和 10% 的通信，那么首先修复数据加载，因为你可能将其开销减半。然后再看 GPU 内核优化。

**变更前后都要分析**
每当你应用一项优化时，都要测量其影响。这听起来很明显，但通常调整是基于理论做出的，在实践中可能没有帮助——甚至有反作用。考虑这样一个场景：你的工作负载不受内存限制，但你决定尝试在训练作业中启用激活检查点（Activation Checkpointing）。这实际上可能会因为使用额外的计算来减少内存而减慢作业速度。换句话说，在进行更改之前和之后，始终比较吞吐量、延迟和利用率等关键指标。使用内置的分析器进行简单的计时，例如 100 次迭代的平均迭代时间。

**拥抱自适应自动调优反馈循环**
实施利用实时性能反馈的高级自动调优框架——使用强化学习或贝叶斯优化等技术——来动态调整系统参数。这种方法使你的系统能够根据不断变化的工作负载和操作条件持续微调设置。

**为优化时间做预算**
性能工程是一项迭代投资。存在边际收益递减——摘取唾手可得的果实，如启用自动混合精度（AMP）和数据预取。这些可能很容易带来 2 倍的提升。更难的优化，如编写自定义内核，可能会带来较小的增量。始终权衡工程时间与运行时间收益和成本节省。对于像训练旗舰模型这样的大型经常性作业，即使是 5% 的增益也值得数周的调优，因为它可能节省数百万美元。对于一次性或小型工作负载，专注于更大的胜利并保持务实。

**随时了解框架的改进**
我们讨论的许多优化，如混合精度、融合内核和分布式算法，都在深度学习框架和库中不断改进。升级到最新的 PyTorch 或 TensorFlow 有时会产生立竿见影的加速，因为它们整合了新的融合算子或更好的启发式算法。利用这些改进，因为它们本质上是免费的收益。阅读发布说明以了解与性能相关的更改。

**与供应商和社区协同设计**
与硬件供应商和更广泛的性能工程社区保持联系，以使软件优化与最新的硬件架构保持一致。这种协同设计方法可以通过定制算法来利用新兴的硬件功能，从而揭示巨大的性能收益机会。定期查看供应商文档，参与论坛，并测试驱动程序或框架的测试版。这些互动通常会揭示新的优化机会和最佳实践，可以集成到你的系统中。集成新的驱动程序优化、库更新和特定于硬件的技巧可以提供额外的、有时是显著的性能提升。

**利用云的灵活性控制成本**
如果在云环境中运行，请明智地使用更便宜的竞价实例（Spot Instances）或预留实例。它们可以大幅削减成本，但你可能会在几分钟的通知下失去竞价实例。还要考虑实例类型，因为有时稍微旧一点的 GPU 实例只需一小部分成本就能提供更好的性价比，前提是你的工作负载不需要绝对最新的硬件。我们在 H800 与 H100 的讨论中表明，在次优硬件上通过努力也能做出很棒的工作。在云端，你可以获得类似的权衡。通过在不同的实例配置（包括 CPU 核数、CPU 内存、GPU 数量、GPU 显存、L1/L2 缓存、统一内存、NVLink/NVSwitch 互连、网络带宽和延迟以及本地磁盘配置）上进行基准测试来评估成本/性能。计算诸如“每美元吞吐量”之类的指标来指导你的优化决策。

**监控利用率指标**
持续监控 GPU 利用率、SM 效率、显存带宽使用情况，以及对于多节点环境的网络利用率。使用 DCGM exporter、Prometheus 等工具设置仪表板，以便在任何资源未充分利用时捕捉到情况。如果 GPU 利用率只有 50%，请深究原因。这可能是数据等待/停滞和缓慢的同步通信。如果网络利用率只有 10% 但 GPU 在等待数据，也许是锁（Lock）等其他问题。这些指标有助于查明要关注哪个子系统。

**迭代并调优超参数以提高吞吐量**
某些模型超参数，如批处理大小（Batch Size）、序列长度和 MoE 活跃专家数量，可以在不降低最终精度的情况下进行调优以提高吞吐量。例如，较大的批处理大小可提供更好的吞吐量，但可能需要调整学习率调度以保持精度。不要害怕调整这些以找到速度和精度的最佳平衡点。这也是性能工程的一部分——有时可以为了效率调整模型或训练过程，例如使用激活检查点或在相同的有效批次下进行更多步的计算。你可能需要调整训练学习率调度来补偿这种情况。

**记录并复用**
记录你应用的优化及其影响。在代码中或内部类似 Wiki 的共享知识库系统中进行文档记录。这为未来的项目建立了一个知识库。许多技巧是可复用的模式，如启用重叠（Overlapping）和特定的环境变量，这对集群有帮助。拥有这些历史记录可以在开始新任务或让新团队成员加入性能调优工作时节省时间。

**平衡优化与复杂性**
以实现所需性能的最简单解决方案为目标。例如，如果带有 `torch.compile` 的原生 PyTorch 满足你的速度目标，你可能不需要编写自定义 CUDA 内核。这将有助于避免额外的维护工作。过度优化和高度定制的代码会使系统变得脆弱。既快速又可维护的解决方案才优雅。因此，应用干扰最小且能产生所需增益的优化，仅在需要时才升级到更复杂的优化。

**优化 AI 驱动的性能**
利用机器学习模型分析历史遥测数据并预测系统瓶颈，从而实现参数的实时自动调整，以优化资源分配和吞吐量。

## 可复现性与文档最佳实践

除非性能胜利是可复现的、有版本控制的并且持续检查的，否则它们不会持久，并且会随着时间的推移悄悄回退。将文档、CI 基准测试和共享知识视为保持加速效果、加速入职和审计的粘合剂。

在源代码控制中锁定版本、配置和基准测试，以便实验可重复且回退可追踪。将性能检查引入 CI/CD，配置端到端的监控和警报，并将优化与安全性和详尽的文档相结合，以创建一个持久、可审计的实践。以下是提高可复现性和文档的提示列表：

**严格的版本控制**
对所有系统配置、框架/驱动程序版本、操作系统设置、优化脚本和基准测试保持全面的版本控制。使用 Git（或类似系统）跟踪更改并标记发布版本。这样，实验可以精确复现——并且可以轻松识别性能回退。

**持续集成进行性能回归测试**
将自动化性能基准测试和实时监控集成到你的 CI/CD 流水线中。这确保了每一次更改——从代码更新到配置更改——都在一组性能指标上进行验证，有助于尽早捕捉回退，并保持一致和可衡量的性能增益。采用行业标准基准测试（如 MLPerf）来建立可靠的性能基线并跟踪随时间的改进。

**端到端工作流优化**
确保优化是跨整个 AI 流水线整体应用的——从数据摄取和预处理到训练和推理部署。协调的跨系统调优可以揭示孤立调整可能错过的协同效应，从而产生更显著的整体性能增益。

**自动化监控与诊断**
部署端到端的监控解决方案，收集跨硬件、网络和应用层的实时指标。将这些与仪表板（如 Prometheus/Grafana）集成，并配置自动警报以迅速检测异常，如 GPU 利用率突然下降或网络延迟激增。

**容错与自动恢复**
通过使用分布式检查点、冗余硬件配置和动态作业重新调度，将容错性纳入系统设计。该策略最大限度地减少了停机时间，并在面对硬件或网络故障时保持性能。

**编译器与构建优化**
在构建过程中利用激进的编译器标志和基于配置文件的优化（PGO），从代码中提取最大性能。定期更新和调整你的构建配置，并通过严格的基准测试验证每个更改的影响，以确保最佳执行。

**安全性、合规性与性能**
整合并协同设计安全性、合规性和性能。定期审计配置，强制执行访问控制，并维护行业标准的安全措施，包括加密、安全数据通道、零信任网络、硬件安全模块（HSM）和安全飞地。确保性能调优绝不损害系统安全性。同样，确保安全性不会产生不必要的性能开销。

**全面的文档与知识共享**
维护所有优化步骤、系统配置和性能基准测试的详细记录。开发内部知识库以促进团队协作和快速入职，确保最佳实践得以保留并在不同项目中复用。

**面向未来与可扩展性规划**
设计模块化、适应性强的系统架构，可以轻松整合新兴的硬件和软件技术。持续评估可扩展性要求并更新优化策略，以随着工作负载的增长维持具有竞争力的性能。

## 系统架构与硬件规划

你的硬件、互连和数据路径设定了性能和成本效率的上限——没有任何软件调整能超越一个“吃不饱”的 GPU。规划每美元/每瓦的有效吞吐量（Goodput），通过匹配加速器、CPU/DRAM/I/O 以及冷却/电源与工作负载，从一开始就避免瓶颈。

具体来说，**为有效吞吐量（Goodput）设计**——即每美元/每瓦的有用工作——而不仅仅是原始 FLOPS。将加速器和互连与工作负载相匹配，合理调整 CPU/内存/I/O 以保持 GPU 忙碌，保持数据本地化，并规划电源/冷却以使硬件维持峰值时钟频率。在添加更多 GPU 之前评估扩展效率。以下是优化系统架构和改进硬件规划效率的一些提示：

**为有效吞吐量和效率设计**
将有用的吞吐量视为目标。每一比特的性能提升在大规模下都转化为巨大的成本节约。专注于最大化每美元/每瓦的生产性工作——而不仅仅是原始 FLOPS。

**选择合适的加速器**
优先选择现代 GPU，以获得卓越的每瓦性能和显存容量。较新的架构提供原生 FP8 和 FP4 精度支持等功能——以及更快的互连。这些比老一代 GPU 和系统能产生巨大的加速。

**利用高带宽互连**
对于多 GPU 工作负载，使用带有 NVLink/NVSwitch 的系统（如 GB200/GB300 NVL72），而不是仅使用 PCIe 连接。NVLink 5 提供高达 1.8 TB/s 的双向 GPU 到 GPU 带宽（超过 PCIe Gen5 的 14 倍），实现跨 GPU 的近线性扩展。NVLink Switch 域可以通过二级交换机扩展，在一个 NVLink 域中连接多达 576 个 GPU。这实现了分层集合通信，在回退到机架间网络之前尽可能长时间地停留在 NVLink 上。

**平衡 CPU/GPU 和内存比例**
为每个 GPU 配置足够的 CPU 核心、DRAM 和存储吞吐量。例如，为每个 GPU 分配约 1 个快速 CPU 核心用于数据加载和网络任务。确保系统 RAM 和 I/O 能够以每 GPU 数百 MB/s 的速率向 GPU 供数据，以避免饥饿。

**规划数据局部性**
如果在多节点上训练，尽量减少节点外通信。只要可能，将紧密耦合的工作负载保持在同一个 NVLink/NVSwitch 域中以利用全带宽，并使用你可以访问的最高速互连。理想情况下，节点内和机架内通信使用 NVLink，机架间通信使用 InfiniBand。

**避免链条中的瓶颈**
识别最慢的环节——无论是 CPU、内存、磁盘还是网络——并将其扩展。例如，如果 GPU 利用率因 I/O 而低，投资更快的存储或缓存比购买更多 GPU 更好。所有组件匹配良好的端到端设计可以防止浪费 GPU 周期。

**选择合适的集群规模**
添加 GPU 时要注意收益递减。超过一定集群规模后，开销可能会增加——确加速比能证明成本的合理性。通常最好先在 N 个 GPU 上优化利用率（例如达到 95%），然后再扩展到 2N 个 GPU。

**为冷却和电源设计**
确保数据中心可以处理 GPU 的热量和电源需求。像 GB200/GB300 这样的高性能系统具有非常高的 TDP（热设计功耗）。提供足够的冷却（可能是液冷）和电源配置，以便 GPU 可以维持加速时钟频率而不发生降频。

## 统一 CPU-GPU “超级芯片”架构

统一内存和封装内链路让你能容纳更大的模型，并在将正确的数据放置在正确的层级时减少拷贝开销。使用 Grace 进行预处理，使用 HBM 存储“热”张量，将超级芯片变成一个紧密耦合、停顿更少的引擎。

在 Grace Blackwell 超级芯片上，将 CPU 和 GPU 视为一个共享内存综合体。将热权重/激活值保留在 HBM 中，并通过 NVLink-C2C 将溢出或不频繁的数据保留在 Grace LPDDR 中。利用封装内的 Grace CPU 进行预处理/编排，并利用预取或流水线管理的内存来隐藏超大模型的延迟。利用超级芯片架构的建议如下：

**利用统一 CPU-GPU 内存**
利用 Grace Blackwell (GB200/GB300) 超级芯片的统一内存空间。两个 Blackwell GPU 和一个 72 核 Grace CPU 通过 NVLink-C2C (900 GB/s) 共享一个一致性内存池。使用 CPU 的大内存（例如 480 GB LPDDR5X）作为超大模型的扩展，同时将“热”数据保留在 GPU 的 HBM 中以保证速度。

**为局部性放置数据**
即使有统一内存，也要优先考虑数据放置。将模型权重、激活值和其他频繁访问的数据放在 GPU HBM3e 上（其本地带宽要高得多），让不常使用或溢出的数据驻留在 CPU RAM 中。这确保了 900 GB/s 的 NVLink-C2C 链路不会成为关键数据的瓶颈。

**利用可用的 CPU-GPU 直接内存访问**
利用 GPU 直接访问 CPU 内存的能力（在 GB200 和 GB300 等组合 CPU-GPU 超级芯片上）。GPU 可以通过 NVLink-C2C 一致性地读写 Grace LPDDR 内存，而无需通过主机 PCIe 进行中转。带宽和延迟仍然低于 HBM，因此预取托管指针、暂存数据和流水线传输以隐藏延迟。因此，建议将热激活值和 KV 缓存保留在 HBM 中，并使用 CPU 内存作为带有显式预取的低层级缓存。

**有效使用 Grace CPU**
封装内的 Grace CPU 提供 72 个高性能核心——利用它们！将数据预处理、增强和其他 CPU 友好型任务卸载到这些核心。它们可以使用 NVLink-C2C 快速向 GPU 供数据，本质上充当 GPU 的极速 I/O 和计算伴侣。

**为超大模型规划**
对于超过 GPU 显存的万亿参数模型训练，GB200/GB300 系统允许你使用 CPU 内存作为模型内存池的一部分。优先使用框架缓存分配器，并在自定义代码中使用 `cudaMallocAsync` 以最小化碎片并启用图捕获。使用 CUDA 统一内存或托管内存 API 优雅地处理溢出，并考虑从 CPU → GPU 内存的显式预取（例如 `cudaMemPrefetchAsync`）即将到来的层，以隐藏延迟。

**考虑超级芯片优化的算法**
SuperOffload 是一套针对超级芯片优化的算法示例，专注于提高卸载和张量转换/复制策略的效率。创新包括“推测后验证”（Speculation-Then-Validation, STV）、异构优化器计算和基于 ARM 的 CPU 优化器。SuperOffload 专为 NVIDIA 超级芯片（如 Grace Hopper, Grace Blackwell, Vera Rubin）设计，相对于传统卸载策略，它提高了 Token 处理吞吐量和芯片利用率。

## 多 GPU 扩展与互连优化

只有当通信快速且具有拓扑感知时，扩展才有回报——否则添加的 GPU 只是在互相等待。依靠 NVLink/NVSwitch 带宽、现代集合通信和网络结构感知放置来接近线性加速比。

具体来说，利用 NVLink/NVSwitch 域（例如 NVL72）进行近线性扩展，并选择适合该网络结构的并行策略。使用拓扑感知放置、更新的 NCCL 集合通信（例如 PAT）和遥测技术，以验证你是否有效使用了每 GPU 约 1.8 TB/s 的双向吞吐量带宽。在扩展时规划分层通信。以下是关于通过互连和拓扑优化利用多 GPU 扩展的提示：

**为高速全互联拓扑设计**
在 NVL72 NVSwitch 集群上，有 72 个全互联的 GPU，例如，任何 GPU 都可以以全 NVLink 5 速度与任何其他 GPU 通信。在网络结构层面，NVLink Switch 域是无阻塞的。应用层吞吐量可能会随并发流量和路径调度而变化，因此在假设点对点饱和之前，请使用 DCGM NVLink 计数器和 Nsight Systems 追踪来验证行为。利用这种拓扑结构，使用数据并行、张量并行和流水线并行等并行策略，这些策略在较差的互连上会成为瓶颈。

**利用拓扑感知调度**
如果可能，始终将多 GPU 作业并置在一个 NVLink Switch 域内。将作业的所有 GPU 保持在 NVL72 网络结构上意味着通信密集型工作负载的近线性扩展。跨 NVLink 域或标准网络混合 GPU 会引入瓶颈，对于紧密耦合的任务应避免这种情况。

**利用前所未有的带宽**
认识到 NVLink 5 每个方向每个 GPU 有 900 GB/s，这是上一代每 GPU 带宽的两倍。一个 NVL72 机架总共提供约 130 TB/s 的机架内聚合带宽。这极大地减少了通信等待时间，即使是数十 GB 的梯度数据也可以在几毫秒内以 1.8 TB/s 的速度完成全归约（All-reduce）。设计训练算法，如梯度同步和参数分片，以充分利用这相对免费的通信预算。

**拥抱现代集合通信算法**
使用针对 NVSwitch 优化的最新 NVIDIA NCCL 库。具体来说，启用并行聚合树（PAT）算法，这是为 NVLink Switch 拓扑引入的。这通过利用 NVL72 拓扑比其他树/环算法更有效地执行归约，进一步减少了同步时间。

**考虑细粒度并行**
有了全带宽全互联连接，可以考虑以前不可行的细粒度模型并行。例如，当每个 GPU 与其他所有 GPU 都有 1.8 TB/s 的双向吞吐量时，层级并行或跨许多 GPU 的张量并行可以是高效的。以前，人们可能会避免过多的跨 GPU 通信，但 NVL72 允许激进地划分工作而不会触及网络限制。

**监控饱和度**
尽管 NVL72 极快，但在分析时仍要关注链路利用率。如果你的应用程序以某种方式使用极端的全对全（All-to-all）操作使 NVSwitch 饱和，例如，你可能需要通过聚合梯度等方式来限制通信。使用 NVIDIA 的工具或 NVSwitch 遥测来验证通信是否在 NVLink 容量范围内，并根据需要调整模式。例如，你可以交错全对全交换以避免网络争用。DCGM 暴露了 NVLink 计数器，可以帮助验证链路平衡并在集合通信期间检测热点。

**为未来扩展做计划**
请注意，NVLink Switch 可以扩展到单机架之外——使用二级交换机在一个连接域中最多连接 576 个 GPU。如果你在那种超大规模下运行，请规划分层通信，首先使用本地 NVL72 机架内集合通信，然后仅在必要时使用机架间互连。这有助于首先最大化机架内 NVLink 使用率。这确保了在诉诸机架间 InfiniBand 跳跃之前使用的是最快的链路。

**识别联邦和分布式优化的机会**
对于跨越异构环境（如多云或边缘到云设置）的部署，采用自适应通信协议和动态负载均衡策略。这最小化了延迟并最大化了分布式系统的吞吐量，确保即使资源在能力和容量上有所不同，也能保持稳健的性能。

## 操作系统与驱动程序优化

操作系统抖动（Jitter）、NUMA 未命中和驱动程序不匹配会悄悄地消耗吞吐量并造成无法调优的变异性。加固堆栈（大页、亲和性、一致的 CUDA/驱动、持久性模式）创建了一个稳定的高性能基线。

运行精简的、HPC 调优的 Linux。设置 NUMA/IRQ 亲和性并启用透明大页（THP）和高 memlock。在节点间保持 NVIDIA 驱动/CUDA 一致。隔离系统抖动，调整 CPU 库/存储，正确设置容器限制，并保持 BIOS/固件/NVSwitch 网络结构最新以获得可预测的吞吐量。以下是你应该在环境中探索的一些主机、操作系统和容器优化：

**使用为 HPC 调优的 Linux 内核**
确保你的 GPU 服务器运行的是配置为高性能计算的最新、稳定的 Linux 内核。禁用消耗 CPU 或 I/O 的不必要后台服务。使用“performance” CPU 调节器——而不是“on-demand”或“power-save”——以保持 CPU 核心处于高时钟频率来为 GPU 供数据。

**为性能关键型工作负载禁用交换（Swap）**
在训练服务器上禁用 Swap 以避免页面抖动，或者如果必须启用 Swap，请使用 `mlock` 或 `cudaHostAlloc` 锁定关键缓冲区以确保它们保留在 RAM 中。

**通过激进的预分配避免内存碎片**
为频繁使用的张量预分配大的、连续的内存块，以减少运行时分配开销和碎片。这种主动策略确保了在长训练运行期间更稳定和高效的内存管理。

**优化 CPU 库的环境变量**
微调参数，如 `OMP_NUM_THREADS` 和 `MKL_NUM_THREADS`，以更好地匹配你的硬件配置。调整这些变量可以减少线程争用并提高 CPU 受限操作的并行效率。

**设计 NUMA 感知**
对于多 NUMA 服务器，将 GPU 进程/线程绑定到本地 NUMA 节点的 CPU。使用 `numactl` 或 `taskset` 等工具将每个训练进程绑定到离其分配的 GPU 最近的 CPU。同样，将内存分配绑定到本地 NUMA 节点 (`numactl --membind`)，以便 GPU DMA 的主机内存来自最近的 RAM。这避免了昂贵的跨 NUMA 内存流量，该流量可能会使有效 PCIe/NVLink 带宽减半。

**利用网络和 GPU 任务的 IRQ 亲和性**
显式将网卡中断绑定到与网卡位于同一 NUMA 节点的 CPU 核心，同样将 GPU 驱动程序线程绑定到专用核心——包括来自长期运行服务（如 `nvidia-persistence-service` 守护进程）的线程。该策略最小化了跨 NUMA 流量，并在重负载下稳定性能。

**启用透明大页 (Transparent Hugepages, THP)**
在 `always` 或 `madvise` 模式下打开透明大页，以便大内存分配使用 2 MB 页面。这减少了分配数十或数百 GB 主机内存用于框架时的 TLB 抖动和内核开销。通过检查 `/sys/kernel/mm/transparent_hugepage/enabled` 验证 THP 是否处于活动状态。启用 THP 后，你的进程将为大分配使用大页。如果你的工作负载对延迟敏感且你观察到抖动，首选 `madvise` 模式。

**增加最大锁定内存**
配置操作系统以允许大的锁定（即页锁定/Pinned）分配。GPU 应用程序经常锁定内存以进行更快的传输——设置 `ulimit -l unlimited` 或一个高值，以便你的数据加载器可以分配锁定缓冲区而不会触及操作系统限制。这防止了失败或回退到可分页内存，后者会减慢 GPU DMA。

**使用最新的 NVIDIA 驱动和 CUDA 堆栈**
在所有节点上保持 NVIDIA 驱动程序和 CUDA 运行时最新（在经过测试的稳定版本内）。新驱动程序可以带来性能改进，并且是新 GPU 计算能力所必需的。确保所有节点具有相同的驱动/CUDA 版本，以避免多节点作业中的任何不匹配。在启动时启用 GPU 的持久性模式 (`nvidia-smi -pm 1`)，以便驱动程序保持加载状态，GPU 不会产生重新初始化延迟。更新所有节点上的 NVIDIA 驱动程序和工具包，以继承错误修复和性能改进。

**在使用 MIG 配置时启用 GPU 持久性**
启用持久性模式后，GPU 保持“热”状态并准备就绪，减少了作业的启动延迟。如果使用多实例 GPU (MIG) 分区，这尤其关键——没有持久性，MIG 配置会在每次作业时重置，但保持驱动程序活动可以保留切片。使用 MIG 时始终配置持久性模式。

**隔离系统任务**
在每个服务器上奉献一个核心——或一小组核心——用于操作系统内务处理，如中断处理和后台守护进程。这样，你喂给 GPU 的主 CPU 线程就不会被打断。这可以使用 CPU 隔离或 cgroup 绑定来完成。消除操作系统抖动可确保持续的吞吐量。

**优化系统 I/O 设置**
如果你的工作负载进行大量日志记录或检查点操作，请使用有利于吞吐量的选项挂载文件系统。考虑对数据磁盘使用 `noatime` 并增加文件系统预读（read-ahead）以进行流式读取。确保磁盘调度程序设置得当，对 NVMe SSD 使用 `mq-deadline` 或 `noop` 以减少延迟变异性。

**执行定期维护**
保持 BIOS/固件更新以获取性能修复。一些 BIOS 更新可以提高 PCIe 带宽或修复 GPU 的输入输出内存管理单元 (IOMMU) 问题。此外，定期检查 NIC 和 NVSwitch/Fabric（如果适用）的固件更新，如 NVIDIA 提供的 Fabric Manager 升级等。微小的固件调整有时可以解决模糊的瓶颈或可靠性问题。

**调优 Docker 和 Kubernetes 配置以获得最大性能**
在容器中运行时，添加选项，如 `--ipc=host` 用于共享内存，并设置 `--ulimit memlock=-1` 以防止内存锁定问题。这保证了你的容器化进程可以在没有操作系统强加限制的情况下访问内存。

## GPU 资源管理与调度

更智能的放置和分区可以在不购买新硬件的情况下提高利用率——并保护混合工作负载的可预测性。尊重拓扑结构，在适当的地方使用 MPS/MIG，并控制时钟/电源以最小化争用和尾部延迟。

在调度时考虑到 GPU/NUMA/NVLink 拓扑，并使用 MPS 或 MIG 来提高较小作业的利用率，同时保留 ECC 和持久性以保证可靠性。在需要时锁定时钟或功率限制以保持稳定性，避免 CPU 超额订阅，并智能地打包作业以在无争用的情况下最大化 ROI。以下是一些 GPU 资源管理和调度提示：

**拓扑感知作业调度**
确保像 Kubernetes 和 SLURM 这样的编排器在尊重 NUMA 和 NVLink 边界的节点上调度容器，以最小化跨 NUMA 和跨 NVLink 域的内存访问。这种对齐减少了延迟并提高了整体吞吐量。

**多进程服务 (MPS)**
在单个 GPU 上运行多个进程时启用 NVIDIA MPS 以提高利用率。MPS 允许来自不同进程的内核在 GPU 上并发执行，而不是时间分片。如果单个作业不能完全使 GPU 饱和，这很有用——例如，在一个 GPU 上使用 MPS 运行 4 个训练任务可以重叠它们的工作并提高整体吞吐量。

**多实例 GPU (MIG)**
使用 MIG 将高端 GPU 分割成更小的实例用于多个作业。如果你有许多轻量级工作负载，如推理小模型或运行许多实验，你可以切分 GPU 以确保每个作业有保证的资源。例如，现代 GPU 可以被分成多个 MIG 切片（最多 7 个）。不要将 MIG 用于紧密耦合的并行作业，因为那些作业受益于完整的 GPU 访问。当作业小于一个完整 GPU 时，部署 MIG 以实现隔离并最大化 GPU ROI。

**MIG 的持久性**
保持持久性模式开启以维持作业间的 MIG 分区。这避免了重新分区的开销，并确保后续作业无延迟地看到预期的 GPU 切片。在集群启动时配置 MIG 并保持启用，以便调度是可预测的，因为即时更改 MIG 配置需要重置 GPU，这可能会中断正在运行的作业。规划维护窗口，因为 MIG 设备分区不会在 GPU 重启后持久保存。使用 NVIDIA 的 MIG Manager 在启动时自动重新创建所需的布局。

**GPU 时钟和电源设置**
如果需要运行间的一致性，考虑使用 `nvidia-smi -lgc/-lmc` 将 GPU 时钟锁定在固定的高频率。默认情况下，GPU 使用自动加速（Auto Boost），这通常是最佳的，但固定时钟可以避免任何瞬时的降频。在受限场景中，你可能会稍微降频或设置功率限制，以使 GPU 保持在稳定的热/功率包络内——如果偶尔的节流是个问题，这可以产生一致的性能。

**ECC 内存**
为了可靠性，在数据中心 GPU 上保持启用 ECC，除非你有特定理由禁用它。性能成本极小——在带宽和内存方面大约损失百分之几——但 ECC 可以捕捉可能破坏长时间训练的内存错误。大多数服务器 GPU 默认启用 ECC。保持开启以保护多周的训练。

**作业调度器感知**
将 GPU 拓扑集成到你的作业调度器中，如 SLURM 和 Kubernetes。配置调度器在需要低延迟耦合时将作业分配在同一节点或同一 NVSwitch 组上。使用 Kubernetes 设备插件或 SLURM Gres 为较小的作业调度 MIG 切片。GPU 感知调度器可以防止单个作业跨越遥远的 GPU 并遭受带宽问题的情况。

**CPU 超额订阅**
调度作业时，要考虑每个 GPU 任务的 CPU 需求，如数据加载线程等。不要在一个节点上打包超过 CPU 处理能力的 GPU 作业。让 GPU 空闲比让 CPU 过载导致所有 GPU 都供养不足要好。监控每个 GPU 作业的 CPU 利用率以指导调度决策。

**为 NVSwitch 使用 NVIDIA Fabric Manager**
在带有 NVSwitch 的系统上，GB200/GB300 NVL72 机架确保 NVIDIA Fabric Manager 正在运行。它管理 NVSwitch 拓扑和路由。没有它，多 GPU 通信可能无法完全优化，甚至对于大型作业可能会失败。Fabric Manager 服务通常在配备 NVSwitch 的服务器上默认运行，但你应该仔细检查它是否已启用并正在运行——特别是在驱动程序更新后。

**为利用率打包作业**
通过智能地打包作业来最大化利用率。例如，在一个 4-GPU 节点上，如果你有两个不怎么使用 CPU 的 2-GPU 作业，将它们一起运行在同一个节点上可以节省资源，如果在同一个计算节点或启用 NVLink 的机架内运行，甚至可以使用更快的 NVLink 进行通信。相反，避免并置那些总和超过节点内存或 I/O 容量的作业。目标是在无争用的情况下实现高硬件利用率。

## I/O 优化

如果数据跟不上，GPU 就会闲置——通常最大、最便宜的加速来自于修复输入，而不是数学运算。并行性、锁页内存、异步传输和快速存储确保模型持续得到喂养。

通过并行化数据加载器、使用锁页内存和异步传输以及将数据存储在快速 NVMe 上——最好使用 GPUDirect Storage——来保持 GPU 饱食。明智地进行条带化、缓存和压缩。测量端到端吞吐量，使 I/O 随集群规模扩展，并异步写入检查点/日志。以下是关于数据管道 I/O 优化的一些提示：

**并行加载数据**
使用多个工作进程/线程为 GPU 加载和预处理数据。默认的一到两个数据加载工作进程可能不足。使用 PyTorch 的 `DataLoader(num_workers=N)` 等配置，分析并增加数据加载进程/线程的数量，直到数据输入不再是瓶颈。高核心数的 CPU 存在的意义就是喂养那些 GPU，所以确保你利用了它们。

**为 I/O 锁定主机内存**
为数据传输缓冲区启用锁定（即页锁定/Pinned）内存。许多框架都有选项，如 PyTorch 的 `DataLoader` 的 `pin_memory=True`，用于分配 GPU 可以直接 DMA 的主机内存。使用锁定内存显著提高了 H2D 复制吞吐量。将其与异步传输结合使用，以重叠数据加载与计算。

**重叠计算与数据传输**
流水线化你的输入数据。当 GPU 忙于计算批次 N 时，在 CPU 上加载并准备批次 N+1，并使用 CUDA 流和非阻塞 `cudaMemcpyAsync` 在后台传输它。这种双重缓冲隐藏了延迟——GPU 理想情况下永远不需要等待数据。确保你的训练循环使用异步传输。例如，在 PyTorch 中，你可以使用 `non_blocking=True` 将张量复制到 GPU。异步传输允许 CPU 在后台进行数据传输时继续运行。这将通过重叠计算与数据传输来提高性能。

**使用快速存储 (NVMe/SSD)**
将训练数据存储在快速的本地 NVMe SSD 或高性能并行文件系统上。旋转磁盘将严重限制吞吐量。如果可用，启用 GPUDirect Storage (GDS)，以便 GPU 可以直接从 NVMe 或网络存储流式传输数据——绕过 CPU。这进一步降低了读取大型数据集时的 I/O 延迟和 CPU 负载。对于大数据集，考虑每个节点拥有一份本地副本或分片数据。如果使用网络存储，首选像 Lustre 这样支持条带化的分布式文件系统，或者可以并行服务许多客户端的对象存储。

**调整 I/O 并发与条带化**
避免单文件访问带来的瓶颈。如果所有工作节点都使用一个大文件，请将其条带化到多个存储目标上，或将其拆分为块，以便多个服务器可以提供服务。例如，将数据集拆分为多个文件，并让每个数据加载工作进程同时读取不同的文件。这最大化了存储系统的聚合带宽。

**优化小文件访问**
如果你的数据集由数百万个小文件组成，请减轻元数据开销。每秒打开太多小文件会压垮文件系统的元数据服务器。解决方案包括将小文件打包到更大的容器中，如 tar 或 RecordIO 文件；使用批量读取的数据摄取库；或确保在客户端启用元数据缓存。这减少了每个文件的开销并加快了 Epoch 启动时间。

**使用客户端缓存（如果可用）**
利用任何缓存层。如果使用 NFS，增加客户端缓存大小和持续时间。对于分布式文件系统，考虑缓存守护进程，甚至手动将部分数据集缓存到本地磁盘上。目标是避免从慢速源重复读取相同的数据。如果每个节点在不同时间处理相同的文件，本地缓存可以大幅削减冗余 I/O。

**明智地压缩数据**
如果 I/O 是瓶颈，将数据集压缩存储，但使用轻量级压缩，如 LZ4 或 Zstd 快速模式。这用一些 CPU 换取 I/O 量的减少。如果由于解压导致 CPU 成为瓶颈，考虑多线程解压或卸载到加速器。此外，通过使用一个线程读取压缩数据，另一个线程并行解压数据来重叠解压与读取。现代 GPU 在配合 GPUDirect Storage 和 cuFile I/O 堆栈时，可以使用 GPU 计算资源（或用于图像/视觉数据的专用解码器）执行即时数据解压。

**测量吞吐量并消除瓶颈**
持续监控数据管道的吞吐量。如果 GPU 没有接近 100% 的利用率且你怀疑有输入滞后，测量你从磁盘读取的 MB/s 数以及数据加载核心的忙碌程度。像 `dstat` 或 NVIDIA 的 DCGM 这样的工具可以揭示 GPU 是否在等待数据。系统地调整每个组件，增加预取缓冲区，增加网络缓冲区大小，优化磁盘 RAID 设置等。这样做直到输入管道能够像 GPU 消耗数据一样快地提供数据。通常，这些优化通过消除 I/O 停顿，在相同硬件上将 GPU 利用率从 ~70% 提高到 > 95%。

**为多节点扩展 I/O**
在集群规模下，确保存储系统可以处理聚合吞吐量。例如，8 个 GPU 每个消耗 200 MB/s，每节点就是 1.6 GB/s。跨越 100 个节点，就是需要 160 GB/s。很少有中央文件系统能维持这一点。通过跨存储服务器分片数据、使用每节点缓存或将数据预加载到每个节点的本地磁盘来缓解。为了吞吐量牺牲存储空间（例如，数据的多个副本）通常是值得的，以避免昂贵的 GPU 饥饿。

**最小化检查点和日志记录开销**
高效地写入检查点和日志。如果可能，对检查点使用异步写入，或写入本地磁盘，然后复制到网络存储以避免停止训练。压缩检查点或使用稀疏存储格式以减小大小。限制每步的日志记录频率，通过聚合迭代统计数据并仅每 N 次迭代记录一次，而不是每次迭代都记录。这将大大减少 I/O 开销。
你还可以使用 `cuda-checkpoint` 和用户空间检查点/恢复 (CRIU) 挂起正在运行的 GPU 进程以持久化进程映像。准备好恢复时，CUDA 驱动程序可以恢复设备内存和 CUDA 状态——甚至恢复到同设备类型的其他 GPU 上。将其视为模型 state-dict 或分片检查点文件的补充，而非替代。

## 数据处理管道

数据的格式、布局和位置决定了管道在规模化时的运行顺畅程度。二进制格式、分片、缓存和优先线程将 I/O 从瓶颈转变为稳定的流。

将数据集转换为二进制或内存映射格式，跨存储和节点分片，并提高线程优先级或将简单的增强操作移动到 GPU 以防止停顿。缓存热数据/KV 状态，积极预取和缓冲，并调整批次大小以保持管道从磁盘到设备的顺畅。以下是改进数据处理的提示：

**使用二进制数据格式**
将数据集转换为二进制格式，如 TFRecords、LMDB 或内存映射数组。这种转换减少了处理数百万个小文件的开销并加速了数据摄取。

**调优文件系统**
除了使用 `noatime` 挂载文件系统和增加预读之外，考虑跨多个存储节点分片数据以分发 I/O 负载并防止单个服务器上的瓶颈。

**为 CPU 密集型工作负载禁用超线程**
对于 CPU 极其密集的数据管道，禁用超线程可以减少资源争用并导致更一致的性能。这在单线程性能至关重要的系统上特别有益。

**提升线程优先级**
使用 `chrt` 或 `pthread_setschedparam` 等工具提高数据加载器和预处理 CPU 线程的调度优先级。通过给予这些线程更高的优先级，你确保数据以最小的延迟馈送给 GPU，减少管道停顿的机会。

**缓存常用数据**
利用操作系统页面缓存或专用 RAM 盘来缓存频繁访问的数据。这种方法在 NLP 等应用中特别有益，其中某些词元或短语被重复访问，减少了冗余处理和 I/O 开销。

**预取和缓冲数据**
始终在需要它的迭代之前加载数据。使用后台数据加载线程或进程，例如带有 `prefetch_factor` 的 PyTorch `DataLoader`。对于分布式训练，使用 `DistributedSampler` 确保每个进程获得唯一数据以避免冗余 I/O。

**并行化数据转换**
如果 CPU 预处理——如图像增强和文本分词——很繁重，将其分发到多个工作线程/进程中。分析以确保 CPU 不是瓶颈而让 GPU 等待。如果是，要么增加工作进程，要么将一些转换移动到 GPU，因为像 NVIDIA DALI 这样的库可以异步在 GPU 上执行图像操作。

**缓存模型状态和输出**
在 LLM 推理时，缓存常见词元的嵌入和 V 缓存是有益的，以避免重复计算它们。同样，如果 LLM 训练作业多次重用相同的数据集（称为 Epoch），你应该利用操作系统页面缓存或 RAM 来存储热数据。

**跨节点分片数据**
在多节点训练中，给每个节点一个数据子集，以避免每个节点都从单一来源读取整个数据集。这扩展了 I/O。使用分布式文件系统或手动分片分配，让每个节点读取不同的文件。这加快了速度并自然地与数据并行对齐，因为每个节点处理自己的数据分片。DeepSeek 的 Fire-Flyer File System (3FS) 是分布式数据集分片文件系统的一个例子。DeepSeek 的 3FS 通过将数据集分片分布在每个节点上的 NVMe SSD 上——同时最小化传统缓存——实现了每秒多 TB 的吞吐量。这种设计为每个 GPU 提供本地高速数据，避免了 I/O 瓶颈。

**监控管道并调整批处理大小**
有时增加批处理大小会将更多工作推给 GPU 并减少 I/O 频率，提高整体利用率——但仅在影响收敛的某一点之前有效。相反，如果 GPU 经常等待数据，且你无法加速 I/O，你实际上可能会减少批处理大小以缩短每次迭代，从而减少空闲时间，或者对较小的批次进行梯度累积，以便数据读取更加连续。找到一个 GPU 几乎总是忙碌的平衡点。

**在 GPU 上应用数据增强**
如果增强很简单但应用于海量数据，如添加噪声或归一化，在 GPU 上做可能值得，以避免使 CPU 饱和。GPU 在数据加载期间通常未充分利用，因此使用小的 CUDA 内核在加载后增强数据可以是高效的。但要小心不要序列化管道。使用流来重叠批次 N+1 的增强与批次 N 的训练。利用 GPU 加速库（如 NVIDIA DALI）异步执行这些任务。这有助于保持平滑和高吞吐量的数据管道。

**关注端到端吞吐量（例如，每秒 Token 数）**
记住，如果你的数据管道将吞吐量削减了一半，加速模型计算也无济于事。始终进行端到端分析，而不仅仅是隔离训练循环。使用 Nsight Systems 和 Nsight Compute 测量内核时间线和停顿，或使用 PyTorch 分析器进行框架级归因。然后比较使用合成数据与真实数据的迭代时间，看看数据加载引入了多少开销。目标是将开销控制在理想情况的 10% 以内。如果超过这个值，投入时间进行管道优化；它通常会在训练中产生巨大的“免费”加速。

## 性能分析、调试与监控

无法测量的东西就无法优化；性能分析揭示了你是受限于计算、内存、I/O 还是网络，以便你针对性地修复。持续的遥测和回归测试防止随着代码、驱动程序和数据的演变而失去优势。

具体来说，使用 Nsight Systems/Compute 和带有 NVTX 的框架分析器来确定你是受限于计算、内存、I/O 还是通信。削减 Python 开销，观察利用率缺口，平衡各 Rank 的工作，跟踪内存/网络/磁盘健康状况，并使用性能回归测试和警报来把关变更。使用以下指导来分析、监控和调试你的 AI 工作负载的性能：

**分析以寻找瓶颈和根本原因分析**
定期在你的训练/推理作业上运行分析器。使用 NVIDIA Nsight Systems 获取 CPU 和 GPU 活动的时间线。你也可以使用 Nsight Compute 或 PyTorch 分析器深入了解内核效率。确定你的作业是受限于计算、内存，还是等待 I/O/通信。针对性地进行优化。例如，如果你的工作负载受限于内存，重点放在减少内存流量上，而不是实施计算受限的优化。结合机器学习驱动的分析来预测和抢占性能瓶颈。这有助于实时自动化微调调整。使用 GPUDirect Storage 时，启用 GDS 追踪以将 cuFile 活动与内核缺口相关联。

**消除 Python 开销**
分析你的训练脚本以识别 Python 瓶颈——如过多的循环或日志记录——并用向量化操作或优化库调用替换它们。最小化 Python 开销有助于确保 CPU 不会成为整体系统性能中的隐形瓶颈。

**测量 GPU 利用率和空闲缺口**
持续监控 GPU 利用率、SM 效率、显存带宽使用情况等。如果你注意到利用率周期性下降，将其与事件相关联。例如，每 5 分钟一次的利用率下降可能与检查点保存相吻合。此类模式指出了优化机会，如交错检查点和使用异步刷新。利用 DCGM 或守护进程模式下的 `nvidia-smi` 随时间记录这些指标。

**使用 NVTX 标记**
使用 NVTX 范围或框架分析 API 对代码进行插桩，以标记不同阶段，包括数据加载、前向传递、后向传递等。这些标记显示在 Nsight Systems 或 Perfetto 时间线上，帮助你将 GPU 空闲时间或延迟归因于管道的特定部分。这使得向开发人员传达代码的哪一部分需要关注变得更加容易。对于 PyTorch，你可以使用 `torch.profiler.record_function()`。

**利用除 PyTorch 分析器之外的内核分析和分析工具**
对于性能关键型内核，使用 Nsight Compute 检查内核级指标，如占用率和吞吐量，或使用 Nsight Systems 分析 GPU/CPU 时间线和重叠。检查实现的占用率、内存吞吐量和指令吞吐量。寻找内存瓶颈的迹象，如内存带宽接近硬件最大值。这有助于识别内存受限的工作负载。分析器的“Issues”部分通常直接建议内核是受限于内存还是计算以及原因。使用此反馈指导代码更改，例如在全局加载效率低时改进内存合并。

**检查 Warp 分歧 (Warp Divergence)**
使用分析器查看 Warp 是否分歧，因为它可以显示分支效率和分歧分支指标。分歧意味着 Warp 中的一些线程由于分支而不活跃，这损害了吞吐量。如果显著，重新审视内核代码以重构条件或数据分配，以最小化 Warp 内分歧，并确保每个 Warp 处理一致的工作。

**验证负载均衡**
在多 GPU 作业中，跨 Rank 进行分析。有时一个 GPU（Rank 0）做额外的工作，如聚合统计数据和数据收集——并且经常成为瓶颈。监控每个 GPU 的时间线。如果一个 GPU 始终滞后，分发该额外工作负载。例如，你可以让非零 Rank 分担 I/O 和日志记录责任。确保所有 GPU/Rank 具有类似的工作负载，避免最慢的 Rank 拖累其余 Rank。

**监控内存使用**
随时间跟踪 GPU 内存分配和使用情况。确保你没有接近 OOM，这会导致框架意外地将张量交换到主机，从而导致巨大的减速。如果内存使用量逐次迭代攀升，你可能发现了泄漏。在这种情况下，使用 `torch.cuda.memory_summary()` 和 Nsight Systems 的 GPU 内存追踪等工具来分析详细的分配。在 CPU 方面，监控分页，因为你的进程的常驻内存（RES）不应显着超过物理 RAM。如果你看到分页，减少数据集预加载大小或增加 RAM。

**监控网络和磁盘**
对于分布式作业，使用操作系统工具监控网络吞吐量和磁盘吞吐量。确保实际吞吐量符合预期。例如，在 100 Gbps 链路上，如果充分利用，你应该看到 12.5 GB/s（12.5 GB/s = 100 Gb/s ÷ 8 bit/byte）。如果没有，网络可能是瓶颈或配置错误。同样，监控训练节点上的磁盘 I/O。如果你看到 100% 的磁盘利用率峰值且 GPU 空闲，你可能需要更好地缓冲或缓存数据。

**设置异常警报**
在生产或长时间运行的训练环境中，为 GPU 错误（如 ECC 错误、设备过热等）设置自动警报或日志。这将有助于识别异常缓慢的迭代。例如，NVIDIA 的 DCGM 可以观察健康指标，如果 GPU 开始节流或遇到错误，你可以触发操作。这有助于立即捕捉性能问题——如冷却故障导致节流——而不是在作业完成后。

**执行回归测试**
维护一组基准任务，以便在更改软件（包括 CUDA 驱动程序、CUDA 版本、AI 框架版本甚至你的训练代码）时运行。将性能与之前的运行进行比较以尽早捕捉回退。驱动程序更新或代码更改无意中降低吞吐量的情况并不少见——在标准工作负载上进行快速分析运行将突出显示这一点，以便你可以进行调查。例如，也许某个内核意外地不再使用 Tensor Cores 了。这确实值得关注。

## GPU 编程与 CUDA 调优优化

将内核与内存层次结构和硬件特性对齐是获得巨大、持久收益的地方。融合、Tensor Cores、CUDA Graphs 和编译器路径（如 `torch.compile` 和 OpenAI Triton）将启动开销转化为有用的数学运算。

针对内存层次结构进行优化：合并全局加载，分块到共享内存，管理寄存器/占用率，并将传输（如 `cp.async`/TMA）与计算重叠。优先使用经过调优的库和 CUDA Graphs，利用 `torch.compile` 和 OpenAI Triton 进行融合，并通过 Roofline 分析和 PTX/SASS 检查验证可扩展性。以下是一些 GPU 和 CUDA 编程优化提示和技术：

**理解 GPU 内存层次结构**
牢记 GPU 的分层内存结构——每线程寄存器、每块/SM 的共享内存/L1 缓存、跨 SM 的 L2 缓存和全局 HBM。最大化高层级的数据重用。例如，使用寄存器和共享内存来重用值，并最小化对较慢全局内存的访问。一个好的内核确保绝大多数数据要么在寄存器中，要么使用合并和缓存从 HBM 高效加载。

**合并全局内存访问**
确保同一 Warp 中的线程访问连续的内存地址，以便硬件可以用尽可能少的事务为它们提供服务。Warp 线程的跨步或分散内存访问将导致每个 Warp 进行多次内存事务，有效地浪费带宽。重组数据布局或索引计算，以便每当 Warp 加载数据时，都在单个宽内存事务中进行。

**使用共享内存进行数据重用**
共享内存就像一个具有非常高带宽的手动管理缓存。将频繁使用的数据——如矩阵的块——加载到共享内存中。并让线程在移动之前多次操作这些块。这种流行的分块（Tiling）技术极大地减少了全局内存流量。注意共享内存的 Bank 冲突。组织共享内存访问模式或填充数据，以确保线程不会争夺同一个内存 Bank，这会序列化访问并降低性能。

**优化内存对齐**
只要可能，将数据结构对齐到 128 字节，特别是对于批量内存复制或向量化加载。未对齐的访问即使在理论上是合并的，也可能强制进行多次事务。对全局内存 I/O 使用像 `float2` 和 `float4` 这样的向量化类型可以帮助每条指令加载/存储多个值，但要确保你的数据指针正确对齐到向量大小。

**最小化内存传输**
仅在必要时且以大块形式将数据传输到 GPU。如果可以，将许多小传输合并为一个大传输。例如，如果你每次迭代有许多小数组要发送，将它们打包到一个缓冲区中并发送一次。小而频繁的 `cudaMemcpy` 会成为瓶颈。如果使用统一内存，使用显式预取 (`cudaMemPrefetchAsync`) 在需要之前将数据暂存到 GPU 上，避免在关键计算部分期间发生按需缺页。

**避免过多的临时分配**
频繁分配和释放 GPU 内存会损害性能。例如，在内核中频繁使用 `cudaMalloc`/`cudaFree` 或设备 malloc 会导致额外开销。相反，重用内存缓冲区或使用大多数深度学习框架（如 PyTorch）中可用的内存池，这些框架实现了 GPU 缓存分配器。如果编写自定义 CUDA 代码，考虑使用带有内存池的 `cudaMallocAsync` 或自己管理暂存内存池，以避免重复分配/释放的开销。

**平衡线程与资源使用**
实现良好的占用率-资源平衡。使用更多线程以获得更高的占用率有助于隐藏内存延迟，但如果每个线程使用过多的寄存器或共享内存，占用率就会下降。调整你的内核启动参数——包括每块线程数——以确保你有足够的飞行 Warp 来覆盖延迟，但不要多到每个线程都缺乏寄存器或共享内存。在具有高指令级并行性 (ILP) 的内核中，减少寄存器使用以提高占用率实际上可能会损害性能。最佳点通常位于占用率谱的中间，因为最大占用率并不总是理想的。使用 NVIDIA Nsight Compute 占用率计算器来试验配置。

**监控寄存器和共享内存使用情况**
使用 Nsight Compute 等分析工具持续监控每线程寄存器和共享内存消耗。如果观察到占用率低于 25%，考虑增加每块线程数以更好地利用可用的硬件资源。然而，通过查看详细的占用率报告和内核执行指标，验证此调整不会导致过多的寄存器溢出（Spilling）。寄存器溢出会导致额外的内存流量并降低整体性能。

**重叠内存传输与计算**
只要可能，将内存传输与计算重叠。在多个 CUDA 流中使用 `cudaMemcpyAsync` 在内核运行时进行预取。优先使用张量内存加速器（TMA）进行到共享内存的批量移动，并使用 `cp.async` 进行细粒度的分阶段复制和预取。这些方法通过将数据传输与计算重叠，有效地掩盖了全局内存延迟，确保 GPU 核心保持充分利用，而无需等待内存操作完成。

**尽可能使用批量预取**
对于可预测的模式，使用 PTX `cp.async.bulk.prefetch.tensor.[1–5]d.L2.global*`（或 `prefetch.global.L2` 系列）预取到 L2，并使用 TMA（例如 `cp.async.bulk.tensor`）将块暂存到共享内存中。你也可以使用 `cp.async` 将全局内存异步暂存到共享内存，并将复制与计算重叠。你还可以显式地将数据提前加载到寄存器中。这些主动方法减少了全局内存访问造成的延迟，并确保关键数据在需要时即可在更快、更低延迟的存储中（如寄存器或共享内存）可用，从而最小化执行停顿并提高整体内核效率。

**利用协作组 (Cooperative Groups)**
利用 CUDA 的协作组在线程子集之间实现高效、局部的同步，而不是强制执行全块范围的屏障。该技术实现了更细粒度的同步控制，减少了不必要的等待时间和开销。通过对共享数据或执行相关计算的线程进行分组，你可以仅同步那些需要协调的线程，这可以导致更高效的执行模式和更好的整体吞吐量。

**优化 Warp 分歧**
构建你的代码，使 Warp 内的线程尽可能遵循相同的执行路径。分歧会使该 Warp 的执行时间加倍——例如，半个 Warp（16 个线程）走一个分支，半个 Warp（16 个线程）走另一个分支。如果你有某些数据很少触发的分支，考虑“排序”或分组数据，以便 Warp 处理统一的情况，如全真或全假。使用 Warp 级原语如 `ballot` 和 `shuffle` 为某些问题创建无分支解决方案。将 Warp 视为工作单元，并以此为目标让所有 32 个线程步调一致地做相同的工作以获得最大效率。

**利用 Warp 级操作**
在适当的时候使用 CUDA 的 Warp 内联函数让线程在不经过共享内存的情况下进行通信。例如，使用 `__shfl_sync` 向 Warp 中的所有线程广播一个值，或进行 Warp 级归约——如跨 Warp 求和寄存器——而不是每个线程写入共享内存。这些内联函数绕过了较慢的内存，并可以加速如归约或扫描等可以在 Warp 内完成的算法。通过在 Warp 内处理这些任务，你避免了与共享内存和全块同步相关的延迟。

**使用 CUDA 流进行并发**
在单个进程/GPU 内，如果独立的内核没有使用全部资源，就在不同的 CUDA 流中启动它们以重叠执行。重叠计算与计算——例如，一个流计算模型的一部分，而另一个流启动一个独立的内核，如 GPU 上的数据预处理或异步 memcpy。注意依赖关系，并在需要时使用 CUDA 事件进行同步。正确使用流可以通过不让任何资源闲置来增加 GPU 利用率——特别是当你有一些轻量级内核时。

**优先使用库函数**
只要可能，使用 NVIDIA 的优化库，如 cuBLAS、cuDNN、Thrust 和 NCCL，进行核心数学和集合运算。对于分布式推理中的点对点 GPU 数据移动，如果可用，使用 NIXL。当你需要细粒度的 GPU 发起传输时，也可以使用 NVSHMEM。这些针对每种 GPU 架构进行了大量优化，通常接近理论“光速”峰值。这将省去你自己重新发明它们的麻烦。例如，对于矩阵乘法使用 cuBLAS GEMM 而不是自定义内核，除非你有非常特殊的模式。这些库还透明地处理新的硬件特性。AI 框架如 PyTorch（及其编译器）在底层使用这些优化库。

**对重复启动使用 CUDA Graphs**
如果你有一个启动数千次的静态训练循环，考虑使用 CUDA Graphs 将操作序列捕获并启动为一个图。这可以显著减少每次迭代的 CPU 启动开销，特别是在多 GPU 场景中，启动许多内核和 memcpy 可能会给 CPU 带来额外压力并导致额外延迟。

**检查可扩展性限制**
当你优化内核时，定期检查它如何随问题大小和跨架构扩展。一个内核可能在小输入上实现极好的占用率和性能，但不能很好地扩展到大输入，因为它可能开始导致 L2 缓存抖动或遇到内存缓存驱逐。使用 Roofline 分析。比较实现的 FLOPS 和带宽与硬件限制，以确保你没有遗留性能潜力。

**检查 PTX 和 SASS 以进行高级内核分析**
对于性能关键型自定义 CUDA 内核，使用 Nsight Compute 检查生成的 PTX 和 SASS。这种深入挖掘可以揭示内存 Bank 冲突或冗余计算等问题，指导你进行针对性的底层优化。

**使用 PyTorch 编译器**
利用 PyTorch 的 `torch.compile` 通过 TorchInductor 将 Python 级操作融合到优化内核中。编译器还可以通过集成 CUDA Graphs 减少启动开销。一旦优化预热，通常可以获得约 10%–40% 的增益。这消除了解释器开销并解锁了编译器级优化。
在实践中，启用 `torch.compile` 通过自动组合内核和更高效地利用 NVIDIA GPU 硬件（例如 Tensor Cores），产生了实质性的加速（例如，在许多模型上为 20%–50%）。始终在你的模型上测试编译模式。虽然它可以极大地提高吞吐量，但在部署之前你应该确保兼容性和正确性。当图稳定时，启用 CUDA Graphs 以减少每迭代 CPU 开销。保持静态内存池以满足指针稳定性约束。

**为动态形状做计划**
如果你的输入大小变化，使用 `torch._dynamo.mark_dynamic()` 标注动态维度，或使用 `torch.export()` 导出形状多态图，然后编译。使用 `torch.compiler.set_stance()` 的 `"fail_on_recompile"` 和 `torch._dynamo.error_on_graph_break()` 控制重新编译行为，以在测试和 CI 中显现问题的形状变动。在可能的情况下使用静态形状以启用 CUDA Graphs 来减少每迭代 CPU 开销。

**利用 Triton 内核使用 `torch.compile`**
如果 PyTorch 不能很好地融合某个操作，考虑用 Triton 编写自定义 GPU 内核并集成它。PyTorch 让使用 `torch.library.triton_op` 注册自定义 GPU 内核变得容易。

**在可用时使用自动调优**
启用库自动调优功能以最大化底层性能。例如，当输入大小固定时，设置 `torch.backends.cudnn.benchmark=True`。这让 NVIDIA 的 cuDNN 库尝试多种卷积算法并为你的硬件挑选最快的一个。一次性开销带来的是可以加速训练和推理的优化内核。如果不需要精确的可复现性，通过禁用 `cudnn.deterministic` 允许非确定性算法，以解锁这些更快的实现。

**利用只读路径**
将频繁使用的常量或系数标记为只读，以便 GPU 可以将它们缓存在专用的 L1 只读缓存中。在 CUDA C++ 中，你可以使用 `const __restrict__` 指针提示数据是不可变的。在现代 GPU 架构上，编译器为 `const __restrict__` 限定的指针生成缓存的全局加载。当使用 AI 框架和库时，确保查找表或静态权重在设备上并被视为常量。这种优化减少了这些值的全局内存流量和延迟，因为每个 SM 可以快速从缓存中获取它们，而不是重复访问慢速 DRAM。

## 内核调度与执行优化

启动开销和不必要的同步会产生空闲缺口，从而破坏吞吐量。融合小内核并使用持久/动态策略可保持设备忙碌并隐藏延迟。

通过最小化同步、融合小内核以及在重复启动相同工作时使用持久内核来保持设备忙碌。对于不规则任务，考虑 GPU 动态并行——但要明智使用以避免增加开销。以下是关于改进内核调度和执行的提示：

**最小化 GPU 同步调用**
避免阻碍 GPU 进度的不必要的全局同步。过度使用 `cudaDeviceSynchronize()` 或阻塞 GPU 操作（如同步内存复制）会插入空闲缺口，在此期间 CPU 和 GPU 都无法做有用功。仅在绝对需要时同步。例如，在传输最终结果或调试时同步。通过让异步操作排队，你保持 GPU 忙碌和 CPU 自由以准备进一步的工作。这导致了更连续的执行流水线。

**融合小内核以分摊启动开销**
如果你有许多微小的 GPU 内核背靠背启动，考虑在可能的情况下合并它们的操作以在单个内核中运行。每个内核启动都有大约数十微秒的固定成本，因此通过手动 CUDA 内核融合、XLA 融合或像 NVIDIA CUTLASS/Triton 这样的工具用于自定义操作来组合操作可以提高吞吐量。融合内核花费更多时间做实际工作，而在启动开销或内存往返上花费更少时间。这在推理或预处理管道中特别有帮助，其中一系列逐元素操作可以一次性执行。先尝试 `torch.compile(mode="reduce-overhead")`。编译器可以融合操作链并将稳定区域包装在 CUDA Graphs 中。这将减少 CPU 启动开销。对于未融合的热点，考虑将它们迁移到 Triton 内核，并在适用的情况下使用异步 TMA 和自动 Warp 特化。

**利用 GPU 动态并行进行 GPU 内工作调度**
利用 CUDA 的动态并行让 GPU 内核从 GPU 启动其他内核，而无需返回 CPU。在具有不可预测或迭代工作的场景中，例如需要根据中间结果生成额外任务的算法，动态并行通过移除 CPU 启动瓶颈来削减延迟。例如，父内核可以直接在设备上划分并启动子内核进行进一步处理。这保持整个工作流在 GPU 上，避免 CPU 干预并实现更好的重叠和利用率。但是要明智地使用，因为如果滥用，它会引入自己的开销。

**对重复工作负载使用持久内核**
当工作负载涉及快速连续启动相同内核时（如处理工作队列或流式传输具有相同计算的批次），使用持久内核策略。持久内核启动一次并保持活跃，在循环中重用线程处理许多工作单元，而不是为每个单元启动新内核。这种方法通过显著降低调度开销来换取更复杂的内核设计。通过保持内核存活，你避免了重复的启动成本，并可以实现更高的持续占用率。高性能分布式训练和推理系统通常采用此技术来最大化吞吐量并最小化迭代任务的延迟。

**评估线程块集群 (Thread Block Clusters)**
使用线程块集群将数据保持在近处并减少重新启动开销。在 Blackwell 上，多达 16 个线程块可以形成一个集群（在增加非便携限制后）。使用集群感知同步和共享内存驻留来提高持久式设计中的局部性。使用内核级分析工具（如 Nsight Compute）分析占用率与驻留率的权衡。

## 算术优化与降低/混合精度

更低的精度和稀疏性让你用位宽换取巨大的速度和内存胜利——通常对准确性的影响可以忽略不计。混合精度、TF32/FP8/INT8 和融合缩放利用硬件数学路径来提高每美元吞吐量。

具体来说，使用混合精度 (BF16/FP16) 和 Tensor Cores 获得巨大收益，采用 TF32 获得简单的 FP32 加速，并在质量允许的情况下评估 FP8/FP4。利用结构化稀疏性、低精度梯度/通信以及用于推理的 INT8/INT4 量化——融合缩放/激活以保持准确性。以下优化技术适用于提高算术计算性能和利用降低/混合精度：

**使用混合精度训练**
利用 FP16 或 BF16 进行训练以加速数学运算并减少内存使用。现代 GPU 拥有 Tensor Cores，可以大规模加速 FP16/BF16 矩阵运算。将关键部分（如最终累加或权重副本）保留在 FP32 中以保持数值稳定性，但在半精度下运行大量计算。这通常提供大约 1.5–3.5 倍的加速（取决于模型和内核组合，在矩阵乘法繁重的工作负载上增益更大），精度损失极小，并且现在是大多数带有自动混合精度 (AMP) 框架的标准配置。

**拥抱梯度累积和激活检查点**
详细说明使用梯度累积在不增加额外内存使用的情况下有效增加批处理大小，并考虑激活检查点以减少非常深网络的内存占用。当训练接近或超过 GPU 内存限制的模型时，这些技术至关重要。

**在较新硬件上优先使用 BF16 而非 FP16**
如果可用，使用 BF16 而非 FP16，因为它具有更大的指数范围且不需要损失缩放（Loss Scaling）。现代 GPU 支持 BF16 Tensor Cores，速度与 FP16 相同。BF16 将通过避免溢出/下溢问题简化训练，同时仍获得半精度的性能优势。

**利用 FP8、新型精度和缩放技术**
在现代 GPU 上，FP8 Tensor Cores 在计算受限内核上提供的数学吞吐量大约是 FP16 或 BF16 的两倍，同时减少了激活和权重带宽。此外，FP4 (NVFP4) Tensor Cores 使 FP8 的吞吐量翻倍，并用于推理，配合微张量缩放（一种维持精度的纠错技术）来提高 Token 吞吐量。对于训练，使用带有 NVIDIA Transformer Engine 的 FP8，并在需要时保持 FP16 或 FP32 累加器。对于推理，首先评估 FP8，仅在校准显示对你的任务有可接受质量后才采用 NVFP4。建议使用混合 FP8（E4M3 用于前向激活/权重，E5M2 用于梯度）进行训练。具体来说，考虑将 E4M3 用于前向传播（例如，激活和权重），将 E5M2 用于反向传播（例如，梯度）。使用 256–1024 的延迟缩放窗口通常是有益的。对于推理，考虑校准后的 NVFP4。TE 与 PyTorch 集成并受现代 GPU 硬件支持。优先使用框架 TE 内核而非临时 FP8 自定义操作。端到端加速取决于内核组合、内存带宽和校准，因此请在你的模型和工作负载上验证准确性和性能。

**利用 Tensor Cores 和张量内存加速器 (TMA)**
如果可能，确保你的自定义 CUDA 内核利用 Tensor Cores 进行矩阵运算。这可能涉及为了简单起见使用 CUTLASS 模板。通过使用 Tensor Cores 和 TMA 进行到共享内存的异步张量移动，你可以为 GEMM、卷积和其他张量操作实现巨大的加速——通常达到 GPU 的近峰值 FLOPS。确保你的数据根据需要为 FP16/BF16/TF32，并对齐到 Tensor Core 分块维度（8 或 16 的倍数）。

**使用 TF32 获得简单加速**
对于 32 位矩阵乘法，设置 `torch.set_float32_matmul_precision("high")` 以启用 TF32（快速 FP32），用于 PyTorch 中数值安全的操作。像 cuBLAS 和 cuDNN 这样的库将在现代 GPU 硬件上自动选择最佳的 Tensor Core 代码路径。如果你强制使用全精度 FP32（使用 "highest" 而不是 "high"），请确保了解性能影响。

**利用结构化稀疏性**
现代 NVIDIA GPU 支持矩阵乘法中的 2:4 结构化稀疏性，它以结构化模式将 50% 的权重归零。这允许硬件使其吞吐量翻倍。通过剪枝你的模型来利用这一点。如果你可以剪枝权重以满足 2:4 稀疏模式，对于这些层，你的 GEMM 可以运行得快 ~2 倍。使用 NVIDIA 的 SDK 或库支持应用结构化稀疏性并确保使用稀疏 Tensor Core 路径。如果你的模型可以容忍或经过稀疏性训练（通常需要带有稀疏正则化的再训练），这可以提供免费的速度提升。

**尽可能降低梯度和激活的精度**
即使你将权重保持在较高精度，也可以考虑将梯度或激活压缩到较低精度。例如，使用 FP16/BF16 或 FP8 通信进行梯度传输。许多框架支持 FP16 梯度全归约。同样，对于激活检查点，以 16 位而不是 FP32 存储激活可以节省内存。关于 FP8 和 FP4 优化器及量化梯度的研究仍在继续。这些有助于在降低内存和带宽成本的同时维持模型质量。在带宽受限的环境中，梯度压缩尤其可以改变游戏规则。DeepSeek 展示了通过压缩梯度在受限 GPU 上进行训练。

**使用自定义量化进行推理**
对于部署，尽可能使用 INT8 量化。GPU 上的 INT8 推理非常快且节省内存。使用 NVIDIA 的 TensorRT 或量化工具将模型量化为 INT8 并校准它们。许多神经网络（如 Transformer）可以在 INT8 下运行，精度下降可以忽略不计。相对于 FP16，加速比可达 2–4 倍。在最新的 GPU 上，还可以探索和评估某些模型的 FP8 或 INT4，以进一步提高推理吞吐量。

**尽可能融合缩放和计算操作**
使用较低精度时，记得融合操作以保持准确性。例如，Blackwell 的 FP4“微缩放”建议每组值保持一个比例。通过在一次传递中进行缩放和计算来合并这些融合操作——而不是使用可能导致精度损失的单独传递。许多这些操作都由现有库处理，所以直接使用它们而不是从头开始实现。

## 高级调优策略与算法技巧

算法转变通常通过减少工作量而不是推得更快来在 ROI 上击败硬件升级。自动调优、FlashAttention、通信/计算重叠和分片在减少浪费的同时解锁了规模。

具体来说，自动调优内核和层参数，换入融合/FlashAttention 内核，并在分布式训练中重叠通信与计算。使用流水线/张量并行和 ZeRO 分片扩展深度模型，并考虑异步更新或剪枝/稀疏性，以牺牲一点准确性工作换取巨大的吞吐量胜利。以下是一些高级性能优化和算法技巧：

**自动调优内核参数**
为目标 GPU 自动调优你的自定义 CUDA 内核。选择正确的块大小、分块大小、展开因子等会影响性能，并且最佳设置通常在 GPU 代际之间（如 Ampere, Hopper, Blackwell 及以后）有所不同。使用自动调优脚本或框架（如 OpenAI Triton）——甚至在预处理步骤中进行暴力搜索——来找到最佳启动配置。这可以轻松产生 20%–30% 的改进，而这些改进是你使用静态“合理”设置会错过的。在自动调优循环中使用 Triton 功能——例如，设置 `num_warps` 和 `num_stages`，启用自动 Warp 特化，并测试异步 TMA 布局。在迁移到不同硬件时重新基准测试分块形状，因为最佳选择会随 GPU 代际而不同。优先使用张量映射描述符 API 进行共享内存暂存。

**在 ML 工作负载中使用内核融合**
利用深度学习库提供的融合内核。例如，启用融合优化器将融合权重更新、动量等逐元素操作。这也将使用融合多头注意力实现和融合归一化内核。NVIDIA 的库和一些开源项目（如 Transformer Engine 和 FasterTransformer）为常见模式提供了融合操作，例如融合 LayerNorm + Dropout。这些减少了启动开销并更有效地使用内存。

**利用像 FlashAttention 这样的内存高效注意力**
为 Transformer 模型集成像 FlashAttention 这样的高级算法。FlashAttention 以分块、流式方式计算注意力，避免具体化大型中间矩阵，大幅减少内存使用并提高速度——特别是对于长序列。用 FlashAttention 替换标准注意力可以同时提高吞吐量和内存占用，允许在相同硬件上使用更大的批处理大小或序列长度。

**重叠通信与计算**
在分布式训练中，只要可能，就将网络通信与 GPU 计算重叠。例如，对于梯度全归约，一旦每层的梯度准备好，就在下一层仍在计算反向传播时异步启动全归约。如果做得正确，这种流水线可以完全隐藏全归约延迟。使用异步 NCCL 调用或框架库（如 PyTorch 的分布式数据并行 DDP），它们提供了开箱即用的重叠。这确保了 GPU 不会闲置等待网络。

**对深层模型使用流水线并行**
当模型大小迫使你使用张量并行或流水线并行跨 GPU 进行流水线处理时，你可以使用足够的微批次来保持所有流水线阶段忙碌。利用 NVLink/NVSwitch 在阶段之间快速发送激活值。通过使用交错调度来重叠并减少流水线气泡。一些框架自动化了这种类型的调度。NVL72 网络结构在这里特别有帮助，因为即使是通信密集型的流水线阶段也可以以多 TB 的速度交换数据，最大限度地减少流水线停顿。

**利用分布式优化器分片**
使用像零冗余优化器 (ZeRO) 这样的内存节省优化策略，它跨 GPU 分片张量（如优化器状态和梯度），而不是复制它们。这允许通过分发内存和通信负载来扩展到极端模型大小。它通过减少每 GPU 内存压力、避免交换到 CPU 以及如果分块进行则减少通信量来提高吞吐量。许多框架如 DeepSpeed 和 Megatron-LM 提供这种类型的分片。对于大模型利用它来保持高速运行而不发生 OOM 或遭受交换带来的减速。

**尽可能异步训练**
如果适用，考虑异步更新。例如，你可以使用陈旧随机梯度下降 (SGD)，其中工作节点并不总是等待彼此共享更新。这种方法可以增加吞吐量，虽然它可能需要仔细调整以不影响收敛。如果做得当，异步训练可以提供巨大的性能优势。

**结合稀疏性和剪枝**
大模型通常有冗余。在训练期间使用剪枝技术引入稀疏性，你可以在推理时利用这一点——如果支持的话，在训练期间也可以部分利用。现代 GPU 硬件支持加速的稀疏矩阵乘法 (2:4)，未来的 GPU 可能会扩展此功能。即使你保留训练为密集型并仅针对推理进行剪枝，较小的模型也会运行得更快并使用更少的内存。这提高了模型部署的成本效率。探索彩票假设、蒸馏或结构化剪枝，以在修剪模型大小的同时保持准确性。

## 分布式训练与网络优化

在集群规模下，网络成为限制因素。如果不加处理，网络会破坏线性扩展并推高成本。RDMA/巨型帧、分层集合通信、亲和性和压缩保护了带宽并驯服了延迟。

在可用时使用 RDMA (InfiniBand/RoCE)；如果在以太网上，调整 TCP 缓冲区，启用巨型帧，并选择现代拥塞控制。对齐 NIC/CPU 亲和性，调整 NCCL 线程/缓冲区（以及支持的地方使用 SHARP/CollNet），压缩或累积梯度，并测试网络结构以捕捉丢包或配置错误。遵循此指导来优化分布式环境（如多 GPU 和多节点模型训练）的网络：

**在可用时使用 RDMA 网络**
为你的多节点集群配备 InfiniBand 或 RoCE 以获得低延迟和高吞吐量。确保 NCCL 和 MPI 在训练中使用 RDMA。NCCL 将自动检测 InfiniBand 并在可用时使用 GPUDirect RDMA。RDMA 绕过内核网络堆栈，与传统 TCP 相比可以显著降低延迟。如果你只有以太网，请在支持 RDMA 的 NIC 上启用 RoCE 以获得类似 RDMA 的性能。在 NVLink 域系统（NVL72, GB200/GB300 等）上，尽可能将集合通信保持在网络结构上。为岛间链路保留主机网络。将 NCCL 拓扑提示与你的 NVLink/NVSwitch 域对齐。

**如果使用以太网，调整 TCP/IP 堆栈**
对于基于 TCP 的集群，增加网络缓冲区大小。提高 `/proc/sys/net/core/{r,w}mem_max` 和自动调优限制 (`net.ipv4.tcp_{r,w}mem`) 以允许更大的发送/接收缓冲区。这有助于饱和 10/40/100 GbE 链路。在所有节点和交换机上启用巨型帧（MTU 9000）以减少每包开销，这提高了吞吐量并减少了 CPU 使用率。还要考虑现代 TCP 拥塞控制，如 BBR，用于广域网或拥塞网络。

**为 NIC 分配 CPU 亲和性**
将网络中断和线程绑定到与 NIC 位于同一 NUMA 节点的 CPU 核心。这避免了网络流量的跨 NUMA 惩罚，并保持网络堆栈的内存访问本地化。检查 `/proc/interrupts` 并使用 `irqaffinity` 设置来确保，例如，你在 NUMA 节点 0 中的 NIC 由 NUMA 节点 0 中的核心处理。这可以提高网络性能和一致性，特别是在高包率下。

**为你的环境优化 NCCL 环境变量**
为大型多节点作业试验 NCCL 参数。例如，将 `NCCL_NTHREADS`（NCCL 的每 GPU CPU 线程数）从默认的 4 增加到 8 或 16，以通过消耗更多 CPU 使用率来驱动更高带宽。将 `NCCL_BUFFSIZE`（每 GPU 缓冲区大小）从默认的 1 MB 增加到 4 MB 或更多，以获得大消息的更好吞吐量。如果你的集群使用支持 SHARP 的交换机，安装 NCCL SHARP 插件并通过设置 `NCCL_COLLNET_ENABLE=1` 启用 CollNet，然后使用 SHARP 插件变量，如 `SHARP_COLL_LOCK_ON_COMM_INIT=1` 和 `SHARP_COLL_NUM_COLL_GROUP_RESOURCE_ALLOC_THRESHOLD=0`，如文档所述。只有当你的归约足够大且网络结构支持 SHARP 卸载时，才期望有加速。

**针对慢速网络使用梯度累积**
如果你的网络因为扩展了太多由中等性能互连链接的节点而成为瓶颈，使用梯度累积来执行更少、更大的全归约操作。在同步之前累积几个微批次的梯度，这样你就为 N 个批次通信一次，而不是每个批次都通信。这牺牲了一点额外内存和一些模型准确性调优，换取了显著降低的网络开销。当添加更多 GPU 因通信成本导致收益递减时，这特别有帮助。

**优化全归约拓扑**
确保你正在使用适合你集群拓扑的最佳全归约算法。NCCL 会自动选择环或树算法，但在混合互连（如每个节点上的 GPU 通过 NVLink 连接，节点之间通过 InfiniBand 或以太网连接）上，分层全归约可能是有益的。分层全归约首先在节点内执行全归约操作，然后跨节点进行。大多数框架默认执行基于 NCCL 的分层聚合，但通过分析进行验证。在传统的 MPI 设置中，你可能会考虑手动执行这种两级归约——首先是节点内，然后是节点间。

**避免网络超额订阅**
在多 GPU 服务器上，确保 GPU 的组合流量不会使 NIC 超额订阅。例如，八个 GPU 在全归约期间可以轻松产生超过 200 Gbps 的流量，因此只有一个 100 Gbps NIC 会限制你。如果扩展到每个节点许多 GPU，考虑每个节点多个 NIC 和 200/400 Gbps InfiniBand。同样，如果你的 NIC 和 GPU 共享同一个 PCIe 根复合体，请注意 PCIe 带宽限制。

**压缩通信**
就像单节点内存一样，考虑压缩用于网络传输的数据。技术包括 16 位或 8 位梯度压缩、为跨节点流水线传输量化激活，甚至更奇特的方法如草图（Sketching）。如果你的网络是最慢的组件，稍微高一点的计算成本来压缩/解压数据可能是值得的。NVIDIA 的 NCCL 本身不支持压缩，但你可以在框架中集成压缩（例如，Horovod 中的梯度压缩或 PyTorch 中的自定义 AllReduce 钩子）。这是 DeepSeek 成功的关键之一——压缩梯度以应对有限的节点间带宽。

**监控网络健康**
确保没有无声的问题阻碍你的分布式训练。检查丢包（在 InfiniBand 上表现为重试或超时，使用计数器检查重发；在以太网上，检查 TCP 重传）。即使是小的丢包也会由于拥塞控制介入而严重降低吞吐量。使用带外网络测试（如 iPerf 或 NCCL 测试）来验证你是否获得了预期的带宽和延迟。如果没有，调查交换机配置、NIC 固件或 CPU 亲和性。

## 高效推理与服务

服务是一场成本与延迟的博弈——利用率通过编排和批处理而不仅仅是更大的 GPU 来提升。专用运行时、KV 缓存策略和预热保持了高吞吐量而不违反 SLO。

通过自动伸缩、微服务和动态/连续批处理来编排需求，使 GPU 保持热度而不违反延迟 SLO。使用专用运行时（vLLM, SGLang, TensorRT-LLM），利用 NIXL 和 KV 缓存卸载进行分离式服务，预热模型，并隔离资源以控制尾部延迟。遵循这些技术来提高模型推理效率和性能：

**高效编排动态资源**
集成高级容器编排平台，如带有自定义性能指标的 Kubernetes。这实现了基于实时使用模式和吞吐量目标的动态扩展和负载均衡。

**拥抱无服务器架构进行推理**
探索用于推理工作负载的无服务器架构和微服务设计，这可以高效处理突发流量，并通过在需求低时缩减规模来减少空闲资源开销。

**优化批处理和并发**
对于推理工作负载，找到正确的批处理策略。对于推理工作负载，倾向于动态或连续批处理以自动批处理传入请求。较大的批处理大小通过保持 GPU 忙碌来提高吞吐量，但太大可能会增加延迟。此外，如果一个流没有使用所有 GPU 资源，并行运行多个推理流——例如，两个并发推理批次以充分利用 GPU SM 和 Tensor Cores。

**利用 NIXL 进行分布式推理**
当跨 GPU 或节点服务大模型时，使用 NVIDIA Inference Xfer Library 通过 RDMA 在预填充和解码工作节点之间流式传输 KV 缓存。在 NIXL 的情况下，大型基于 Transformer 的 KV 缓存是在节点间传输的。NIXL 提供了一个高吞吐量、低延迟的 API，用于在分离式 LLM 推理集群中从预填充 GPU 向解码 GPU 流式传输 KV 缓存。它使用 GPUDirect RDMA 和最佳路径来实现这一点——且无需 CPU 参与。这减少了跨节点分离式预填充解码服务的尾部延迟。

**必要时卸载 KV 缓存**
如果 LLM 的注意力 KV 缓存增长超过 GPU 显存，使用分层卸载。NVIDIA Dynamo 的分布式 KV 缓存管理器将较少访问的 KV 页面卸载到 CPU 内存、SSD 或网络存储，而像 TensorRT-LLM 和 vLLM 这样的推理引擎支持分页和量化的 KV 缓存。重用缓存以降低内存压力和首字延迟。验证端到端影响，因为卸载未命中会引入额外的 I/O 延迟。这允许对原本会超出 GPU 显存的序列进行推理——并且由于快速 NVMe 和计算-I/O 重叠，性能损失最小。如果你预期有非常长的提示词或聊天，请确保你的推理服务器配置为使用此功能。卸载到磁盘总比完全失败好。

**高效地服务模型**
使用优化的模型推理系统，如 vLLM、SGLang、NVIDIA Dynamo 和 NVIDIA TensorRT-LLM，以低延迟和高吞吐量服务大模型。它们应实现量化、低精度格式、融合、高度优化的注意力内核和其他技巧，以在推理期间最大化 GPU 利用率。这些库还应处理张量并行、流水线并行、专家并行、上下文并行、推测解码、分块预填充、分离式预填充/解码和动态请求批处理——以及许多其他高性能功能。

**监控并调优尾部延迟**
在实时服务中，平均延迟和（长）尾部延迟（第 99 百分位）都很重要。分析推理延迟的分布。如果尾部很高，识别异常原因，如意外的 CPU 参与、垃圾回收 (GC) 暂停或过多的上下文切换。将你的推理服务器进程绑定到特定核心，将其与嘈杂的邻居隔离，并在必要时使用实时调度以获得更一致的延迟。

**预热以避免冷启动延迟**
通过将模型加载到 GPU 并运行几次虚拟推理来预热 GPU。这将避免第一个真实请求进入推理服务器时的一次性冷启动延迟冲击。

**为服务质量 (QoS) 高效划分资源**
如果在同一基础设施上运行混合、异构工作负载，如训练和推理——或具有不同架构的模型，考虑划分资源以确保延迟敏感的推理获得优先权。这可能意味着将某些 GPU 完全专用于推理，或使用 MIG 给推理服务一个有保证的 GPU 切片（如果它不需要完整的 GPU 但需要可预测的延迟）。如果可能，将推理与训练分离在不同节点上，因为训练可能会因大量 I/O 或突然的通信爆发而引入抖动。

**利用 Grace CPU 进行推理预处理**
在 Grace Blackwell 系统中，服务器级 CPU 可以在与 GPU 相同的内存空间中极快地处理预处理——如分词和批次整理。将此类任务卸载到 CPU，并让它在 GPU 可以直接使用的共享内存中准备数据。这减少了缓冲区的重复，并利用强大的 CPU 处理部分推理管道，释放 GPU 以专注于计算更密集的神经网络计算。

**为边缘 AI 和延迟关键型部署仔细调优**
通过利用专用边缘加速器和优化中央服务器与边缘设备之间的数据传输协议，将性能调优扩展到边缘。这将有助于为时间敏感型应用程序实现超低延迟。

## 多节点推理与服务

分离预填充/解码和分片模型让你能够处理更大的上下文和更多的用户，并获得更高的占用率。连续批处理和分层内存/卸载即使在长提示词和重并发下也能保持流动。

具体来说，跨设备分离预填充和解码，跨请求持续池化 Token，并通过张量/流水线并行分片超大模型。为非常长的上下文添加分层内存/卸载，这样你就可以服务更多请求而不会 OOM，以小延迟换取高得多的容量。以下性能提示适用于多节点推理和服务：

**分离推理管道**
将推理工作流分为不同的阶段，包括处理通过所有模型层的输入提示词的“预填充”阶段，以及逐个生成输出 Token 的迭代“解码”阶段。将这些阶段分配给不同的资源以允许独立扩展。这种两阶段方法防止较快的任务被较慢的任务瓶颈化。对于大型语言模型，一种策略是运行完整模型对提示词进行编码，然后基于阶段处理自回归解码，可能每个阶段都有专门的工作节点。通过分离管道，你确保 GPU 持续在它们最高效的任务部分工作，避免队头阻塞，即一个长生成任务拖延了后面的任务。

**使用连续批处理处理 LLM**
超越简单的请求批处理，使用连续批处理策略在重负载下最大化吞吐量。传统的动态批处理将传入请求分组并作为一个批次处理以提高 GPU 利用率。连续批处理更进一步，即时动态合并和拆分跨请求的 Token 序列。像 vLLM 这样的系统实现了 Token 池化，只要任何线程准备好生成下一个 Token，它就会与其他准备好的线程分组形成一个新批次。这种方法使 GPU 始终保持高占用率，并大幅减少空闲时间。结果是显着更高的 Token 吞吐量和更好的延迟一致性，尤其是在服务许多具有不同序列长度的并发用户时。

**跨 GPU 和节点高效分片模型**
对于太大而无法装入单个 GPU 内存的模型，采用模型并行推理技术，通过跨多个 GPU 甚至多个服务器对模型进行分区。这可以通过张量并行（跨设备拆分每层的权重和计算）或流水线并行（将模型的层拆分到托管在不同 GPU 上的段中并顺序流式传输数据）来完成。虽然模型分片引入了通信开销和一些增加的延迟（因为数据必须在分片之间流动），但它使得部署原本无法服务的万亿参数模型成为可能。确保 GPU 之间有高速互连，如 NVLink 或 InfiniBand，以使其可行，并在可能的情况下重叠通信与计算。关键是平衡负载，使所有设备并行工作，没有单一阶段成为瓶颈。

**为扩展上下文卸载内存**
使用分层内存策略支持需要比 GPU 可用内存更多内存的推理工作负载。在服务非常大的模型或长序列上下文（如长多轮对话和大文档）时整合内存卸载。较不频繁使用的数据，如旧的注意力 KV 缓存条目或不常访问的模型权重，可以在 GPU 内存紧张时移动到 CPU RAM 甚至 NVMe 存储。现代推理框架可以自动交换出这些张量，并在需要时即时带回。虽然这为缓存未命中引入了额外的延迟，但它防止了内存溢出错误，并允许你处理极端情况。通过深思熟虑地卸载和预取数据，你牺牲一点速度换取了服务具有大工作集请求的能力，在内存限制下实现了更好的整体吞吐量。

## 电源与热管理

每瓦性能是一等指标——热或功率节流会抹去调优收益并缩短硬件寿命。功率上限、高效打包和主动冷却在削减能源支出的同时稳定了时钟频率。

跟踪每瓦性能和热量以及速度：为了更好的效率，对内存受限的工作负载设置功率上限或降频，且吞吐量损失最小。主动管理冷却，合并作业以使 GPU 接近满载运行，监控每 GPU 功耗，并在降低成本时围绕电价/可再生能源进行调度。以下是关于管理 AI 系统电源和热特性的提示：

**尽可能利用高效和环保的能源**
跟踪并优化能源消耗与性能。除了管理功率和热限制外，监控能源使用指标并考虑提高性能和可持续性的技术。例如，通过实施动态功率封顶或基于可再生能源可用性的工作负载转移，你可以降低运营成本和碳足迹。这种双重关注减少了运营成本并支持负责任、环保的 AI 部署。

**监控热量和时钟**
在运行期间关注 GPU 温度和时钟频率。如果 GPU 接近热限制（在某些情况下为 85°C），它们可能会开始降低时钟频率，这会降低性能。使用 `nvidia-smi dmon` 或遥测来查看时钟是否从最大值下降。如果你检测到节流，改善冷却，增加风扇速度，改善气流，或稍微降低功率限制以保持在稳定的热包络内。目标是一致的性能，没有热诱导的下降。

**使用能源感知动态电源管理**
现代数据中心越来越多地使用能源感知调度，根据实时能源成本和可再生能源可用性调整工作负载。整合自适应功率封顶和动态时钟缩放有助于优化每瓦吞吐量，同时降低运营成本和碳足迹。

**优化每瓦性能**
在功率预算受限（或能源成本高）的多 GPU 部署中，考虑为效率进行调优。许多工作负载，尤其是内存受限的工作负载，可以在略微降低的 GPU 时钟下运行，性能损失可以忽略不计，但功耗显着降低。例如，如果内核受限于内存，将 GPU 锁定在较低时钟可以节省电力而不损害运行时间。这提高了每瓦吞吐量。测试几个功率限制（使用 `nvidia-smi -pl`），看看你的每瓦吞吐量是否提高。对于某些模型，从 100% 到 80% 的功率限制产生的速度几乎相同，但功耗减少 20%。

**使用自适应冷却策略**
如果在具有可变冷却或能源可用性的环境中运行，请与集群管理集成以调整工作负载。例如，在一天中较凉爽的时间或当可再生能源供应高时调度繁重的作业——如果这是成本因素的话。一些站点实施策略，将非紧急作业排队在电价便宜的夜间运行。这不会改变单作业性能，但会显着削减成本。

**合并工作负载**
以高利用率运行 GPU，而不是以低利用率运行许多 GPU。繁忙的 GPU 在每瓦完成的工作方面比空闲或轻度使用的 GPU 更节能。这是因为当 GPU 忙碌时，基准功率得到了更好的分摊。在一个 GPU 上以 90% 的利用率依次运行一个作业，可能比在两个 GPU 上并行以 45% 的利用率运行更好——除非你需要为最小挂钟时间进行优化。计划调度以在不使用时关闭或闲置整个节点，而不是让大量硬件以低利用率运行。

**高效配置冷却**
对于风冷系统，考虑在繁重运行期间将 GPU 风扇设置为较高的固定速度，以先发制人地冷却 GPU。一些数据中心总是以最大速度运行风扇以提高一致性。确保数据中心的进气温度在规格范围内。定期检查服务器 GPU 中的灰尘或阻塞物。堵塞的散热片会大大降低冷却效率。对于水冷，确保流速最佳且水温受到控制。

**仔细监控功率**
使用工具监控每 GPU 功耗。`nvidia-smi` 报告瞬时功耗，这有助于理解你的工作负载的功率概况。功率峰值可能与某些阶段相关。例如，全归约阶段可能测得较少的计算负载和较少的功率，而密集层会使负载和功率测量值激增。了解这一点，你可以潜在地对工作负载进行排序以平滑功耗。如果在受限电源电路上运行集群，这很重要。在功率受限的情况下，你可能需要避免在同一节点上同时运行多个功率尖峰型作业，以避免触发功率限制。

**提高长期运行作业的弹性**
如果你正在运行一个长达数月的训练作业或 24-7 的推理作业，请考虑热量对硬件寿命的影响。持续以 100% 功率和热限制运行可能会随着时间的推移略微增加故障风险。在实践中，数据中心 GPU 专为这种弹性而构建，但如果你想格外安全，以 90% 的功率目标运行可以减少组件压力，且减速极小。这是长期训练运行与硬件磨损之间的权衡——特别是如果该硬件将在很长一段时间内被多个项目重复使用。

## 结论

将此清单视为可重复的剧本：分析，在正确的层级调整正确的瓶颈，并在扩展前验证增益。通过有条不紊地应用这些实践——从操作系统和内核到分布式通信和服务——无论规模大小，你都能实现快速、具有成本效益且可靠的 AI 系统。

这份清单虽然全面，但并未穷尽。随着硬件、软件和算法的演进，AI 系统性能工程领域将继续增长。并非此处的每一条最佳实践都适用于每种情况。但是，总的来说，它们涵盖了 AI 系统性能工程场景的广度。这些提示封装了多年来优化 AI 系统性能所积累的大量实践智慧。

在调优你的 AI 系统时，你应该系统地浏览本章列出的每个相关类别，并运行清单中的每一项。例如，你应该确保操作系统已调优，确认 GPU 内核高效，检查是否正确使用了库，监控数据管道，优化训练循环，调优推理策略，并优雅地扩展。遵循这些最佳实践，你可以诊断并解决大多数性能问题，并从你的 AI 系统中提取最大性能。

并且记住，在大幅扩展集群之前，你应该在较少数量的节点上进行分析，并识别潜在的扩展瓶颈。例如，如果你看到全归约集合操作在 8 个 GPU 上已经占据了迭代时间的 20%，那么在更大规模下只会变得更糟——特别是当你超过单个计算节点或数据中心机架系统的容量时，如 Grace Blackwell GB200 和 GB300 NVL72 以及 Vera Rubin VR200 和 VR300 NVL 系统。

随身携带这份清单，并在发现新技巧时添加进去。将这些提示和最佳实践与前面章节的深入理解相结合，你将设计和运行高效、可扩展、可维护、具有成本效益且可靠的 AI 系统。

现在，去让你们最雄心勃勃的想法成为现实吧。祝优化愉快！











