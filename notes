AI Systems Performance Engineering



贯穿硬件、软件与算法，



神经网络的基础知识、python、机器学习





PUE

- 液冷
- 风冷





第一章

1. 充分利用可用硬件，例如 deepseek 利用通信带宽

2. AI Systems Performance Engineer 核心能力

    1. 硬件、软件和算法
    2. 低层操作系统、存储层次结构、网络基础，并掌握多种语言如 Python 和 C++，以及诸如 PyTorch、OpenAI 的 Triton 和 NVIDIA 的Compute Unified Device Architecture (CUDA) 等不同的 AI 框架与库。
        在任何一天，AI 系统性能工程师可能会检查低级 GPU 内核效率、优化操作系统线程调度、分析内存访问模式、提高网络吞吐效率，或调试分布式训练算法
    3. 职责：基准测试、性能分析、调试、优化、扩展分布式训练与推理、高效管理资源
        1. 基准测试和性能分析包括在不同工作负载（包括训练和推理）下测量延迟、吞吐量、内
            存使用和其他性能指标。为了识别瓶颈，我们必须迭代性地将NVIDIANsight
            Systems和NVIDIANsightCompute与PyTorch分析器一起使用。

3. MLPerf

4. 100万亿参数，

    1. 每个参数以16位（2字节）精度存储，加载该模型大约需要182TB的GPU内存（182TB= 100 万亿参数× 16 位每权重× 8 位每字节）。B200GPU上的192GB（可用约180GB）GPU内存 1000块，700块 B300 288GB

5. goodput 有效吞吐量：理论硬件能力中有多少真正用于有用计算。

    传统的吞吐量指标如FLOPS和设备利用率往往显得偏高，因为大量时间可能被阻塞的通信、空闲的计算或失败的任务重启所占用。



第二章

1. 与 H100 相比，它降低了 NVLink 互连带宽和 FP64 性能，同时保持 HBM 容量和带宽在大体上相似。
    1. NVIDIAH100每块GPU的NVLink互连带宽约为900GB/s，而H800每块GPU约为400GB/s。
    2. **3.35 TB/s**的显存带宽（SXM版本）或**2 TB/s**的显存带宽（PCIe版本）
2. GB200NVL72：36 个通过NVLink和NVSwitch互联的GraceBlackwell，每个GraceBlackwell超芯片由一颗NVIDIAGraceCPU （ 72个CPU核心）和两颗NVIDIABlackwellGPU组成，总计每个机架共有36颗GraceCPU和72颗BlackwellGPU。每块GPU双向1.8TB/s的完整NVLink5带宽相互通信（18 × 100 GB/s链路）
3. GB300NVL72Ultra：每个GPU配备 288GB HBM3e 的Blackwell Ultra GPU，并保持一个72GPU的NVLink域，聚合带宽约为每秒130TB。
4. 在整个机架范围内，NVLink交换系统在一个NVL72域内提供约130TB/s的总体GPU到GPU带宽。
5. 72块GPU共有大约13.5TB（13,824GB= 192 GB每块GPU× 72 GPUs）的HBM3e内存，若将同一NVLink域内的GraceCPU内存也计入，总内存约为30TB。
6. 一整套GB200NVL72机架在理论上对于FP4（2:1结构化稀疏）可达到约1.44
    exaFLOPS，对于FP8（2:1结构化稀疏）可达到约720petaFLOPS
7. 机架功耗约为120到132kW，具体取决于供应商配置和冷却方案
8. 在 GB200 Superchip中，Grace CPU 提供高达~480 GB的 LPDDR5X，带宽高达~500 GB/s，两个 Blackwell GPU共计贡献高达~384 GB 的 HBM3e 内存（每个GPU共192GB）。总体而言，GB200 Superchip 暴露出大约~900GB的一致性统一内存，GPU 和 CPU 可在统一地址空间中访问。
9. **B200** 并不是单一的一颗巨型 GPU 芯片，而是由**两个独立的 GPU die（芯片）**封装在同一个模块里，通过高速芯片间互连互通数据。这种设计叫 **Multi-Chip Module (MCM)**：一种专用的高速10TB/s晶片间互连NV‑HBI（高带宽接口）进行通信。
10. 每个BlackwellB200GPU模块在两个GPU芯片（每个96GB）上共有192GB（可用180GB）HBM3e内存，分为8层堆栈。一个8层HBM3e堆栈是通过垂直叠加八个DRAM芯片构建的——每个3GB——每个堆栈总计24GB。
11. HBM3e每个GPU的总带宽约为8 TB/s
12. SM??
13.  每个BlackwellGPU暴露18个NVLink5端口。单个 NVLink 端口 = 100 GB/s 双向，18 个端口全部接入 NVLink 网络 → **聚合双向带宽 = 18 × 100 GB/s = 1800 GB/s ≈ 1.8 TB/s**
14. NVLink 每个托盘有 **144 个 NVLink 端口**（速率 100 GB/s 双向）九个托盘
15. NVSwitch/NVLink 5/
16. 计算托盘通常每个节点使用四个ConnectX‑8800Gb/s网卡以提供高外部带宽。每个计算节点配备四个800Gb/s的网卡，总计每节点3.2Tbit/s，每个机架约57.6Tbit/s。
17. 对于需要在网络内加速或卸载以处理存储、安全和控制平面任务的场景，会使用BlueField‑3DPU。
18. 可帮助卸载诸如RDMA、TCP/IP和NVMeSSD存储访问等网络任务，
19. NVL72中的每个计算节点包含两个 Grace Blackwell Superchip，两者合计约消耗6kW。18个计算节点的总耗电为~110 kW。NVSwitch托盘、网络交换机、空气冷却和水冷泵共计占用~20 kW，总计整个NVL72机架的耗电为130kW。
20. NVL72机架在装满硬件和冷却液时重约3,000磅（ 1.3–1.4吨）
21. 要高效运行NVL72，需要对性能、利用率和功耗进行仔细监控。
22. DataCenterGPUManager(DCGM)，每个GPU的指标，例如GPU利用率百分比、显存使用、温度和NVLink吞吐量
23. 每枚BlackwellUltraB300GPU的内存容量大约比B200（180GB）多50%（288GB），同时在AI计算性能方面提高了1.5× 倍
24. 一套由72块GPU构成的GB300机架由36个GraceBlackwellUltra模块组成（每个模块包含2个GPU和+ 1CPU），~20.7TB的HBM（72 × 288 GB），以及~18 TB的DDR（36 × 500 GB）。合计每个GB300NVL72机架拥有~38 TB的高速内存。
25. VR  LPDDR6内存，带宽约为1TB/s。GPU高带宽内存（HBM），带宽约为13–14TB/s。第六代NVLink6带宽翻倍.

